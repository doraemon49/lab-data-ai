"""
[Single cell clustering based on cell-pair differentiability correlation and variance analysis](https://academic.oup.com/bioinformatics/article/34/21/3684/4996592)
ë…¼ë¬¸ ê¸°ë°˜ ì½”ë“œ ì‘ì„± (ì œê³µëœ github ì—†ìŒ)

**Jiang et al., 2018 (Corr)**ì˜ â€œcell-pair differentiability correlationâ€ ì•„ì´ë””ì–´ë¥¼ Scanpy ê¸°ë°˜ìœ¼ë¡œ ì¬í˜„í•œ ì½”ë“œ ìŠ¤ì¼€ì¹˜ì…ë‹ˆë‹¤.
- ì› ë…¼ë¬¸ì€ U_ijë¥¼ â€œi, j ë‘ ì…€ì„ ì œì™¸í•œ ì „ì²´ ëŒ€ë¹„ ì°¨ë“±ë°œí˜„ íŒ¨í„´â€ìœ¼ë¡œ ì •ì˜í•˜ê³ , ë‘ íŒ¨í„´ ê°„ ìƒê´€ì„ ìœ ì‚¬ë„ë¡œ ì”ë‹ˆë‹¤(ì‹ (1)â€“(4)) .
- ê³„ì‚°ëŸ‰ì„ ì¤„ì´ê¸° ìœ„í•´, ë…¼ë¬¸ì´ ì œì•ˆí•œ ê·¼ì‚¬(O(np))ì²˜ëŸ¼ U_ij ëŒ€ì‹  ì…€ i ê¸°ì¤€ì˜ ë‹¨ì¼ Ui (ì „ì²´ í‰ê·  ëŒ€ë¹„ ë¶€í˜¸)ë¡œ ë°”ê¾¸ê³ , í–‰(ì…€) ê°„ ìƒê´€ â‰ˆ differentiability correlationë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤(ì €ìë“¤ì´ ëª…ì‹œí•œ ê·¼ì‚¬) .
- ê·¸ ìœ ì‚¬ë„(=ìƒê´€)ë¡œ **ê³„ì¸µì  êµ°ì§‘(HCA)**ì„ í•˜ê³ , ë…¼ë¬¸ ë°©ì‹ì˜ ë¶„ì‚°ë¶„í•´(ANOVA) ê¸°ë°˜ K ê²°ì •ì„ ë‹¨ìˆœí™”í•´ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤(ì„¹ì…˜ 2.2) .
- ì´í›„ ê° í´ëŸ¬ìŠ¤í„°ë¥¼ í‰ê· /í•©/ë©”ë„ì´ë“œë¡œ ëŒ€í‘œí•˜ëŠ” pseudo-cellì„ ë§Œë“­ë‹ˆë‹¤ (ë‹¹ì‹ ì˜ ëª©ì ).
ì‚¬ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬: scanpy, numpy, scipy, scikit-learn, pandas


ë¬´ì—‡ì„ í•´ì£¼ë‚˜ìš”?
1. ê° adataì—ì„œ ìƒìœ„ HVG ì„ íƒ í›„ í–‰ë ¬ ğ‘‹ì¶”ì¶œ
2. Differentiability vector ğ‘ˆ = sign(X - row_mean_excluded) (ì—¬ê¸°ì„  ì „ì²´í‰ê·  ê·¼ì‚¬) ìƒì„±
3. í–‰(ì…€) ê°„ ìƒê´€(centered cosine)ì„ ìœ ì‚¬ë„ë¡œ, 1 - similarityë¥¼ ê±°ë¦¬ë¡œ ì‚¬ìš©
4. HCA + ë¶„ì‚°ë¶„í•´ ê¸°ë°˜ K ì„ íƒ(ì²« ë¡œì»¬ ìµœëŒ€)
5. ëŒ€í‘œ pseudo-cell ìƒì„±: mean/sum/medoid ì˜µì…˜
6. ìš”ì²­í•˜ì‹  5Ã—5 ë°˜ë³µ ë£¨í”„ì— ë°”ë¡œ ë¼ì›Œë„£ê¸°


<< ì‚¬ìš© íŒ / ì˜µì…˜ >>
< ëŒ€í‘œê°’ ë°©ì‹ >
- rep_method='mean': ê° í´ëŸ¬ìŠ¤í„° í‰ê·  â†’ ê°€ì¥ í”í•œ pseudo-cell
- rep_method='sum': ì¹´ìš´íŠ¸ ê¸°ë°˜ downstream(DESeq2) ë“±ê³¼ ì¹œí™”ì 
- rep_method='medoid': ì‹¤ì œ ì…€ í•˜ë‚˜ë¡œ ëŒ€í‘œ(í•´ì„ ìš©ì´)
< K ê²°ì • >
- ìœ„ êµ¬í˜„ì€ ë…¼ë¬¸ì˜ ANOVA ì•„ì´ë””ì–´ë¥¼ ê°„ì†Œí™”í–ˆìŠµë‹ˆë‹¤(ìœ ì „ì ì „ë°˜ì˜ âˆ‘ğ‘Ÿğ‘— ìµœëŒ€). ë…¼ë¬¸ì€ â€œì—¬ëŸ¬ m(ìƒìœ„ DE gene ìˆ˜) ì¡°í•© ë°˜ë³µ í›„ ìµœë¹ˆ Kâ€ë¡œ ì•ˆì •í™”í•©ë‹ˆë‹¤ . ì›ë¦¬ì— ë” ê°€ê¹ê²Œ í•˜ë ¤ë©´, choose_k_via_variance_analysisì—ì„œ ì—¬ëŸ¬ HVG í¬ê¸°/ë¶€ë¶„ì§‘í•©ì„ ìƒ˜í”Œë§í•´ ìµœë¹ˆê°’ì„ ì±„íƒí•˜ì„¸ìš”. 
< ê³„ì‚°ëŸ‰ >
- ë³¸ ê·¼ì‚¬ ë²„ì „ì€ ğ‘ˆğ‘–ë¡œ ëŒ€ì²´í•´ O(nÂ·p) ìƒì„± + ìœ ì‚¬ë„ í–‰ë ¬ ì—°ì‚°(nÃ—n). nì´ ë§¤ìš° í¬ë©´ block ì²˜ë¦¬ ë˜ëŠ” kNN sparsification í›„ HCA ëŒ€ì‹  ê·¸ë˜í”„êµ°ì§‘(Leiden)ë„ ëŒ€ì•ˆì…ë‹ˆë‹¤.
< ì¼ì¹˜ì„± >
- ì›ì‹ (4)ì˜ â€˜i, j ìŒ ë³„ U_{ij}, U_{ji}â€™ë¥¼ ì •í™•íˆ êµ¬í˜„í•˜ë ¤ë©´ O(nÂ²p)ë¼ í° ë°ì´í„°ì—ì„œ ë¶€ë‹´ì´ í½ë‹ˆë‹¤. ë…¼ë¬¸ë„ nì´ í´ ë•Œ ğ‘ˆğ‘– ê·¼ì‚¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤ .
"""

# pip install scanpy scipy scikit-learn pandas
# pip install -U pip setuptools wheel
# python -m pip install -U pip setuptools wheel # venv ì•ˆì—ì„œ pip ê³ ì • ì—…ë°ì´íŠ¸
# python -m pip install "numpy==1.26.4"
# python -m pip install --no-cache-dir "scikit-misc==0.3.1"



# python data_cell_reduce2.py

import os
import numpy as np
import pandas as pd
import scanpy as sc
from scipy.spatial.distance import squareform
from sklearn.cluster import AgglomerativeClustering
from sklearn.preprocessing import StandardScaler

# ---------- Core: Differentiability correlation (approx. O(n p)) ----------
def differentiability_vectors(X):
    """
    X: (n_cells, n_genes) float array (normalized/log1p ê¶Œì¥)
    Return U: (n_cells, n_genes) in {-1,0,1}, where sign(X - global_mean)
    ê·¼ì‚¬: ì› ë…¼ë¬¸ì˜ U_ij ëŒ€ì‹  U_i (ì „ì²´ í‰ê·  ëŒ€ë¹„) ì‚¬ìš©  
    """
    gmean = X.mean(axis=0, keepdims=True)  # â‰ˆ (sum over l x_lk - x_ik) / (n-1)
    U = np.sign(X - gmean)  # -1, 0, +1
    return U

def rowwise_center_normalize(M):
    """
    ê° í–‰(ì…€)ì—ì„œ í‰ê·  ì œê±° í›„ L2 ì •ê·œí™” â†’ í–‰ ê°„ ë‚´ì  = í”¼ì–´ìŠ¨ ìœ ì‚¬ë„ ìœ ì‚¬
    """
    M = M - M.mean(axis=1, keepdims=True)
    norms = np.linalg.norm(M, axis=1, keepdims=True) + 1e-8
    return M / norms

def differentiability_correlation(X):
    """
    Return S: similarity (n x n) in [-1,1], and D: distance = 1 - S in [0,2]
    """
    U = differentiability_vectors(X)
    Uc = rowwise_center_normalize(U.astype(np.float32))
    S = Uc @ Uc.T  # (n x n) similarity
    np.fill_diagonal(S, 1.0)
    D = 1.0 - S
    return S, D

# ---------- ANOVA-based K selection (simplified from paper Section 2.2) ----------
def anova_R_score(X, labels):
    """
    ì„¹ì…˜ 2.2ì˜ r_j = SSB_j / SST_j ë¥¼ ì—¬ëŸ¬ ìœ ì „ìì— ëŒ€í•´ í•©í•¨  
    X: (n, p); labels: (n,)
    """
    n, p = X.shape
    labs = np.asarray(labels)
    uniq = np.unique(labs)
    # ì „ì²´ í‰ê· 
    mu = X.mean(axis=0, keepdims=True)
    # ìœ ì „ìë³„ ì´ì œê³±í•©(SST)
    SST = ((X - mu)**2).sum(axis=0) + 1e-12
    # êµ°ì§‘ë³„ í‰ê· 
    SSB = np.zeros(p, dtype=np.float64)
    for k in uniq:
        Xk = X[labs == k]
        nk = Xk.shape[0]
        if nk == 0: 
            continue
        muk = Xk.mean(axis=0, keepdims=True)
        SSB += nk * (muk - mu).ravel()**2
    r = SSB / SST  # ê° ìœ ì „ìë³„ ë¹„ìœ¨
    return float(r.sum())

def choose_k_via_variance_analysis(X, D, k_range=range(2, 11), linkage='average'):
    """
    D: precomputed distance (n x n)
    X: expression matrix (n x p) â€” HVG ê³µê°„ ì‚¬ìš© ê¶Œì¥
    ë…¼ë¬¸ì²˜ëŸ¼ 'ì²« ë²ˆì§¸ ë¡œì»¬ ìµœëŒ€'ë¥¼ ì„ íƒ. ê°„ë‹¨í™”ë¥¼ ìœ„í•´ ì „ì—­ ìµœëŒ€ë¡œ ëŒ€ì²´ ê°€ëŠ¥. 

    - ë°ì´í„° í–‰ë ¬ X ìì²´ë¥¼ ë„£ê³  ì‹¶ë‹¤ â†’ linkage='ward', metric='euclidean' (HVG ê³µê°„ì—ì„œ ê°€ì¥ í”íˆ ì“°ëŠ” ì¡°í•©)
    - ì‚¬ì „ ê³„ì‚°ëœ ê±°ë¦¬í–‰ë ¬ Dë¥¼ ì“°ê³  ì‹¶ë‹¤ â†’ linkage='average', metric='precomputed' (cosine similarityë‚˜ correlation distance ë“±ì„ ì§ì ‘ ê³„ì‚°í•´ì„œ ë„£ì„ ë•Œ ìœ ìš©)
    """
    n = D.shape[0]
    best_k, best_R, best_labels = None, -np.inf, None
    for k in k_range:
        model = AgglomerativeClustering(
            n_clusters=k, metric='euclidean', linkage=linkage
        )
        labels = model.fit_predict(D)
        R = anova_R_score(X, labels)
        if R > best_R:
            best_k, best_R, best_labels = k, R, labels
    return best_k, best_labels, best_R

# ---------- Cluster representatives ----------
def cluster_representatives(X, labels, method='mean'):
    """
    X: (n, p), labels: (n,), method in {'mean','sum','medoid'}
    Return dict: cluster_id -> 1D vector (p,)
    """
    reps = {}
    uniq = np.unique(labels)
    for k in uniq:
        idx = np.where(labels == k)[0]
        Xk = X[idx]
        if method == 'mean':
            reps[k] = Xk.mean(axis=0)
        elif method == 'sum':
            reps[k] = Xk.sum(axis=0)
        elif method == 'medoid':
            # medoid: í´ëŸ¬ìŠ¤í„° ë‚´ë¶€ í‰ê· ê±°ë¦¬ ìµœì†Œ ì…€
            from sklearn.metrics import pairwise_distances
            dk = pairwise_distances(Xk, Xk, metric='euclidean')
            ci = np.argmin(dk.mean(axis=1))
            reps[k] = Xk[ci]
        else:
            raise ValueError("method must be one of {'mean','sum','medoid'}")
    return reps

# ---------- End-to-end for one AnnData ----------
def run_corr_pipeline(adata, n_hvg=2000, rep_method='mean', k_range=range(2, 11)):
    """
    ì…ë ¥: AnnData (adata.XëŠ” raw/normalized ëª¨ë‘ ê°€ëŠ¥; ì—¬ê¸°ì„  log1p ê¶Œì¥)
    ì¶œë ¥: dict with labels, D(similarity-based distance), representatives
    """
    ad = adata.copy()
    # ê°„ë‹¨ ì „ì²˜ë¦¬ (í•„ìš”ì‹œ ì¡°ì •)
    if 'log1p' not in ad.uns.get('pp', {}):
        sc.pp.normalize_total(ad, target_sum=1e4)
        sc.pp.log1p(ad)
    sc.pp.highly_variable_genes(ad, n_top_genes=n_hvg, flavor='seurat_v3')
    ad = ad[:, ad.var['highly_variable']].copy()

    X = ad.X.A if hasattr(ad.X, 'A') else ad.X
    X = np.asarray(X, dtype=np.float32)

    # ì„ íƒì ìœ¼ë¡œ gene-wise scaling (ANOVA ì•ˆì •í™”ì— ë„ì›€)
    X = StandardScaler(with_mean=True, with_std=True).fit_transform(X)

    # Corr ìœ ì‚¬ë„ & ê±°ë¦¬
    S, D = differentiability_correlation(X)  # D = 1 - S

    # K ì„ íƒ + êµ°ì§‘
    k_opt, labels, R = choose_k_via_variance_analysis(X, D, k_range=k_range)

    # ëŒ€í‘œ pseudo-cell
    reps = cluster_representatives(X, labels, method=rep_method)

    out = dict(
        k_opt=k_opt,
        labels=labels,
        R_score=R,
        distance=D,
        similarity=S,
        representatives=reps,
        hvg_mask=np.asarray(ad.var['highly_variable'])
    )
    return out

# ---------- 5x5 ë£¨í”„ ----------
base = "/data/project/kim89/0804_data"
save = "/data/project/kim89/0804_data_cell_reduce"
results = {}  # (repeat, fold) -> outputs

for repeat in range(5):
    for fold in range(5):
        print(f"ğŸ” Repeat {repeat}, Fold {fold}")
        tr_path = os.path.join(base, f"repeat_{repeat}", f"fold_{fold}_train.h5ad")
        te_path = os.path.join(base, f"repeat_{repeat}", f"fold_{fold}_test.h5ad")

        adata_train = sc.read_h5ad(tr_path)
        adata_test  = sc.read_h5ad(te_path)

        # Trainì— ëŒ€í•´ Corr ê¸°ë°˜ êµ°ì§‘ & ëŒ€í‘œ ìƒì„±
        train_out = run_corr_pipeline(
            adata_train, n_hvg=2000, rep_method='mean', k_range=range(2, 16)
        )

        # Testì—ë„ ë™ì¼ íŒŒì´í”„ë¼ì¸ ì ìš©(ì˜µì…˜: trainì˜ HVG/ìŠ¤ì¼€ì¼ëŸ¬ ê³ ì • ê°€ëŠ¥)
        test_out = run_corr_pipeline(
            adata_test, n_hvg=2000, rep_method='mean', k_range=range(2, 16)
        )

        results[(repeat, fold)] = dict(train=train_out, test=test_out)

        # ì˜ˆì‹œ: train ëŒ€í‘œ pseudo-cellì„ ìƒˆë¡œìš´ AnnDataë¡œ ë§Œë“¤ì–´ ì €ì¥
        reps_mat = np.vstack([train_out['representatives'][k] for k in sorted(train_out['representatives'])])
        rep_ids  = [f"cluster_{k}" for k in sorted(train_out['representatives'])]
        var_hvg  = adata_train.var_names[train_out['hvg_mask']]
        ad_rep   = sc.AnnData(reps_mat, obs=pd.DataFrame(index=rep_ids), var=pd.DataFrame(index=var_hvg))
        out_dir  = os.path.join(save, f"repeat_{repeat}")
        os.makedirs(out_dir, exist_ok=True)
        out_file = os.path.join(out_dir, f"fold_{fold}_train_corr_pseudocells.h5ad")
        ad_rep.write(out_file)
        print(f"âœ… saved pseudo-cells: {out_file}")
