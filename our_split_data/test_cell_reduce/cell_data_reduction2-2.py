"""
ver2. ì½”ë“œ íë¦„ì€ nÃ—n ìœ ì‚¬ë„(ë°€ì§‘ í–‰ë ¬) ê³„ì‚° â†’ precomputed ê±°ë¦¬ë¡œ HCAë¼ì„œ, ì…€ ìˆ˜ê°€ ì¡°ê¸ˆë§Œ ì»¤ì ¸ë„ ì‹œê°„Â·ë©”ëª¨ë¦¬ í­ë°œì´ ë‚©ë‹ˆë‹¤.

í•µì‹¬ ë³€ê²½ì  (ìš”ì•½)

1. ìœ ì‚¬ë„ í¬ì†Œí™”:
U = sign(X - mean)ê¹Œì§€ëŠ” ë™ì¼
NearestNeighbors(metric="cosine")ë¡œ **kNN ê·¸ë˜í”„(í¬ì†Œ)**ë§Œ ê³„ì‚°
â†’ nÃ—n ë°€ì§‘í–‰ë ¬ ëŒ€ì‹  O(nÂ·k) ë©”ëª¨ë¦¬/ì—°ì‚°

2. í´ëŸ¬ìŠ¤í„°ë§ êµì²´:
HCA(precomputed ê±°ë¦¬) â†’ Leiden(ê·¸ë˜í”„ êµ°ì§‘)
Scanpyì˜ sc.tl.leidenì€ í¬ì†Œ kNN ê·¸ë˜í”„ì— ìµœì í™”

3. ì¦‰ì‹œ ì €ì¥/ì²´í¬í¬ì¸íŠ¸:
ë°˜ë³µ/í´ë“œë§ˆë‹¤ ëŒ€í‘œ pseudo-cellì„ ë°”ë¡œ .h5adë¡œ ì €ì¥
ì§„í–‰ë¥ /íƒ€ì´ë° ì¶œë ¥

4. í…ŒìŠ¤íŠ¸ ì‹œê°„ ë‹¨ì¶• ì˜µì…˜:
n_hvg, n_neighbors, k_range ë“± í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ í•©ë¦¬ì ìœ¼ë¡œ ì¶•ì†Œ
í•„ìš”ì‹œ train ê¸°ì¤€ HVG/ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ testì— ì¬ì‚¬ìš©(ì¬í˜„ì„± + ì†ë„)
"""

import os, time
import numpy as np
import pandas as pd
import scanpy as sc
from sklearn.neighbors import NearestNeighbors
from sklearn.preprocessing import StandardScaler
import scipy.sparse as sp

# pip3 install igraph
# pip3 install leidenalg

# python data_cell_reduce2-2.py

# ----- ë¹ ë¥¸ Corr-approx: kNN on U (sparse graph) + Leiden -----
def build_corr_knn_graph(X, n_neighbors=30):
    """
    X: (n_cells, n_genes) float32, scaled
    1) U = sign(X - global_mean)
    2) cosine kNN (í–‰ ì¤‘ì‹¬í™”/ì •ê·œí™”ëŠ” cosineìœ¼ë¡œ ì¶©ë¶„)
    return: sparse adjacency (symmetric), where weight ~ similarity
    """
    gmean = X.mean(axis=0, keepdims=True)
    U = np.sign(X - gmean).astype(np.float32)

    # kNN in U-space with cosine distance
    nn = NearestNeighbors(n_neighbors=n_neighbors, metric="cosine", n_jobs=-1)
    nn.fit(U)
    dists, idxs = nn.kneighbors(U, return_distance=True)  # d in [0,2], cosine
    # similarity = 1 - distance
    sims = 1.0 - dists
    n = U.shape[0]

    # build sparse directed graph, then symmetrize (max)
    rows = np.repeat(np.arange(n), n_neighbors)
    cols = idxs.ravel()
    data = sims.ravel()

    G = sp.csr_matrix((data, (rows, cols)), shape=(n, n))
    # ensure self-loops (needed by Leiden sometimes)
    G = G.maximum(G.T)
    G.setdiag(1.0)
    G.eliminate_zeros()
    return G

def cluster_leiden_from_graph(adata, G, resolution=1.0):
    """
    Plug user graph into AnnData and run Leiden.
    """
    # neighbors slots
    adata.obsp["connectivities"] = G
    # distance is optional; we can approximate as 1 - w
    adata.obsp["distances"] = sp.csr_matrix(G.shape, dtype=np.float32)
    sc.tl.leiden(adata, resolution=resolution, key_added="corr_leiden")

def cluster_representatives(X, labels, method='mean'):
    reps = {}
    uniq = np.unique(labels)
    for k in uniq:
        idx = np.where(labels == k)[0]
        Xk = X[idx]
        if method == 'mean':
            reps[k] = Xk.mean(axis=0)
        elif method == 'sum':
            reps[k] = Xk.sum(axis=0)
        elif method == 'medoid':
            # ê°„ë‹¨í•œ medoid (ìœ í´ë¦¬ë“œ)
            from sklearn.metrics import pairwise_distances
            dk = pairwise_distances(Xk, Xk, metric='euclidean')
            reps[k] = Xk[np.argmin(dk.mean(axis=1))]
        else:
            raise ValueError("method must be one of {'mean','sum','medoid'}")
    return reps

def run_fast_corr_pipeline(
    adata, n_hvg=2000, n_neighbors=30, resolution=1.0, rep_method='mean'
):
    ad = adata.copy()
    # í‘œì¤€ ì „ì²˜ë¦¬ (ì¡´ì¬í•˜ë©´ ìƒëµë¨)
    if 'log1p' not in ad.uns.get('pp', {}):
        sc.pp.normalize_total(ad, target_sum=1e4)
        sc.pp.log1p(ad)

    sc.pp.highly_variable_genes(ad, n_top_genes=n_hvg, flavor='seurat_v3')
    ad = ad[:, ad.var['highly_variable']].copy()

    X = ad.X.A if hasattr(ad.X, 'A') else ad.X
    X = np.asarray(X, dtype=np.float32)

    # gene-wise scaling: ANOVAëŠ” ì•ˆ ì“°ì§€ë§Œ, cosine kNN ì•ˆì •í™”ì— ë„ì›€
    X = StandardScaler(with_mean=True, with_std=True).fit_transform(X).astype(np.float32)

    # í¬ì†Œ ê·¸ë˜í”„ êµ¬ì„±
    G = build_corr_knn_graph(X, n_neighbors=n_neighbors)

    # Leiden êµ°ì§‘
    cluster_leiden_from_graph(ad, G, resolution=resolution)
    labels = ad.obs['corr_leiden'].astype(str).values

    # ëŒ€í‘œê°’ (pseudo-cell)
    reps = cluster_representatives(X, labels, method=rep_method)

    return dict(
        labels=labels,
        representatives=reps,
        n_clusters=len(np.unique(labels)),
        hvg_mask=np.asarray(ad.var_names)
    )

import numpy as np
import pandas as pd
import scanpy as sc
from collections import Counter

def majority_vote(series: pd.Series):
    """ë²”ì£¼í˜• ìµœë¹ˆê°’ê³¼ ë¹„ìœ¨ ë°˜í™˜"""
    cnt = series.value_counts(dropna=True)
    if len(cnt) == 0:
        return np.nan, np.nan
    mode = cnt.index[0]
    frac = float(cnt.iloc[0]) / float(series.notna().sum())
    return mode, frac

def save_pseudocells_with_metadata(
    adata_orig: sc.AnnData,
    labels: np.ndarray,          # ê¸¸ì´ = adata_orig.n_obs
    reps_dict: dict,             # {cluster_id: vector}
    gene_names: pd.Index,        # reps_dict ë²¡í„° ìˆœì„œì— ëŒ€ì‘í•˜ëŠ” var_names (HVG)
    out_h5ad_path: str,
    mapping_csv_path: str = None,
    obs_cols_to_summarize: list = None,   # ìš”ì•½í•˜ê³  ì‹¶ì€ ì›ë³¸ obs ì»¬ëŸ¼ë“¤
):
    """
    - adata_orig: ì›ë³¸ AnnData (HVG ì„ íƒ/ìŠ¤ì¼€ì¼ ì „ ë‹¨ê³„ í˜¹ì€ ê°™ì€ HVG ìˆœì„œì—¬ë„ ë¬´ë°©)
    - labels: ê° ì…€ì˜ í´ëŸ¬ìŠ¤í„° ë¼ë²¨ (ë¬¸ì/ì •ìˆ˜ ìƒê´€ì—†ìŒ)
    - reps_dict: cluster_id -> ëŒ€í‘œ ë²¡í„° (len = len(gene_names))
    - gene_names: ëŒ€í‘œ ë²¡í„°ì˜ ìœ ì „ì ì¸ë±ìŠ¤
    - obs_cols_to_summarize: Noneì´ë©´ ì›ë³¸ì˜ ì£¼ìš” ì»¬ëŸ¼ ì¼ë¶€ ìë™ ì„ íƒ ì˜ˆì‹œ ì œê³µ
    """
    # ì •ë ¬ëœ í´ëŸ¬ìŠ¤í„° ìˆœì„œ
    ks = sorted(reps_dict, key=lambda x: int(x) if str(x).isdigit() else str(x))
    mat = np.vstack([reps_dict[k] for k in ks]).astype(np.float32)

    # --- obs ìš”ì•½ ë©”íƒ€ë°ì´í„° ë§Œë“¤ê¸° ---
    obs_df = pd.DataFrame(index=[f"cluster_{k}" for k in ks])
    obs_df["n_cells"] = 0

    # ìš”ì•½ ëŒ€ìƒ ì»¬ëŸ¼ ì§€ì • (ì—†ìœ¼ë©´ ì˜ˆì‹œë¡œ ëª‡ ê°œ ìë™ ì„ íƒ)
    if obs_cols_to_summarize is None:
        # ì›ë³¸ì— ì¡´ì¬í•˜ë©´ ì¢‹ì€ ê²ƒë“¤ ì˜ˆì‹œ
        candidates = [
            "donor_id", "patient", "manual_annotation", "singler_annotation",
            "Coarse_Cell_Annotations", "Detailed_Cell_Annotations",
            "disease__ontology_label", "organ__ontology_label", "sex", "age"
        ]
        obs_cols_to_summarize = [c for c in candidates if c in adata_orig.obs.columns]

    # ê° í´ëŸ¬ìŠ¤í„°ë³„ ì›ë³¸ ì¸ë±ìŠ¤ ìˆ˜ì§‘
    labels = pd.Series(labels, index=adata_orig.obs_names, name="cluster")
    clust_to_cells = {k: labels.index[labels.astype(str) == str(k)] for k in ks}

    # ë§¤í•‘ CSV (ì›ë³¸ ì…€ -> í´ëŸ¬ìŠ¤í„°)
    if mapping_csv_path is not None:
        map_df = pd.DataFrame({
            "cell": labels.index,
            "cluster": labels.values
        })
        map_df.to_csv(mapping_csv_path, index=False)

    # ìˆ«ìí˜•/ë²”ì£¼í˜• ë‚˜ëˆ ì„œ ìš”ì•½
    for k in ks:
        cell_ids = clust_to_cells[k]
        obs_df.loc[f"cluster_{k}", "n_cells"] = len(cell_ids)

        if len(cell_ids) == 0:
            continue

        sub = adata_orig.obs.loc[cell_ids]

        for col in obs_cols_to_summarize:
            if col not in sub.columns:
                continue
            s = sub[col]

            if pd.api.types.is_numeric_dtype(s):
                obs_df.loc[f"cluster_{k}", f"mean_{col}"] = float(np.nanmean(s.values))
            else:
                mode, frac = majority_vote(s.astype("object"))
                obs_df.loc[f"cluster_{k}", f"mode_{col}"] = mode
                obs_df.loc[f"cluster_{k}", f"mode_{col}_frac"] = frac

    # --- var ë©”íƒ€ë°ì´í„° êµ¬ì„± ---
    var_df = pd.DataFrame(index=pd.Index(gene_names, name="gene"))
    # ì›ë³¸ varì— ê°™ì€ ì´ë¦„ì´ ìˆìœ¼ë©´ ëª‡ ê°œ í•„ë“œë¥¼ ë³µì‚¬ (ì˜ˆ: 'highly_variable', 'means' ë“±)
    for col in ["highly_variable", "means", "dispersions", "dispersions_norm", "Gene", "id_in_vocab", "n_cells"]:
        if col in adata_orig.var.columns:
            common = var_df.index.intersection(adata_orig.var.index)
            var_df.loc[common, col] = adata_orig.var.loc[common, col]

    # --- AnnData ì‘ì„± ë° ì €ì¥ ---
    ad_pseudo = sc.AnnData(
        X=mat,
        obs=obs_df,
        var=var_df
    )
    ad_pseudo.uns["pseudo_info"] = {
        "note": "Corr-based pseudo-cells with summarized metadata",
        "n_original_cells": int(adata_orig.n_obs),
        "n_original_genes": int(adata_orig.n_vars),
        "n_clusters": int(len(ks)),
    }
    ad_pseudo.write(out_h5ad_path)

# -------------------- 5x5 ë£¨í”„ + ì²´í¬í¬ì¸íŠ¸ ì €ì¥ --------------------
base = "/data/project/kim89/0804_data"
save = "/data/project/kim89/0804_data_cell_reduce"
out_tag = "corr_sparse"  # ì¶œë ¥ íŒŒì¼ëª… íƒœê·¸
os.makedirs(base, exist_ok=True)

# def save_pseudocells(reps_dict, gene_names, out_path):
#     # reps_dict: {cluster_id: vector}
#     ks = sorted(reps_dict, key=lambda x: int(x) if str(x).isdigit() else str(x))
#     mat = np.vstack([reps_dict[k] for k in ks]).astype(np.float32)
#     obs = pd.DataFrame(index=[f"cluster_{k}" for k in ks])
#     var = pd.DataFrame(index=gene_names)
#     ad = sc.AnnData(mat, obs=obs, var=var)
#     ad.write(out_path)

for repeat in range(5):
    for fold in range(5):
        t0 = time.time()
        print(f"\nğŸ” Repeat {repeat}, Fold {fold}")
        tr_path = os.path.join(base, f"repeat_{repeat}", f"fold_{fold}_train.h5ad")
        te_path = os.path.join(base, f"repeat_{repeat}", f"fold_{fold}_test.h5ad")

        adata_train = sc.read_h5ad(tr_path)
        adata_test  = sc.read_h5ad(te_path)

        # --- TRAIN ---
        print("  â–¶ TRAIN: build graph & cluster ...")
        train_out = run_fast_corr_pipeline(
            adata_train,
            n_hvg=1500,           # ë” ë¹ ë¥´ê²Œ
            n_neighbors=30,
            resolution=1.0,
            rep_method='mean'
        )
        # ì¦‰ì‹œ ì €ì¥ (ì²´í¬í¬ì¸íŠ¸)
        out_dir = os.path.join(save, f"repeat_{repeat}")
        os.makedirs(out_dir, exist_ok=True)
        # train_outfile = os.path.join(out_dir, f"fold_{fold}_train_{out_tag}_pseudocells.h5ad")
        # save_pseudocells(train_out['representatives'], train_out['hvg_mask'], train_outfile)
        # print(f"  âœ… saved TRAIN pseudo-cells -> {train_outfile} (k={train_out['n_clusters']})")
        # ê¸°ì¡´ save_pseudocells(...) ëŒ€ì‹ :
        
        train_outfile = os.path.join(out_dir, f"fold_{fold}_train_{out_tag}_pseudocells.h5ad")
        train_mapcsv  = os.path.join(out_dir, f"fold_{fold}_train_{out_tag}_mapping.csv")

        save_pseudocells_with_metadata(
            adata_orig=adata_train,                   # ì›ë³¸ adata (ì „ì²˜ë¦¬ ì „ì´ì–´ë„ OK)
            labels=train_out['labels'],               # ê¸¸ì´ = adata_train.n_obs
            reps_dict=train_out['representatives'],   # cluster -> ë²¡í„°
            gene_names=train_out['hvg_mask'],         # ì‚¬ìš©ëœ HVG ì´ë¦„(Index)
            out_h5ad_path=train_outfile,
            mapping_csv_path=train_mapcsv,
            obs_cols_to_summarize=None                # ë˜ëŠ” ["manual_annotation", "patient", ...]
        )
        print(f"âœ… saved TRAIN pseudo-cells -> {train_outfile}")
        print(f"ğŸ§­ saved CELLâ†”CLUSTER map  -> {train_mapcsv}")


        # --- TEST ---
        print("  â–¶ TEST: build graph & cluster ...")
        test_out = run_fast_corr_pipeline(
            adata_test,
            n_hvg=1500,
            n_neighbors=30,
            resolution=1.0,
            rep_method='mean'
        )
        test_outfile = os.path.join(out_dir, f"fold_{fold}_test_{out_tag}_pseudocells.h5ad")
        save_pseudocells(test_out['representatives'], test_out['hvg_mask'], test_outfile)
        print(f"  âœ… saved TEST pseudo-cells  -> {test_outfile} (k={test_out['n_clusters']})")

        print(f"â± elapsed: {time.time()-t0:.1f}s")
