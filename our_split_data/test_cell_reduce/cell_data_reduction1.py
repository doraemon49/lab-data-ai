import scanpy as sc
import numpy as np
import pandas as pd

# ì¬í˜„ì„± ê³ ì •
RANDOM_SEED = 42
rng = np.random.default_rng(RANDOM_SEED)

# for repeat in range(5):
#     for fold in range(5):
#         print(f"ğŸ” Repeat {repeat}, Fold {fold}")

# adata = sc.read_h5ad('/data/project/kim89/0804_data/repeat_0/fold_0_test.h5ad')

# print(adata)
# print("âœ… obs columns:", adata.obs.columns.tolist())

# ---------------------------
# 1) ì „ì²´ì—ì„œ ë¬´ì‘ìœ„ë¡œ Nê°œë§Œ ë‚¨ê¸°ê¸°
# ---------------------------
def downsample_global(adata, n_keep=None, frac=None, random_state=RANDOM_SEED):
    """
    - n_keep: ë‚¨ê¸¸ ì…€ ê°œìˆ˜ (ì •ìˆ˜). fracê³¼ ë™ì‹œì— ì“°ì§€ ë§ ê²ƒ.
    - frac: ë‚¨ê¸¸ ë¹„ìœ¨ (0~1). n_keepê³¼ ë™ì‹œì— ì“°ì§€ ë§ ê²ƒ.
    """
    assert (n_keep is None) ^ (frac is None), "n_keep ë˜ëŠ” frac ì¤‘ í•˜ë‚˜ë§Œ ì§€ì •í•˜ì„¸ìš”."

    n_total = adata.n_obs
    if frac is not None:
        n_keep = max(1, int(round(n_total * frac)))

    n_keep = min(n_keep, n_total)  # ê³¼ë„ ì§€ì • ë°©ì§€
    idx = rng.choice(n_total, size=n_keep, replace=False)
    idx = np.sort(idx)
    adata_ds = adata[idx].copy()
    print(f"ğŸ”» Global downsample: {n_total} â†’ {adata_ds.n_obs} cells")
    return adata_ds

# ì˜ˆì‹œ: ì „ì²´ì˜ 20%ë§Œ ë‚¨ê¸°ê¸°
# adata_small = downsample_global(adata, frac=0.2)

# ì˜ˆì‹œ: ì •í™•íˆ 5,000ê°œë§Œ ë‚¨ê¸°ê¸°
# adata_small = downsample_global(adata, n_keep=5000)


# ------------------------------------------
# 2) íŠ¹ì • ì»¬ëŸ¼ë³„(ì¸µí™”)ë¡œ ê· í˜• ìˆê²Œ ìƒ˜í”Œë§
# ------------------------------------------
def downsample_by_group(adata, group_col, per_group=None, frac_per_group=None, min_per_group=1,
                        include_nan=False, random_state=RANDOM_SEED):
    """
    - group_col: obsì˜ ê·¸ë£¹ ê¸°ì¤€ ì»¬ëŸ¼ëª… (ì˜ˆ: 'manual_annotation', 'donor_id')
    - per_group: ê° ê·¸ë£¹ë³„ë¡œ ë‚¨ê¸¸ 'ê°œìˆ˜'. (ì •ìˆ˜)
    - frac_per_group: ê° ê·¸ë£¹ë³„ë¡œ ë‚¨ê¸¸ 'ë¹„ìœ¨'(0~1). per_groupì™€ ë™ì‹œì— ì“°ì§€ ë§ ê²ƒ.
    - min_per_group: ê° ê·¸ë£¹ì—ì„œ ìµœì†Œ ë³´ì¡´í•  ê°œìˆ˜(ê·¸ë£¹ í¬ê¸°ë³´ë‹¤ í´ ê²½ìš° ê·¸ë£¹ í¬ê¸°ë¡œ ìë™ ì¡°ì •)
    - include_nan: ê·¸ë£¹ ë¼ë²¨ì´ NaNì¸ ì…€ì„ í¬í•¨í• ì§€ ì—¬ë¶€
    """
    assert (per_group is None) ^ (frac_per_group is None), "per_group ë˜ëŠ” frac_per_group ì¤‘ í•˜ë‚˜ë§Œ ì§€ì •í•˜ì„¸ìš”."
    if group_col not in adata.obs.columns:
        raise ValueError(f"'{group_col}' ì»¬ëŸ¼ì´ obsì— ì—†ìŠµë‹ˆë‹¤.")

    # NaN ê·¸ë£¹ ì²˜ë¦¬
    obs = adata.obs[[group_col]].copy()
    if include_nan:
        grp_vals = obs[group_col].astype(object)  # NaN ìœ ì§€
    else:
        mask = obs[group_col].notna()
        obs = obs[mask]
        grp_vals = obs[group_col]

    groups = grp_vals.astype("category")
    df = pd.DataFrame({"group": groups, "idx": np.arange(adata.n_obs)})
    if not include_nan:
        df = df.dropna(subset=["group"])

    keep_indices = []
    for g, sub in df.groupby("group", observed=True):
        n_g = len(sub)
        if frac_per_group is not None:
            k = int(round(n_g * frac_per_group))
        else:
            k = per_group

        # ìµœì†Œ/ìµœëŒ€ í•œê³„
        k = max(min_per_group, k)
        k = min(k, n_g)

        chosen = sub.sample(n=k, random_state=random_state)["idx"].to_numpy()
        keep_indices.append(chosen)

    keep_indices = np.concatenate(keep_indices) if len(keep_indices) > 0 else np.array([], dtype=int)
    keep_indices.sort()

    adata_ds = adata[keep_indices].copy()

    # ì „/í›„ ìš”ì•½
    before = df["group"].value_counts().sort_index()
    after = pd.Series(groups.iloc[keep_indices].values).value_counts().sort_index()
    print(f"ğŸ”» Stratified downsample by '{group_col}': {adata.n_obs} â†’ {adata_ds.n_obs} cells")
    print("ğŸ“Š Before per-group:\n", before)
    print("ğŸ“Š After  per-group:\n", after)
    return adata_ds

# ì˜ˆì‹œ A: manual_annotationë³„ë¡œ ê° ê·¸ë£¹ì—ì„œ ìµœëŒ€ 300ê°œì”©ë§Œ ìœ ì§€
# adata_small = downsample_by_group(adata, group_col="manual_annotation", per_group=300, min_per_group=10)

# ì˜ˆì‹œ B: manual_annotationë³„ë¡œ ê° ê·¸ë£¹ì—ì„œ 30%ë§Œ ìœ ì§€(ìµœì†Œ 10ê°œëŠ” ë³´ì¡´)
# adata_small = downsample_by_group(adata, group_col="manual_annotation", frac_per_group=0.3, min_per_group=10)

# ì˜ˆì‹œ C: donor_idë³„ë¡œ ê° ë„ë„ˆì—ì„œ 200ê°œë§Œ ìœ ì§€
# adata_small = downsample_by_group(adata, group_col="donor_id", per_group=200, min_per_group=50)

import scanpy as sc
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from scipy import sparse

# ===== ê³µí†µ ìœ í‹¸ =====
def _to_dense(X):
    if sparse.issparse(X):
        return X.toarray()
    return np.asarray(X)

def _rowwise_mean(X):
    # X: (n, g) dense or sparse
    if sparse.issparse(X):
        return np.asarray(X.mean(axis=0)).ravel()
    else:
        return X.mean(axis=0)

def _group_mean_indices(n, chunk_size):
    # 0..n-1ë¥¼ chunk_size ë‹¨ìœ„ë¡œ ë‚˜ëˆŒ ë•Œ ì¸ë±ìŠ¤ ë¬¶ìŒ ë¦¬ìŠ¤íŠ¸
    idx = np.arange(n)
    return [idx[i:i+chunk_size] for i in range(0, n, chunk_size)]

# ===== ë°©ë²• A: KMeans + medoid(centroidì— ê°€ì¥ ê°€ê¹Œìš´ ì‹¤ì œ ì…€) ì¶”ì¶œ =====
def reduce_by_kmeans_medoid(
    adata,
    n_targets=1000,
    use_space="pca",        # "pca" | obsm í‚¤ | "gene"
    n_pcs=50,
    random_state=42
):
    """
    - n_targets: ìµœì¢… ë‚¨ê¸¸ ì…€ ê°œìˆ˜(k-meansì˜ k)
    - use_space:
        "pca"  â†’ adata.Xë¡œ PCA n_pcs êµ¬í•œ ë’¤ ê·¸ ê³µê°„ì—ì„œ KMeans
        "gene" â†’ ì›ë˜ ìœ ì „ì ê³µê°„(adata.X)ì—ì„œ KMeans (ê³ ì°¨ì›/ë¬´ê±°ìš¸ ìˆ˜ ìˆìŒ)
        ê·¸ ì™¸  â†’ adata.obsm[use_space]ë¥¼ ì‚¬ìš© (ì˜ˆ: 'X_scGPT', 'scace_emb_1.0')
    - ê° í´ëŸ¬ìŠ¤í„°ì—ì„œ centroidì— ê°€ì¥ ê°€ê¹Œìš´ ì…€(ìœ í´ë¦¬ë“œ ê±°ë¦¬ ìµœì†Œ)ì„ 1ê°œ ì„ íƒ
    """
    assert n_targets >= 1, "n_targetsëŠ” 1 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤."
    print(f"â–¶ KMeans+medoid: target={n_targets}, space={use_space}")

    # 1) ì„ë² ë”© ì¤€ë¹„
    if use_space == "pca":
        # PCAëŠ” scanpyì˜ pca ê²°ê³¼ë¥¼ ì¬ì‚¬ìš©(ì—†ìœ¼ë©´ ìƒˆë¡œ ê³„ì‚°)
        if "X_pca" not in adata.obsm_keys():
            sc.tl.pca(adata, n_comps=n_pcs, use_highly_variable=True if "highly_variable" in adata.var.columns else None)
        Z = adata.obsm["X_pca"][:, :n_pcs]
    elif use_space == "gene":
        Z = _to_dense(adata.X)
    else:
        if use_space not in adata.obsm_keys():
            raise ValueError(f"obsm['{use_space}'] ê°€ ì—†ìŠµë‹ˆë‹¤. obsm_keys={adata.obsm_keys()}")
        Z = np.asarray(adata.obsm[use_space])

    n = Z.shape[0]
    if n_targets > n:
        n_targets = n
        print(f"  (ì£¼ì˜) n_targetsê°€ ì…€ ìˆ˜ë³´ë‹¤ ì»¤ì„œ {n_targets}ë¡œ ì¡°ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # 2) KMeans
    km = KMeans(n_clusters=n_targets, random_state=random_state, n_init="auto")
    labels = km.fit_predict(Z)
    centers = km.cluster_centers_

    # 3) ê° í´ëŸ¬ìŠ¤í„°ì˜ medoid ì…€ ì„ íƒ (centroidì— ê°€ì¥ ê°€ê¹Œìš´ ì‹¤ì œ ì…€ 1ê°œ)
    chosen = []
    for k in range(n_targets):
        idx = np.where(labels == k)[0]
        if len(idx) == 0:
            continue
        sub = Z[idx]
        # centroidì™€ì˜ ê±°ë¦¬
        dists = np.linalg.norm(sub - centers[k], axis=1)
        medoid_local = np.argmin(dists)
        chosen.append(idx[medoid_local])

    chosen = np.unique(np.array(chosen, dtype=int))
    chosen.sort()
    adata_medoid = adata[chosen].copy()
    print(f"  âœ… ì„ íƒëœ ëŒ€í‘œ ì…€: {adata_medoid.n_obs}ê°œ (ì›ë³¸ {n}ê°œ)")
    return adata_medoid

# ===== ë°©ë²• B: ê·¸ë£¹ í‰ê· (pseudocell) =====
def reduce_by_group_mean(adata, group_col):
    """
    - group_col(ì˜ˆ: 'manual_annotation', 'donor_id')ë§ˆë‹¤ í‰ê·  ë°œí˜„ ë²¡í„°ë¥¼ ë§Œë“¤ì–´
      ê·¸ë£¹ë‹¹ 1ê°œ â€˜í‰ê·  ì…€â€™ì„ ìƒì„±í•©ë‹ˆë‹¤.
    - ê²°ê³¼ëŠ” ì‹¤ì œ ì…€ì´ ì•„ë‹ˆë¼ í‰ê· ìœ¼ë¡œ ë§Œë“  pseudo-cell(í–‰=ê·¸ë£¹)ì…ë‹ˆë‹¤.
    """
    if group_col not in adata.obs.columns:
        raise ValueError(f"'{group_col}' ì»¬ëŸ¼ì´ obsì— ì—†ìŠµë‹ˆë‹¤.")
    groups = adata.obs[group_col].astype(str).values
    df = pd.DataFrame({"group": groups, "idx": np.arange(adata.n_obs)})

    X = adata.X  # (n_cells, n_genes)
    mean_rows = []
    new_obs = []

    for g, sub in df.groupby("group", observed=True):
        idx = sub["idx"].to_numpy()
        Xg = X[idx]
        mg = _rowwise_mean(Xg)  # (n_genes,)
        mean_rows.append(mg)
        # ëŒ€í‘œ ë©”íƒ€ë°ì´í„°: group ì´ë¦„ê³¼ ê·¸ë£¹ ì‚¬ì´ì¦ˆë§Œ ì±„ì›Œë‘  (ì›í•˜ë©´ ëª¨ë“œê°’/ë¹„ìœ¨ ì¶”ê°€ ê°€ëŠ¥)
        new_obs.append({"group": g, "n_cells": len(idx)})

    X_mean = np.vstack(mean_rows)  # (n_groups, n_genes)
    obs_new = pd.DataFrame(new_obs).set_index(pd.Index([o["group"] for o in new_obs]))
    var_new = adata.var.copy()

    adata_group = sc.AnnData(X=X_mean, obs=obs_new, var=var_new)
    adata_group.obs_names = obs_new.index.astype(str)
    adata_group.obs[group_col] = adata_group.obs.index
    print(f"â–¶ ê·¸ë£¹ í‰ê· : ê·¸ë£¹ ìˆ˜ {adata_group.n_obs}ê°œ (ì›ë³¸ ì…€ {adata.n_obs}ê°œ â†’ í‰ê· í™”)")
    return adata_group

# ===== (ì˜µì…˜) ê·¸ë£¹ ë‚´ ê· ë“± ë¶„í•  í›„ í‰ê· : ê·¸ë£¹ë‹¹ ì—¬ëŸ¬ ê°œì˜ pseudocell ë§Œë“¤ê¸° =====
def reduce_by_group_chunked_mean(adata, group_col, chunk_size=100):
    """
    - group_colë¡œ ê·¸ë£¹í™”í•œ ë’¤, ê° ê·¸ë£¹ì„ chunk_sizeë¡œ ë‚˜ëˆ 
      chunkë§ˆë‹¤ í‰ê· ì„ ë‚´ì–´ ì—¬ëŸ¬ ê°œì˜ pseudocellì„ ë§Œë“­ë‹ˆë‹¤.
    - ì˜ˆ) group_col='donor_id', chunk_size=200 â†’ ë„ë„ˆë³„ë¡œ 200ê°œì”© í‰ê·  ë¬¶ìŒ ìƒì„±
    """
    if group_col not in adata.obs.columns:
        raise ValueError(f"'{group_col}' ì»¬ëŸ¼ì´ obsì— ì—†ìŠµë‹ˆë‹¤.")
    groups = adata.obs[group_col].astype(str).values
    df = pd.DataFrame({"group": groups, "idx": np.arange(adata.n_obs)})

    X = adata.X
    mean_rows = []
    new_obs = []
    for g, sub in df.groupby("group", observed=True):
        idx = sub["idx"].to_numpy()
        chunks = _group_mean_indices(len(idx), chunk_size)
        for j, ch in enumerate(chunks):
            sel = idx[ch]
            mg = _rowwise_mean(X[sel])
            mean_rows.append(mg)
            new_obs.append({"group": g, "chunk_id": j, "n_cells": len(sel)})

    X_mean = np.vstack(mean_rows)
    obs_new = pd.DataFrame(new_obs)
    obs_new.index = [f"{r['group']}_chunk{r['chunk_id']}" for _, r in obs_new.iterrows()]
    var_new = adata.var.copy()

    adata_chunked = sc.AnnData(X=X_mean, obs=obs_new, var=var_new)
    print(f"â–¶ ê·¸ë£¹-ë¶„í•  í‰ê· : {adata_chunked.n_obs}ê°œ pseudocell ìƒì„± (ì›ë³¸ {adata.n_obs}ê°œ)")
    return adata_chunked

# ======================
# ì‹¤ì œ í˜¸ì¶œ ì˜ˆì‹œ (ë‹¹ì‹  ë°ì´í„°)
# ======================

# [ì˜ˆì‹œ 1] KMeans + medoid: ìµœì¢… 2,000ê°œ ì…€ë§Œ ë‚¨ê¸°ë˜, PCA ê³µê°„ì—ì„œ ëŒ€í‘œ ì…€ ì¶”ì¶œ
# adata_small = reduce_by_kmeans_medoid(adata, n_targets=2000, use_space="pca", n_pcs=50)

# [ì˜ˆì‹œ 2] ì´ë¯¸ ìˆëŠ” í´ëŸ¬ìŠ¤í„° ë¼ë²¨(ì˜ˆ: 'merged_cluster_1.0')ë¡œ ê·¸ë£¹ í‰ê·  â†’ ê·¸ë£¹ë‹¹ 1ê°œ ì…€
# adata_small = reduce_by_group_mean(adata, group_col="merged_cluster_1.0")

# [ì˜ˆì‹œ 3] ë„ë„ˆë³„ë¡œ 200ê°œì”© ë¬¶ì–´ í‰ê·  â†’ ë„ë„ˆ x (ì…€ìˆ˜/200) ê°œì˜ pseudocell ìƒì„±
# adata_small = reduce_by_group_chunked_mean(adata, group_col="donor_id", chunk_size=200)

# ì €ì¥ (ì›í•˜ì‹¤ ë•Œ)
# adata_small.write('/data/project/kim89/0804_data/repeat_0/fold_0_test_reduced.h5ad')

# ---------------------------
# ê²°ê³¼ ì €ì¥ (ì˜µì…˜)
# ---------------------------
for repeat in range(5):
    for fold in range(5):
        print(f"ğŸ” Repeat {repeat}, Fold {fold}")

        adata_train = sc.read_h5ad(f"/data/project/kim89/0804_data/repeat_{repeat}/fold_{fold}_train.h5ad")
        adata_test = sc.read_h5ad(f"/data/project/kim89/0804_data/repeat_{repeat}/fold_{fold}_test.h5ad")

        # ì‹¤í–‰
        adata_small = reduce_by_group_mean(adata_train, group_col="manual_annotation")
        sc.write(f"/data/project/kim89/0804_data_cell_reduce/repeat_{repeat}/fold_{fold}_test.h5ad", adata_small)

        adata_small = reduce_by_group_mean(adata_test, group_col="manual_annotation")
        sc.write(f"/data/project/kim89/0804_data_cell_reduce/repeat_{repeat}/fold_{fold}_test.h5ad", adata_small)

