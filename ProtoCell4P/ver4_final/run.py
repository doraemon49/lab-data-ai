# run.py
import argparse
from config import Config
from utils import set_global_seed
import optuna
from utils import train_with_oom_retry
import numpy as np
import os
from torchmetrics.classification import BinaryAUROC, MulticlassAUROC
from copy import deepcopy  # ìœ„ìª½ importì— ì—†ìœ¼ë©´ ì¶”ê°€

# (íŒŒì´ì¬ 3.7+ ì´ìƒ) stdoutì„ ì¤„ë‹¨ìœ„ ë²„í¼ë§ìœ¼ë¡œ ë³€ê²½
try:
    sys.stdout.reconfigure(line_buffering=True)
except Exception:
    pass

# ì´ˆê¸°í™” ì‹œ 1íšŒ
import torch
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
try:
    torch.set_float32_matmul_precision("high")
except: pass

from torch.cuda.amp import autocast, GradScaler
scaler = GradScaler(enabled=True)


HP_SPACE = {
    "lr":        [1e-4, 1e-3, 1e-2],
    "max_epoch": [50, 75, 100],
    "z_dim":     [32, 64, 128],  # encoder/decoder ì¶œë ¥ ì°¨ì›
    "h_dim":     [8, 16, 32],
    "n_proto":   [8, 16, 32],
    # pretrain epochì€ ê³ ì • {75}
}
# === run.py ìƒë‹¨ì— ì¶”ê°€ ===
import torch
from torch.utils.data import DataLoader
from copy import deepcopy
# run.py
# run.py ìƒë‹¨
from types import SimpleNamespace


def make_trial_cfg(base_cfg, hp):
    tcfg = SimpleNamespace(
        # íŠœë‹ ëŒ€ìƒ
        lr=hp["lr"],
        max_epoch=hp["max_epoch"],
        z_dim=hp["z_dim"],
        h_dim=hp["h_dim"],
        n_proto=hp["n_proto"],

        # ê³ ì •/ë³µì‚¬
        n_layers=getattr(base_cfg, "n_layers", 2),
        batch_size=getattr(base_cfg, "batch_size", 512),
        device=getattr(base_cfg, "device", "cuda:0"),
        d_min=getattr(base_cfg, "d_min", 1),
        lr_pretrain=getattr(base_cfg, "lr_pretrain", 1e-2),
        max_epoch_pretrain=75,
        keep_sparse=getattr(base_cfg, "keep_sparse", True),
        test_step=getattr(base_cfg, "test_step", 1),

        # â˜… ì¶”ê°€: lambdas, n_ct, seedê¹Œì§€ ë³µì‚¬
        lambdas=getattr(base_cfg, "lambdas", None),
        n_ct=getattr(base_cfg, "n_ct", None),
        seed=getattr(base_cfg, "seed", None),

        # â˜… ê²½ë¡œ í•„ìˆ˜ ë³µì‚¬
        checkpoint_dir=getattr(base_cfg, "checkpoint_dir", None),
        log_dir=getattr(base_cfg, "log_dir", None),
    )
    # í•„ìš”ì‹œ ë””ë ‰í† ë¦¬ ë³´ì¥
    if tcfg.checkpoint_dir:
        os.makedirs(tcfg.checkpoint_dir, exist_ok=True)
    if tcfg.log_dir:
        os.makedirs(tcfg.log_dir, exist_ok=True)
    return tcfg


def make_loaders_from_cfg(cfg, batch_size, phase):
    """
    cfg.train_set / cfg.val_set / (ì˜µì…˜) cfg.test_set ì„ ì‚¬ìš©.
    - I/O ë³‘ëª© ì™„í™”: num_workers, prefetch_factor, persistent_workers, pin_memory
    - í•™ìŠµ ì•ˆì •/ì†ë„: drop_last(True), ê³ ì • collate_fn ê·¸ëŒ€ë¡œ ì‚¬ìš©
    - subsample ì˜µì…˜ì€ ê¸°ì¡´ ë¡œì§ ìœ ì§€ (trainë§Œ ë°°ìˆ˜ í™•ì¥)
    """

    # CPU ì½”ì–´ ê¸°ë°˜ í•©ë¦¬ì  ê¸°ë³¸ê°’ (í•„ìš”ì‹œ cfgì—ì„œ ì˜¤ë²„ë¼ì´ë“œ ê°€ëŠ¥)
    nw = int(min(max(os.cpu_count() or 4, 4), 8))               # 4~8 ê¶Œì¥
    pf = int(getattr(cfg, "prefetch_factor", 4))                # 2â†’4 íš¨ê³¼ í¼
    bs_train = batch_size if not getattr(cfg, "subsample", False) else 8 * batch_size
    bs_eval  = int(getattr(cfg, "batch_size", batch_size))      # ê¸°ì¡´ ìœ ì§€

    # persistent_workersëŠ” workerê°€ 1ê°œ ì´ìƒì¼ ë•Œë§Œ True (PyTorch ê¶Œì¥)
    common_args = dict(
        collate_fn      = getattr(cfg, "collate_fn", None),
        num_workers     = nw,
        pin_memory      = True,                      # H2D ë³µì‚¬ ì†ë„ ì¦ê°€
        persistent_workers = (nw > 0),
        prefetch_factor = pf
    )

    train_loader = DataLoader(
        cfg.train_set,
        batch_size = bs_train,
        shuffle    = True,
        drop_last  = True,                           # ë§ˆì§€ë§‰ ì‘ì€ ë°°ì¹˜ ë°©ì§€ â†’ ì†ë„/ì•ˆì • â†‘
        **common_args
    )
    val_loader = DataLoader(
        cfg.val_set,
        batch_size = bs_eval,
        shuffle    = False,
        drop_last  = False,
        **common_args
    )

    loaders = {"train": train_loader, "val": val_loader}

    if phase == "valid_plus_test" and hasattr(cfg, "test_set") and cfg.test_set is not None:
        test_loader = DataLoader(
            cfg.test_set,
            batch_size = bs_eval,
            shuffle    = False,
            drop_last  = False,
            **common_args
        )
        loaders["test"] = test_loader

    return loaders


def build_model_and_optim(tcfg, cfg, phase="valid_only"):
    """
    tcfg: trialìš© í•˜ì´í¼íŒŒë¼ë¯¸í„°ê°€ ë°˜ì˜ëœ Config (z_dim/h_dim/n_proto/lr/max_epoch ë“± ë³€ê²½)
    cfg : ë°ì´í„° split/ë©”íƒ€ë¥¼ ê°€ì§„ ì›ë³¸ Config (ì…ë ¥ì°¨ì›, í´ë˜ìŠ¤ìˆ˜ ë“± ì¶”ì¶œ)
    """
    # input_dim / n_classes / n_ctëŠ” cfg._build()ì—ì„œ ì´ë¯¸ ê³„ì‚°ë¨
    input_dim   = cfg.model.input_dim     # ProtoCellì´ ì´ë¯¸ í•œë²ˆ ë§Œë“¤ì–´ì ¸ ìˆì–´ ì…ë ¥ì°¨ì› ë³´ìœ 
    n_classes   = cfg.n_classes
    n_ct        = cfg.n_ct
    lambdas     = cfg.lambdas
    # n_layersë¥¼ tcfgì—ì„œ ì½ë˜, ì—†ìœ¼ë©´ cfgì˜ ê°’ì„ ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš©
    n_layers = getattr(tcfg, "n_layers", getattr(cfg, "n_layers", 2))

    # fresh model ìƒì„± (trial HP ë°˜ì˜)
    from model import ProtoCell, BaseModel
    if cfg.model_type == "ProtoCell":
        model = ProtoCell(input_dim, tcfg.h_dim, tcfg.z_dim, n_layers,
                          tcfg.n_proto, n_classes, lambdas, n_ct,
                          tcfg.device, d_min=tcfg.d_min)
    elif cfg.model_type == "BaseModel":
        model = BaseModel(input_dim, tcfg.h_dim, tcfg.z_dim, n_layers,
                          tcfg.n_proto, n_classes, lambdas, n_ct,
                          tcfg.device, d_min=tcfg.d_min)
    else:
        raise ValueError(f"Model {cfg.model_type} not supported")

    model.to(tcfg.device)
    optimizer = torch.optim.Adam(model.parameters(), lr=tcfg.lr)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)

    loaders = make_loaders_from_cfg(cfg, batch_size=tcfg.batch_size, phase=phase)
    meta = dict(input_dim=input_dim, n_classes=n_classes, n_ct=n_ct)
    return model, optimizer, scheduler, loaders, meta
import os, math, torch
from torch.cuda.amp import autocast, GradScaler

"""
def run_pretrain(model, loaders, tcfg, logger=None, keep_sparse=True):
"""
    # Pretrain ë‹¨ê³„ (AMP+GradScaler ì ìš©, ê²€ì¦ì€ no_grad).
    # - ìµœì í™”ì—ëŠ” 'loss'ë§Œ í•„ìš”. logits/ct_logitì€ ì§€í‘œ ê³„ì‚°í•  ë•Œë§Œ ì‚¬ìš©.
    # - ë©”ëª¨ë¦¬ í”¼í¬ê°€ í¬ë©´ pretrain ì „ìš© batch í¬ê¸° ì¤„ì´ëŠ” ê²ƒë„ ì¶”ì²œ.
"""
    device = torch.device(tcfg.device)
    model.train()
    optim_pre = torch.optim.Adam(model.parameters(), lr=tcfg.lr_pretrain)
    scaler = GradScaler()

    best_metric = None  # (val ê¸°ì¤€) ë†’ì„ìˆ˜ë¡ ì¢‹ê²Œ: ì—¬ê¸°ì„œëŠ” -val_lossë¥¼ ì“°ë¯€ë¡œ ê°’ì´ í´ìˆ˜ë¡ ì¢‹ìŒ

    for epoch in range(tcfg.max_epoch_pretrain):
        model.train()
        train_loss_sum = torch.zeros((), device=device)
        train_count = 0

        for bat in loaders["train"]:
            # ë°°ì¹˜ -> GPU
            bat = _move_to_device(bat, device)

            optim_pre.zero_grad(set_to_none=True)
            # === í•™ìŠµ: AMP ì¼œê³  forward ===
            with torch.autocast("cuda", dtype=torch.bfloat16):  # PyTorch 2.x: torch.autocast("cuda")ë„ ê°€ëŠ¥
                out = model.pretrain(*bat, sparse=keep_sparse)
                loss = out[0] if isinstance(out, (tuple, list)) else out  # âœ… lossë§Œ ìˆìœ¼ë©´ ì¶©ë¶„

            # === backward & step (GradScaler) ===
            scaler.scale(loss).backward(); scaler.step(optim_pre); scaler.update()


            # ë¡œê¹…ìš©
            # batch_size = len(bat[0]) if hasattr(bat[0], "__len__") else bat[0].size(0)
            train_loss_sum += loss.detach() * batch_size
            train_count    += batch_size

        avg_train_loss = (train_loss_sum / max(1, train_count)).item()

        # ë©”ëª¨ë¦¬ ìƒí™©(ì„ íƒ) â€” reservedëŠ” nvidia-smiì™€ ìœ ì‚¬, allocatedëŠ” ì‹¤ì œ í…ì„œ ì ìœ 
        try:
            alloc = torch.cuda.max_memory_allocated(device) / 1024**2
            reserv = torch.cuda.max_memory_reserved(device) / 1024**2
            torch.cuda.reset_peak_memory_stats(device)
            print(f"[Pretrain][epoch {epoch}] train_loss={avg_train_loss:.4f} | "
                  f"alloc={alloc:.1f}MB, reserved={reserv:.1f}MB", flush=True)
        except Exception:
            print(f"[Pretrain][epoch {epoch}] train_loss={avg_train_loss:.4f}", flush=True)


        # === Validation (ì£¼ê¸°ì ) ===
        if (epoch + 1) % tcfg.test_step == 0:
            model.eval()
            val_count = 0
            # ğŸ”¹GPUì—ì„œ ëˆ„ì (ì¤‘ê°„ì— .item() ê¸ˆì§€)
            val_loss_sum = torch.zeros((), device=device)

            with torch.inference_mode(), torch.autocast("cuda", dtype=torch.bfloat16):
                for bat in loaders["val"]:
                    bat = _move_to_device(bat, device)
                    out = model.pretrain(*bat, sparse=keep_sparse)
                    vloss = out[0] if isinstance(out, (tuple, list)) else out

                    batch_size = bat[0].shape[0] if hasattr(bat[0], "shape") else len(bat[0])
                    val_loss_sum += vloss.detach() * batch_size
                    val_count    += batch_size

            avg_val_loss = (val_loss_sum / max(1, val_count)).item()
            curr_metric  = -avg_val_loss

            print(f"[Pretrain][VAL] epoch {epoch} | val_loss={avg_val_loss:.4f}", flush=True)

            if (best_metric is None) or (curr_metric > best_metric):
                ckpt_dir = getattr(tcfg, "checkpoint_dir", None)
                if ckpt_dir:
                    os.makedirs(ckpt_dir, exist_ok=True)
                    torch.save(model.state_dict(), os.path.join(ckpt_dir, "pretrain.pt"))
                best_metric = curr_metric
            model.train()

"""
# utils.py ë“± _move_to_device ì •ì˜ë¶€
# import torch

def _move_to_device(bat, device):
    def mv(t):
        # í…ì„œë§Œ ë¹„ë™ê¸° ë³µì‚¬
        return t.to(device, non_blocking=True) if torch.is_tensor(t) else t

    if isinstance(bat, (list, tuple)):
        return tuple(mv(t) for t in bat)          # (x, y) ë˜ëŠ” (x, y, ct) êµ¬ì¡° ìœ ì§€
    elif isinstance(bat, dict):
        return {k: mv(v) for k, v in bat.items()} # {"x":..., "y":..., "ct":...}
    else:
        return mv(bat)


def run_train_eval_only_valid(model, loaders, tcfg, logger=None, keep_sparse=True):

    """
    train + validë§Œìœ¼ë¡œ í•™ìŠµí•˜ê³  valid AUC(ë˜ëŠ” val loss ë“±)ë¥¼ ë°˜í™˜ â†’ Optuna objectiveì—ì„œ ì‚¬ìš©.
    (ê¸°ì¡´ train() ë£¨í”„ ë¡œì§ì„ ìš”ì•½ ì´ì‹. ì§€í‘œ ê³„ì‚°ë²•ì€ config.pyì™€ ë™ì¼) 
    """
    print("train + validë§Œìœ¼ë¡œ í•™ìŠµí•˜ê³  valid AUCë¥¼ ë°˜í™˜ â†’ Optuna objectiveì—ì„œ ì‚¬ìš©.")
    print(f"effective batch size: {tcfg.batch_size}")

    import time
    import torch
    from sklearn.metrics import roc_auc_score

    print("current_device:", torch.cuda.current_device(), torch.cuda.get_device_name(torch.cuda.current_device()))
    print("tcfg.device:", tcfg.device)
    # ëª¨ë¸ íŒŒë¼ë¯¸í„° ì‹¤ì œ ì¥ì¹˜ í™•ì¸
    print({p.device for p in model.parameters()})
    # ë©”ëª¨ë¦¬ í†µê³„ëŠ” ì¥ì¹˜ë¥¼ ê¼­ ëª…ì‹œ
    torch.cuda.reset_peak_memory_stats(device=torch.device(tcfg.device))
    

    optim = torch.optim.Adam(model.parameters(), lr=tcfg.lr)
    pretrained = getattr(tcfg, "pretrained", False)
    scaler = GradScaler(enabled=True)
    best_metric = -1e18
    
    best_auc, stale = -1, 0
    best_state = None  # âœ… ì¶”ê°€

    # íŠœë‹ ì „ìš© ê²€ì¦ ì£¼ê¸° íŒŒë¼ë¯¸í„° (tcfgì— ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
    warmup_epochs = getattr(tcfg, "val_warmup_epochs", 1)
    eval_period   = getattr(tcfg, "val_period", 5)

    for epoch in range(tcfg.max_epoch):
        # =======================
        # Train
        # =======================
        model.train()
        train_loss_sum = 0.0
        train_count = 0

        for bat in loaders["train"]:
            bat = _move_to_device(bat, tcfg.device)
            # optim.zero_grad()
            optim.zero_grad(set_to_none=True)
            with torch.autocast(device_type="cuda", dtype=torch.bfloat16):

                # loss, logits = model(*bat, sparse=keep_sparse)

                if len(bat) == 3:  # (x, y, ct)
                    loss, logits, ct_logit = model(*bat, sparse=keep_sparse)
                    # if not pretrained:
                    #     loss = loss + model.pretrain(*bat, sparse=keep_sparse)[0]  # (loss, ct_logit)
                else:              # (x, y)
                    loss, logits = model(*bat, sparse=keep_sparse)
                    # if not pretrained:
                    #     loss = loss + model.pretrain(*bat, sparse=keep_sparse)    # loss only
            
            scaler.scale(loss).backward()
            scaler.step(optim)
            scaler.update()

            # loss.backward()
            # optim.step()

            # í‰ê· ì„ ìœ„í•´ ìƒ˜í”Œ ìˆ˜ ê¸°ì¤€ ëˆ„ì 
            batch_n = len(bat[0])
            train_loss_sum += loss.item() * batch_n
            train_count += batch_n

        train_loss_avg = train_loss_sum / max(1, train_count)

            # optimizerëŠ” ë°”ê¹¥ì—ì„œ ë°›ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ ê°„ë‹¨í™”ë¥¼ ìœ„í•´ ìƒˆë¡œ ì„ ì–¸
        # ê°„ë‹¨í™”ë¥¼ í”¼í•˜ë ¤ë©´ build_model_and_optimì—ì„œ optimizerë¥¼ ë°˜í™˜ë°›ì•„ ì—¬ê¸°ì„œ ì‚¬ìš©í•˜ì„¸ìš”.
        # (ì•„ë˜ ìµœì¢… ì½”ë“œ ë¸”ë¡ì—ì„œëŠ” optimizerë¥¼ í•¨ê»˜ ì‚¬ìš©)

        # valid
        # =======================
        # Validation (GPU fast)
        # =======================
        # -----------------------
        # Validation ì£¼ê¸° ì œì–´
        # -----------------------
        eval_now = (
            epoch < warmup_epochs                                   # warmup: ë§¤ ì—í­ ê²€ì¦
            or ((epoch + 1) % eval_period == 0)                     # ì´í›„: ì£¼ê¸°ì  ê²€ì¦
            or (epoch == tcfg.max_epoch - 1)                        # ë§ˆì§€ë§‰ ì—í­ì€ ë¬´ì¡°ê±´
        )
        if eval_now:
            # =======================
            # Validation (GPU fast)
            # =======================
            model.eval()
            val_count = 0
            val_loss_sum = torch.zeros((), device=tcfg.device)

            from torchmetrics.classification import BinaryAUROC, MulticlassAUROC
            n_classes = tcfg.n_classes if hasattr(tcfg, "n_classes") else loaders.get("n_classes", None)
            if n_classes is None:
                _bat = next(iter(loaders["val"]))
                n_classes = int(model(*_bat, sparse=keep_sparse)[1].shape[1])

            if n_classes == 2:
                auroc = BinaryAUROC().to(tcfg.device)
            else:
                auroc = MulticlassAUROC(num_classes=n_classes, average="macro").to(tcfg.device)

            with torch.inference_mode(), torch.autocast("cuda", dtype=torch.bfloat16):
                for bat in loaders["val"]:
                    bat = _move_to_device(bat, tcfg.device)
                    if len(bat) == 3:
                        loss, logits, _ = model(*bat, sparse=keep_sparse)
                    else:
                        loss, logits = model(*bat, sparse=keep_sparse)
                    batch_n = len(bat[0])
                    val_count += batch_n
                    val_loss_sum += loss.detach() * batch_n

                    if n_classes == 2:
                        probs_pos = torch.softmax(logits, dim=1)[:, 1]
                        auroc.update(probs_pos, bat[1])
                    else:
                        probs = torch.softmax(logits, dim=1)
                        auroc.update(probs, bat[1])

            val_loss_avg = (val_loss_sum / max(1, val_count)).item()
            val_auc = auroc.compute().item()
            print(f"[epoch {epoch}] train_loss={train_loss_avg:.4f} "
                f"val_loss={val_loss_avg:.4f} val_auc={val_auc:.4f}", flush=True)

            # early-stoppingìš©
            if val_auc > best_auc + 1e-4:
                best_auc, stale = val_auc, 0
                best_state = deepcopy(model.state_dict())
            else:
                stale += 1
            if stale >= 1:
                # ì¡°ê¸° ì¢…ë£Œ
                print(f"[epoch {epoch}] early stop: stale={stale}, best_auc={best_auc:.4f}", flush=True)
                break

            curr_metric = val_auc

            if curr_metric > best_metric:
                best_metric = curr_metric

            # ë©”ëª¨ë¦¬ ë¡œê·¸ (ì„ íƒ)
            print("max_alloc(MB)=", torch.cuda.max_memory_allocated()/1024**2)
            torch.cuda.reset_peak_memory_stats()
            print("max_alloc(MB) DEVICE =", torch.cuda.max_memory_allocated(device=torch.device(tcfg.device))/1024**2, flush=True)
            print("reserved(MB) =", torch.cuda.max_memory_reserved(device=torch.device(tcfg.device))/1024**2)
            torch.cuda.reset_peak_memory_stats(device=torch.device(tcfg.device))

        else:
            # ê²€ì¦ì„ ìŠ¤í‚µí•œ ì—í­: í›ˆë ¨ ë¡œê·¸ë§Œ ê°„ë‹¨íˆ
            print(f"[epoch {epoch}] train_loss={train_loss_avg:.4f} (validation skipped)", flush=True)

    return best_metric

# def run_train_eval_only_valid(model, loaders, tcfg, logger=None, keep_sparse=True):
#     """
#     íŠœë‹ ë‹¨ê³„ ì „ìš©:
#       - ì§€ì •ëœ epoch ë™ì•ˆ 'train'ë§Œ ìˆ˜í–‰
#       - ë§ˆì§€ë§‰ epoch ì—ì„œ 'val' 1íšŒë§Œ ìˆ˜í–‰í•˜ì—¬ AUC ê³„ì‚°/ë°˜í™˜
#     """
#     print("trainë§Œ ìˆ˜í–‰í•˜ê³  ë§ˆì§€ë§‰ epochì—ë§Œ validation AUC ê³„ì‚° â†’ Optuna objectiveìš©")
#     print(f"effective batch size: {tcfg.batch_size}")

#     import torch
#     from torch.cuda.amp import GradScaler
#     from torchmetrics.classification import BinaryAUROC, MulticlassAUROC

#     # ë””ë°”ì´ìŠ¤ ì •ë³´/ë©”ëª¨ë¦¬ ì´ˆê¸°í™” ë¡œê·¸
#     print("current_device:", torch.cuda.current_device(), torch.cuda.get_device_name(torch.cuda.current_device()))
#     print("tcfg.device:", tcfg.device)
#     print({p.device for p in model.parameters()})
#     torch.cuda.reset_peak_memory_stats(device=torch.device(tcfg.device))

#     optim = torch.optim.Adam(model.parameters(), lr=tcfg.lr)
#     scaler = GradScaler(enabled=True)

#     best_metric = -1e18  # ë§ˆì§€ë§‰ ì—í­ì—ì„œ ê°±ì‹ ë  ì˜ˆì •

#     for epoch in range(tcfg.max_epoch):
#         # =======================
#         # Train
#         # =======================
#         model.train()
#         train_loss_sum = 0.0
#         train_count = 0

#         for bat in loaders["train"]:
#             bat = _move_to_device(bat, tcfg.device)
#             optim.zero_grad(set_to_none=True)

#             with torch.autocast(device_type="cuda", dtype=torch.bfloat16):
#                 if len(bat) == 3:  # (x, y, ct)
#                     loss, logits, _ = model(*bat, sparse=keep_sparse)
#                 else:
#                     loss, logits = model(*bat, sparse=keep_sparse)

#             scaler.scale(loss).backward()
#             scaler.step(optim)
#             scaler.update()

#             batch_n = len(bat[0])
#             train_loss_sum += loss.item() * batch_n
#             train_count += batch_n

#         train_loss_avg = train_loss_sum / max(1, train_count)

#         # =======================
#         # ë§ˆì§€ë§‰ ì—í­ì—ì„œë§Œ Validation
#         # =======================
#         if epoch == tcfg.max_epoch - 1:
#             model.eval()
#             val_count = 0
#             val_loss_sum = torch.zeros((), device=tcfg.device)

#             # í´ë˜ìŠ¤ ìˆ˜ í™•ì¸
#             n_classes = getattr(tcfg, "n_classes", None)
#             if n_classes is None:
#                 _bat0 = next(iter(loaders["val"]))
#                 with torch.inference_mode(), torch.autocast("cuda", dtype=torch.bfloat16):
#                     _out0 = model(*_move_to_device(_bat0, tcfg.device), sparse=keep_sparse)
#                 n_classes = int(_out0[1].shape[1])

#             # torchmetrics (ë©€í‹°í´ë˜ìŠ¤: OvR ë°©ì‹)
#             if n_classes == 2:
#                 auroc = BinaryAUROC().to(tcfg.device)
#             else:
#                 auroc = MulticlassAUROC(num_classes=n_classes, average="macro").to(tcfg.device)

#             with torch.inference_mode(), torch.autocast("cuda", dtype=torch.bfloat16):
#                 for bat in loaders["val"]:
#                     bat = _move_to_device(bat, tcfg.device)
#                     if len(bat) == 3:
#                         vloss, logits, _ = model(*bat, sparse=keep_sparse)
#                     else:
#                         vloss, logits = model(*bat, sparse=keep_sparse)

#                     batch_n = len(bat[0])
#                     val_count += batch_n
#                     val_loss_sum += vloss.detach() * batch_n

#                     if n_classes == 2:
#                         probs_pos = torch.softmax(logits, dim=1)[:, 1]
#                         auroc.update(probs_pos, bat[1])
#                     else:
#                         probs = torch.softmax(logits, dim=1)
#                         auroc.update(probs, bat[1])

#             val_loss_avg = (val_loss_sum / max(1, val_count)).item()
#             val_auc = auroc.compute().item()
#             best_metric = val_auc  # íŠœë‹ ë°˜í™˜ ì§€í‘œ

#             print(f"[epoch {epoch}] "
#                   f"train_loss={train_loss_avg:.4f} "
#                   f"val_loss={val_loss_avg:.4f} val_auc={val_auc:.4f}", flush=True)

#             # (ì„ íƒ) ë©”ëª¨ë¦¬ ë¡œê·¸
#             print("max_alloc(MB)=", torch.cuda.max_memory_allocated()/1024**2)
#             torch.cuda.reset_peak_memory_stats()
#             print("max_alloc(MB) DEVICE =", torch.cuda.max_memory_allocated(device=torch.device(tcfg.device))/1024**2, flush=True)
#             print("reserved(MB) =", torch.cuda.max_memory_reserved(device=torch.device(tcfg.device))/1024**2)
#             torch.cuda.reset_peak_memory_stats(device=torch.device(tcfg.device))
#         else:
#             # ì¤‘ê°„ ì—í­ì€ ê²€ì¦ ìŠ¤í‚µ
#             print(f"[epoch {epoch}] train_loss={train_loss_avg:.4f} (validation skipped)", flush=True)

#     return best_metric

"""
def run_train_and_test(model, loaders, optimizer, tcfg, keep_sparse=True):
"""
    # train + validë¡œ í•™ìŠµ(ë² ìŠ¤íŠ¸ ëª¨ë¸) í›„ test í‰ê°€ë¥¼ ë°˜í™˜
    # ë°˜í™˜: (auc, acc, cm, recall, precision) í˜•íƒœ
    # (ì§€í‘œ ê³„ì‚°ì€ config.pyì˜ í…ŒìŠ¤íŠ¸ íŒŒíŠ¸ì™€ ë™ì¼) 
"""

    from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, confusion_matrix, recall_score, precision_score
    
    device = tcfg.device
    model.to(device)

    pretrained = getattr(tcfg, "pretrained", False)
    best_metric = -1e18
    best_state = None

    # === í•™ìŠµ ===
    for epoch in range(tcfg.max_epoch):
        model.train()
        train_loss_sum, train_count = 0.0, 0

        for bat in loaders["train"]:
            bat = _move_to_device(bat, device)
            # optimizer.zero_grad()
            optimizer.zero_grad(set_to_none=True)
            with torch.autocast(device_type="cuda", dtype=torch.bfloat16):
                # loss, logits = model(*bat, sparse=keep_sparse)
                if len(bat) == 3:  # (x, y, ct)
                    loss, logits, ct_logit = model(*bat, sparse=keep_sparse)
                    # if not pretrained:
                    #     loss = loss + model.pretrain(*bat, sparse=keep_sparse)[0]  # (loss, ct_logit)
                else:              # (x, y)
                    loss, logits = model(*bat, sparse=keep_sparse)
            
            # out = model(*bat, sparse=keep_sparse)
            # loss, logits = out[0], out[1]                 # CTê°€ ìˆì–´ë„ ì•ˆì „
            # if not pretrained:
            #     # pretrain() ë°˜í™˜: CT ìˆìœ¼ë©´ (loss, ct_logit), ì—†ìœ¼ë©´ loss
            #     # ë³¸ í•™ìŠµ ì¤‘ì—ë„ pretrain ì†ì‹¤ì„ ì¶”ê°€ë¡œ ë„£ì„ì§€ ì—¬ë¶€ë¥¼ ê²°ì •í•˜ëŠ” í”Œë˜ê·¸ì˜ˆìš”.
            #     # tcfg.pretrained=Trueë©´ â†’ ì´ë¯¸ ë³„ë„ë¡œ pretrain ë‹¨ê³„ë¥¼ ë§ˆì¹œ ìƒíƒœë¼ê³  ê°€ì • â†’ ë³¸í•™ìŠµì—ì„œëŠ” cross-entropy ë¶„ë¥˜ ì†ì‹¤ë§Œ ì”€.
            #     # tcfg.pretrained=Falseë©´ â†’ ì‚¬ì „í•™ìŠµì„ ì•ˆ í–ˆë‹¤ê³  ê°€ì • â†’ ë³¸í•™ìŠµ ê³¼ì •ì—ì„œë„ ë§¤ ë°°ì¹˜ë§ˆë‹¤ pretrain ì†ì‹¤ì„ í•¨ê»˜ ì¶”ê°€í•˜ì—¬ latent spaceë¥¼ ê°™ì´ ì •ëˆ.
            #     pre = model.pretrain(*bat, sparse=keep_sparse)
            #     loss = loss + (pre[0] if isinstance(pre, (list, tuple)) else pre)

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            # loss.backward()
            # optimizer.step()

            batch_n = len(bat[0])
            train_loss_sum += loss.item() * batch_n
            train_count += batch_n

        avg_train_loss = train_loss_sum / max(1, train_count)

        # === ê²€ì¦ ===
        model.eval()
        vloss_sum, vcount = 0.0, 0
        y_truth_list, y_score_list = [], []

        with torch.inference_mode():
            for bat in loaders["val"]:
                bat = _move_to_device(bat, tcfg.device)  # â† ì—¬ê¸°ì„œ non_blocking ì´ë™
                with torch.autocast("cuda"):
                    out = model(*bat, sparse=keep_sparse)
                    loss, logits = out[0], out[1]

                batch_n = len(bat[0])
                vloss_sum += loss.item() * batch_n
                vcount    += batch_n

                y_truth_list.append(bat[1].detach().cpu())
                logits_cpu = logits.float().cpu()                 # â† GPUâ†’CPU í›„ softmax
                y_score_list.append(torch.softmax(logits_cpu, dim=1))

        avg_val_loss = vloss_sum / max(1, vcount)


        y_truth = torch.cat(y_truth_list)
        y_score = torch.cat(y_score_list)
        y_pred = y_score.argmax(dim=1)

        if y_score.shape[1] == 2:
            val_auc = roc_auc_score(y_truth.numpy(), y_score.numpy()[:, 1])
        else:
            val_auc = roc_auc_score(y_truth.numpy(), y_score.numpy(), multi_class="ovo")
        val_acc = accuracy_score(y_truth.numpy(), y_pred.numpy())
        val_f1  = f1_score(y_truth.numpy(), y_pred.numpy(), average="macro")

        print(
            f"[Train+Valid] Epoch {epoch+1}/{tcfg.max_epoch} "
            f"| TrainLoss={avg_train_loss:.4f} "
            f"| ValLoss={avg_val_loss:.4f} "
            f"| ValAUC={val_auc:.4f} "
            f"| ValACC={val_acc:.4f} "
            f"| ValF1={val_f1:.4f}",
            flush=True
        )

        if val_auc > best_metric:
            from copy import deepcopy
            best_metric = val_auc
            best_state = deepcopy(model.state_dict())


    # === best ë¡œë“œ í›„ test ===
    if best_state is not None:
        model.load_state_dict(best_state)

    model.eval()
    y_truth_list, y_score_list = [], []
    with torch.inference_mode():
        for bat in loaders["test"]:
            bat = _move_to_device(bat, tcfg.device)
            with torch.autocast("cuda"):
                out = model(*bat, sparse=keep_sparse)
                logits = out[1]

            y_truth_list.append(bat[1].detach().cpu())
            logits_cpu = logits.float().cpu()                 # â† ì—¬ê¸°ì„œë„ CPUì—ì„œ softmax
            y_score_list.append(torch.softmax(logits_cpu, dim=1))


    import torch as _torch
    y_truth = _torch.cat(y_truth_list)
    y_score = _torch.cat(y_score_list)
    y_pred = y_score.argmax(dim=1)

    acc = accuracy_score(y_truth.numpy(), y_pred.numpy())
    if y_score.shape[1] == 2:
        auc = roc_auc_score(y_truth.numpy(), y_score.numpy()[:, 1])
    else:
        auc = roc_auc_score(y_truth.numpy(), y_score.numpy(), multi_class="ovo")
    cm = confusion_matrix(y_truth.numpy(), y_pred.numpy())
    recall = recall_score(y_truth.numpy(), y_pred.numpy(), average="macro")
    precision = precision_score(y_truth.numpy(), y_pred.numpy(), average="macro")

    import numpy as _np
    cm_str = _np.array2string(cm, separator=' ', max_line_width=120)
    print(
        f"[Test] AUC={auc:.4f} | ACC={acc:.4f} | RECALL={recall:.4f} | PREC={precision:.4f}\n"
        f"[Test] Confusion Matrix (rows=true, cols=pred):\n{cm_str}",
        flush=True
    )

    return auc, acc, cm, recall, precision
"""

def tune_one_fold(cfg):
    """
    cfg: í˜„ì¬ foldì˜ ë°ì´í„° splitì„ ë‹´ì€ Config (cfg._build() ì™„ë£Œ ìƒíƒœ)
    ë‚´ë¶€ì—ì„œ trialìš© tcfgë¥¼ ë§Œë“¤ì–´ ëª¨í˜•ì„ freshí•˜ê²Œ ìƒì„± â†’ valid AUC ìµœëŒ€í™”ë¥¼ ëª©í‘œë¡œ íƒìƒ‰
    """
    def objective(trial):
        hp = suggest(trial)
        tcfg = make_trial_cfg(cfg, hp)    # âœ… ê°€ë²¼ìš´ ì„¤ì • ê°ì²´ ìƒì„±
        tcfg.lr        = hp["lr"]
        tcfg.max_epoch = hp["max_epoch"]
        tcfg.z_dim     = hp["z_dim"]
        tcfg.h_dim     = hp["h_dim"]
        tcfg.n_proto   = hp["n_proto"]
        tcfg.max_epoch_pretrain = 75  # ê³ ì •
        print("\n ì‹œë„í•´ë³¼ hyperparameter",hp)

        def _call():
            model, optim, sched, loaders, meta = build_model_and_optim(tcfg, cfg, phase="valid_only")
            # pretrain
            # if tcfg.max_epoch_pretrain > 0:
            #     run_pretrain(model, loaders, tcfg, keep_sparse=cfg.keep_sparse)
            # train+validë§Œ
            return run_train_eval_only_valid(model, loaders, tcfg, keep_sparse=cfg.keep_sparse)

        # OOM ë°±ì˜¤í”„(8â†’4â†’2â†’1)
        return train_with_oom_retry(
            _call,
            backoff=(256, 128, 64, 32, 16, 8, 4, 2, 1),
            get_bs=lambda: tcfg.batch_size,
            set_bs=lambda b: setattr(tcfg, "batch_size", b),
        )

    study = optuna.create_study(direction="maximize")
    target = cfg.n_trials if hasattr(cfg, "n_trials") else 10
    attempts = 0
    while True:
        study.optimize(objective, n_trials=1, catch=())
        attempts += 1
        n_success = sum(t.state.name == "COMPLETE" for t in study.trials)
        if n_success >= target or attempts >= target * 50:
            break
    return sorted(study.best_trials, key=lambda t: t.value)[: getattr(cfg, "top_k", 1) ]

def suggest(trial):
    return {
        "lr": trial.suggest_categorical("lr", HP_SPACE["lr"]),
        "max_epoch": trial.suggest_categorical("max_epoch", HP_SPACE["max_epoch"]),
        "z_dim": trial.suggest_categorical("z_dim", HP_SPACE["z_dim"]),
        "h_dim": trial.suggest_categorical("h_dim", HP_SPACE["h_dim"]),
        "n_proto": trial.suggest_categorical("n_proto", HP_SPACE["n_proto"]),
    }

def main():
    import torch
    # main ì§„ì… ì§í›„ (í•œ ë²ˆë§Œ)
    torch.backends.cudnn.benchmark = True          # ì…ë ¥ í¬ê¸° ê³ ì •ì¼ìˆ˜ë¡ ì´ë“
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
    torch.set_float32_matmul_precision("high")     # TF32 (Ampere+)

    p = argparse.ArgumentParser()

    # í•„ìˆ˜/í•µì‹¬
    p.add_argument("--data", default="split_datasets")
    p.add_argument("--data_location", required=True, help="base path of split datasets")
    p.add_argument("--tasks", nargs="+", default=["covid", "cardio", "kidney"])
    p.add_argument("--n_repeats", type=int, default=5)
    p.add_argument("--n_folds", type=int, default=5)

    # í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„° (í•„ìš”í•œ ê²ƒë§Œ ì˜ˆì‹œë¡œ)
    p.add_argument("--lr", type=float, default=1e-4)
    p.add_argument("--max_epoch", type=int, default=75)
    p.add_argument("--batch_size", type=int, default=3)
    p.add_argument("--h_dim", type=int, default=128)
    p.add_argument("--z_dim", type=int, default=32)
    p.add_argument("--n_layers", type=int, default=2)
    p.add_argument("--n_proto", type=int, default=8)
    p.add_argument("--device", default="cuda:0")
    p.add_argument("--exp_str", default=None) # ì‹¤í—˜ ì´ë¦„(íƒœê·¸)â€**ì´ì—ìš”. ì´ ê°’ì´ Config ì•ˆì—ì„œ ì²´í¬í¬ì¸íŠ¸/ë¡œê·¸ ê²½ë¡œë¥¼ ë§Œë“œëŠ” ë° ê¼­ ì“°ì…ë‹ˆë‹¤.
    p.add_argument(
        "--cell_type_annotation",
        type=str,
        default=None,
        help="obs column name to use as cell type annotation (e.g. 'manual_annotation' or 'singler_annotation')"
)


    # ë¶ˆë¦¬ì–¸ í”Œë˜ê·¸ëŠ” BooleanOptionalAction ê¶Œì¥ (ë„ë ¤ë©´ --no-xxx)
    p.add_argument("--keep_sparse", action=argparse.BooleanOptionalAction, default=False)
    p.add_argument("--lognorm", action=argparse.BooleanOptionalAction, default=True)
    p.add_argument("--pretrained", action=argparse.BooleanOptionalAction, default=False)
    p.add_argument("--subsample", action=argparse.BooleanOptionalAction, default=False)
    p.add_argument("--eval", action=argparse.BooleanOptionalAction, default=False)
    p.add_argument("--load_ct", action=argparse.BooleanOptionalAction, default=False)

    # ê¸°íƒ€
    p.add_argument("--model", default="ProtoCell", choices=["ProtoCell", "BaseModel"],
            help="which model architecture to use")
    p.add_argument("--d_min", type=float, default=1.0)
    p.add_argument("--lambda_1", type=float, default=1.0)
    p.add_argument("--lambda_2", type=float, default=1.0)
    p.add_argument("--lambda_3", type=float, default=1.0)
    p.add_argument("--lambda_4", type=float, default=1.0)
    p.add_argument("--lambda_5", type=float, default=1.0)
    p.add_argument("--lambda_6", type=float, default=1.0)
    p.add_argument("--lr_pretrain", type=float, default=1e-2)
    p.add_argument("--max_epoch_pretrain", type=int, default=0)
    
    # run.py - argparse ì •ì˜ë¶€ ê·¼ì²˜ì— ì¶”ê°€ (Python 3.8 í˜¸í™˜)
    p.add_argument("--tune", dest="tune", action="store_true", help="enable Optuna tuning")
    p.add_argument("--no-tune", dest="tune", action="store_false", help="disable Optuna tuning")
    p.set_defaults(tune=True)

    p.add_argument("--n_trials", type=int, default=3, help="number of successful trials per fold")
    p.add_argument("--top_k", type=int, default=1, help="how many top trials to test on the test set")


    # ì‹œë“œ (ë°ì´í„° splitì—” ì•ˆ ì“°ê³ , ì¬í˜„ì„±ìš©)
    p.add_argument("--base_seed", type=int, default=42)

    args = p.parse_args()

    # args = p.parse_args() ì§í›„ë‚˜, fold ë£¨í”„ ì§„ì… ì§í›„
    import torch
    dev = torch.device(args.device)
    if dev.type == "cuda":
        torch.cuda.set_device(dev.index if dev.index is not None else 0)

    print(">>> Start >>>")
    for task in args.tasks:
        # === [NEW] ì´ taskì—ì„œì˜ ë°˜ë³µë³„ ìš”ì•½ì„ ë‹´ì•„ ë‘˜ ì»¨í…Œì´ë„ˆ
        task_repeat_summaries = []   # list of dicts: {"repeat": int, "auc": float, "acc": float, "recall": float, "prec": float}

        for repeat in range(args.n_repeats):
            # === [NEW] í•œ repeat ì•ˆì—ì„œ 5ê°œ foldì˜ ì§€í‘œë“¤ì„ ëª¨ì•„ë‘ëŠ” ì»¨í…Œì´ë„ˆ
            fold_aucs, fold_accs, fold_recalls, fold_precs = [], [], [], []
            fold_cms = []  # í˜¼ë™í–‰ë ¬ë„ í•©ì‚° ê°€ëŠ¥

            for fold in range(args.n_folds):
                seed = args.base_seed + repeat * 10 + fold
                set_global_seed(seed)
                print(f"\nğŸ” Task={task} | Repeat={repeat} | Fold={fold} | Seed={seed}")

                base = args.exp_str
                derived_exp = f"{base}_{task}_r{repeat}_f{fold}"
                
                print(">>> constructing Config", flush=True)

                cfg = Config(
                    data=args.data,
                    task=task,
                    data_location=args.data_location,
                    repeat=repeat,
                    fold=fold,
                    keep_sparse=args.keep_sparse,
                    lognorm=args.lognorm,
                    seed=seed,
                    # ì•„ë˜ëŠ” ê¸°ì¡´ Configì— ì´ë¯¸ ìˆëŠ” ì¸ìë“¤ ì—°ê²° (í•„ìš”í•œ ê²ƒë§Œ ì˜ˆì‹œ)
                    model=args.model,   
                    cell_type_annotation=args.cell_type_annotation,   # â˜… ì¶”ê°€
                    exp_str=derived_exp,
                    lr=args.lr,
                    max_epoch=args.max_epoch,
                    batch_size=args.batch_size,
                    h_dim=args.h_dim,
                    z_dim=args.z_dim,
                    n_layers=args.n_layers,
                    n_proto=args.n_proto,
                    device=args.device,
                    d_min=args.d_min,
                    lambda_1=args.lambda_1,
                    lambda_2=args.lambda_2,
                    lambda_3=args.lambda_3,
                    lambda_4=args.lambda_4,
                    lambda_5=args.lambda_5,
                    lambda_6=args.lambda_6,
                    pretrained=args.pretrained,
                    lr_pretrain=args.lr_pretrain,
                    max_epoch_pretrain=args.max_epoch_pretrain,
                    load_ct=args.load_ct,
                    eval=args.eval,
                    subsample=args.subsample,
                )
                # cfg ìƒì„± ì§í›„
                cfg.tune = args.tune
                cfg.n_trials = args.n_trials
                cfg.top_k = args.top_k

                # === (A) íŠœë‹: validë§Œìœ¼ë¡œ HP íƒìƒ‰ ===
                print("\n>>> TUNING", flush=True)
                best_trials = tune_one_fold(cfg) if getattr(cfg, "tune", True) else []

                # === (B) ê°™ì€ foldì—ì„œ ìµœì¢… í•™ìŠµ + test í‰ê°€ ===
                print("\n>>> ìµœì¢… í•™ìŠµ + TEST í‰ê°€", flush=True)
                from copy import deepcopy
                auc = acc = rec = pre = None
                cm_use = None

                if best_trials:
                    best_fold = None
                    for t in best_trials:
                        hp = t.params                       # Optunaê°€ ë½‘ì•„ì¤€ HP í•œ ì„¸íŠ¸ë¥¼ êº¼ëƒ„
                        tcfg = make_trial_cfg(cfg, hp)      # ì´ HPë¥¼ ë°˜ì˜í•œ ê¹¨ë—í•œ trial ì„¤ì •ì„ ìƒì„±. ì—¬ê¸°ì—” ì¥ì¹˜, ê²½ë¡œ, ì‹œë“œ ë“± cfgì˜ ê³ ì • ìì›ë„ ë³µì‚¬ë¨
                        tcfg.lr        = hp["lr"]
                        tcfg.max_epoch = hp["max_epoch"]
                        tcfg.z_dim     = hp["z_dim"]
                        tcfg.h_dim     = hp["h_dim"]
                        tcfg.n_proto   = hp["n_proto"]
                        tcfg.max_epoch_pretrain = 75

                        # âœ… ë¡œê·¸
                        logger = getattr(cfg, "logger", print)  # ì•ˆì „í•˜ê²Œ

                        # âœ… ìµœì¢… í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¡œê·¸ (tcfg ê¸°ì¤€)
                        logger("*" * 40)
                        logger(f"[BEST from Optuna] value={t.value:.6f}  params={hp}")
                        logger(f"Learning rate: {tcfg.lr:.6f}")
                        logger(f"Max epoch: {int(tcfg.max_epoch)}")
                        logger(f"Batch size: {int(tcfg.batch_size)}")
                        logger(f"Test step: {int(tcfg.test_step)}")
                        logger(f"Hidden dim: {int(tcfg.h_dim)}")
                        logger(f"Latent dim (z_dim): {int(tcfg.z_dim)}")
                        logger(f"Number of prototypes: {int(tcfg.n_proto)}")
                        logger(f"Lambdas: {tcfg.lambdas}")
                        logger(f"D_min: {tcfg.d_min}")
                        logger(f"Device: {tcfg.device}")
                        logger(f"Seed: {tcfg.seed}")
                        logger("*" * 40)
                        logger("")

                        # íŠœë‹ íŒŒë¼ë¯¸í„° ì£¼ì…
                        cfg.lr        = tcfg.lr
                        cfg.max_epoch = tcfg.max_epoch
                        cfg.z_dim     = tcfg.z_dim
                        cfg.h_dim     = tcfg.h_dim
                        cfg.n_proto   = tcfg.n_proto
                        cfg.batch_size = tcfg.batch_size
                        cfg.test_step  = tcfg.test_step
                        cfg.max_epoch_pretrain = getattr(tcfg, "max_epoch_pretrain", 75)



                        # ìµœì¢… í•™ìŠµ + í…ŒìŠ¤íŠ¸
                        def _call():
                            # model, optimizer, sched, loaders, meta = build_model_and_optim(tcfg, cfg, phase="valid_plus_test")
                            # if getattr(tcfg, "max_epoch_pretrain", 0) > 0:
                            #     run_pretrain(model, loaders, tcfg, keep_sparse=cfg.keep_sparse)
                            # return run_train_and_test(model, loaders, optimizer, tcfg, keep_sparse=cfg.keep_sparse)
                            
                            # (ì˜µì…˜) cfg.build()ê°€ ìˆë‹¤ë©´ í˜¸ì¶œí•´ì„œ ëª¨ë¸/ì˜µí‹°ë§ˆ/ë¡œë” ê°±ì‹ 
                            if hasattr(cfg, "build"):
                                cfg.build()
                            # (ì¤‘ìš”) íŠœë‹ ê²°ê³¼ë¥¼ cfgì— ì£¼ì…í•´ë‘ì—ˆëŠ”ì§€ í™•ì¸: lr/max_epoch/z_dim/h_dim/n_proto/batch_size ë“±
                            results = cfg.train()
                            return results   # dict ìì²´ ë°˜í™˜

                        try:
                            results = train_with_oom_retry(
                                _call,
                                backoff=(256, 128, 64, 32, 16, 8, 4, 2, 1),
                                get_bs=lambda: tcfg.batch_size,
                                set_bs=lambda b: setattr(tcfg, "batch_size", b),
                            )
                            test_auc  = results["test_auc"]
                            test_acc  = results["test_acc"]
                            test_f1   = results["test_f1"]
                            ct_acc    = results.get("ct_acc", None)
                            # best_fold ê°±ì‹ í•  ë•Œ ë³€ìˆ˜ëª… ì£¼ì˜!
                            if (best_fold is None) or (test_auc > best_fold["auc"]):
                                best_fold = dict(auc=test_auc, acc=test_acc, f1=test_f1, ct_acc=ct_acc)


                        except RuntimeError as e:
                            if "out of memory" in str(e).lower():
                                print(f"[WARN] OOM on final test for params {hp} â†’ skip")
                            else:
                                raise

                    # ê° fold ê²°ê³¼ ê¸°ë¡/ë¡œê·¸ 
                    # if best_fold:
                    #     auc, acc, f1, ct_acc = (
                    #         best_fold["test_auc"], best_fold["test_acc"], best_fold["test_f1"], best_fold["ct_acc"]
                    #     )
                    #     print(f"[Fold{fold} Result] AUC={auc:.4f} | ACC={acc:.4f} | F1={f1:.4f} | Cell Type ACC={ct_acc:.4f}")
                    # else:
                    #     # ì „ë¶€ OOM ë“±ìœ¼ë¡œ ì‹¤íŒ¨í•œ ê²½ìš°
                    #     print(f"[Fold{fold} Result] (no valid result)")
                    # ê° fold ê²°ê³¼ ê¸°ë¡/ë¡œê·¸ 
                    auc = acc = f1 = None   # ê¸°ë³¸ê°’ (best_foldê°€ ì—†ì„ ìˆ˜ ìˆìŒ)
                    ct_acc_val = None
                    if best_fold:
                        auc = best_fold["auc"]
                        acc = best_fold["acc"]
                        f1  = best_fold["f1"]
                        ct_acc_val = best_fold["ct_acc"]
                        ct_acc_str = f"{ct_acc_val:.4f}" if ct_acc_val is not None else "N/A"
                        print(f"[Fold{fold} Result] AUC={auc:.4f} | ACC={acc:.4f} | F1={f1:.4f} | Cell Type ACC={ct_acc_str}")
                    else:
                        # ì „ë¶€ OOM ë“±ìœ¼ë¡œ ì‹¤íŒ¨í•œ ê²½ìš°
                        print(f"[Fold{fold} Result] (no valid result)")


                # else:
                #     # íŠœë‹ ì—†ì´ í˜„ì¬ cfgë¡œ
                #     tcfg = deepcopy(cfg)
                #     model, optimizer, sched, loaders, meta = build_model_and_optim(tcfg, cfg, phase="valid_plus_test")
                #     if tcfg.max_epoch_pretrain > 0:
                #         run_pretrain(model, loaders, tcfg, keep_sparse=cfg.keep_sparse)
                #     auc, acc, cm_use, rec, pre = run_train_and_test(model, loaders, optimizer, tcfg, keep_sparse=cfg.keep_sparse)
                #     print(f"[Fold{fold} Result] AUC={auc:.4f} | ACC={acc:.4f} | RECALL={rec:.4f} | PREC={pre:.4f}")



                # === [NEW] fold ê²°ê³¼ë¥¼ ì»¨í…Œì´ë„ˆì— ì ì¬ (Noneì´ë©´ NaN ì²˜ë¦¬)
                def _tofloat(x): 
                    return float(x) if x is not None else np.nan

                fold_aucs.append(_tofloat(auc))
                fold_accs.append(_tofloat(acc))

                # fold_recalls.append(_tofloat(rec))
                # fold_precs.append(_tofloat(pre))
                # if cm_use is not None:
                #     fold_cms.append(np.asarray(cm_use, dtype=np.int64))



            # === [NEW] â¸ ê° repeat ì¢…ë£Œ í›„: 5ê°œ fold í‰ê· /í‘œì¤€í¸ì°¨ ì¶œë ¥
            r_auc_mean, r_auc_std = np.nanmean(fold_aucs), np.nanstd(fold_aucs)
            r_acc_mean, r_acc_std = np.nanmean(fold_accs), np.nanstd(fold_accs)
            # r_rec_mean, r_rec_std = np.nanmean(fold_recalls), np.nanstd(fold_recalls)
            # r_pre_mean, r_pre_std = np.nanmean(fold_precs), np.nanstd(fold_precs)

            print(
                f"\nğŸ“Œ [Repeat {repeat} Summary over {args.n_folds} folds]"
                f"  AUC={r_auc_mean:.4f}Â±{r_auc_std:.4f}"
                f" | ACC={r_acc_mean:.4f}Â±{r_acc_std:.4f}"
                # f" | RECALL={r_rec_mean:.4f}Â±{r_rec_std:.4f}"
                # f" | PREC={r_pre_mean:.4f}Â±{r_pre_std:.4f}"
            )

            # (ì„ íƒ) í˜¼ë™í–‰ë ¬ í•©ì‚° í‘œì‹œ
            # if len(fold_cms) > 0:
            #     cm_sum = np.add.reduce(fold_cms)
            #     print("   Î£ Confusion Matrix over folds:\n", cm_sum)

            # === [NEW] task ìˆ˜ì¤€ ìš”ì•½ì— ì¶”ê°€(ë‚˜ì¤‘ì— ì „ì²´ ìš”ì•½ ë•Œ ì‚¬ìš©)
            task_repeat_summaries.append({
                "repeat": repeat,
                "auc": r_auc_mean,
                "acc": r_acc_mean,
                # "recall": r_rec_mean,
                # "prec": r_pre_mean,
            })


        # === [NEW] âœ… ëª¨ë“  repeat ì¢…ë£Œ í›„: ì´ taskì— ëŒ€í•œ ì „ì²´ ìš”ì•½ ì¬ì •ë¦¬ ì¶œë ¥
        print("\n" + "="*72)
        print(f"ğŸ§¾ Task='{task}' â€” Summary by Repeat (means over {args.n_folds} folds)")
        for rs in task_repeat_summaries:
            print(
                f"  - Repeat {rs['repeat']}: "
                f"AUC={rs['auc']:.4f} | ACC={rs['acc']:.4f} "
            ) # | RECALL={rs['recall']:.4f} | PREC={rs['prec']:.4f}

        # ìµœì¢…(ëª¨ë“  repeat í‰ê· ) 1ì¤„
        final_auc_mean = np.nanmean([rs["auc"] for rs in task_repeat_summaries])
        final_auc_std  = np.nanstd([rs["auc"] for rs in task_repeat_summaries])
        final_acc_mean = np.nanmean([rs["acc"] for rs in task_repeat_summaries])
        final_acc_std  = np.nanstd([rs["acc"] for rs in task_repeat_summaries])

        print("-"*72)
        print(
            f"âœ… FINAL (Task='{task}', across {args.n_repeats} repeats, each averaged over {args.n_folds} folds)\n"
            f"   AUC={final_auc_mean:.4f}Â±{final_auc_std:.4f} | "
            f"ACC={final_acc_mean:.4f}Â±{final_acc_std:.4f}"
        )
        print("="*72 + "\n")


        
if __name__ == "__main__":
    main()

