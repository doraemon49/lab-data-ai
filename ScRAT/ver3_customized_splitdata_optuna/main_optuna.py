from sklearn import metrics
from sklearn.metrics import accuracy_score
import scipy.stats as st
import torch
import torch.nn as nn

from torch.optim import Adam
from utils import *
import argparse
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import RepeatedKFold
from sklearn.model_selection import train_test_split

from model_baseline import *
from Transformer import TransformerPredictor

from dataloader import *
from collections import defaultdict
import pandas as pd
import numpy as np

from collections import Counter

import functools, sys
# ëª¨ë“  print() ê¸°ë³¸ flush=Trueë¡œ ê°•ì œ
print = functools.partial(print, flush=True)
# (íŒŒì´ì¬ 3.7+ ì´ìƒ) stdoutì„ ì¤„ë‹¨ìœ„ ë²„í¼ë§ìœ¼ë¡œ ë³€ê²½
try:
    sys.stdout.reconfigure(line_buffering=True)
except Exception:
    pass


def _str2bool(v):
    return v.lower() in ("yes", "y", "true", "t", "1")


def int_or_float(x):
    try:
        return int(x)
    except ValueError:
        return float(x)


parser = argparse.ArgumentParser(description='scRNA diagnosis')

parser.add_argument('--seed', type=int, default=240)
parser.add_argument('--batch_size', type=int, default=16)
parser.add_argument('--learning_rate', type=float, default=3e-3)
parser.add_argument('--weight_decay', type=float, default=1e-4)
parser.add_argument('--epochs', type=int, default=100)
parser.add_argument("--task", type=str, default="severity")
parser.add_argument('--emb_dim', type=int, default=128)  # embedding dim
parser.add_argument('--h_dim', type=int, default=128)  # hidden dim of the model
parser.add_argument('--dropout', type=float, default=0.3)  # dropout
parser.add_argument('--layers', type=int, default=1)
parser.add_argument('--heads', type=int, default=8)
parser.add_argument("--train_sample_cells", type=int, default=500,
                    help="number of cells in one sample in train dataset") # í•™ìŠµ ì‹œ ê° í™˜ì ìƒ˜í”Œì—ì„œ 500ê°œ ì„¸í¬ë¥¼ ëœë¤ ì„ íƒ # ì‚¬ìš© 1: ë°ì´í„° ë¡œë“œ ì‹œ í™˜ì ëª…ìˆ˜ ì¡°ì •. # ì‚¬ìš©2: --all 1 í•´ì•¼ì§€ë§Œ ëœë¤ ìƒ˜í”Œë§ ì§„í–‰
parser.add_argument("--test_sample_cells", type=int, default=500,
                    help="number of cells in one sample in test dataset") # í…ŒìŠ¤íŠ¸ ì‹œì—ë„ ë™ì¼í•˜ê²Œ 500ê°œ ì„¸í¬ ì„ íƒ
parser.add_argument("--train_num_sample", type=int, default=20,
                    help="number of sampled data points in train dataset") # í•œ ëª…ì˜ í™˜ìì—ì„œ 500ê°œì˜ ì„¸í¬ë¥¼ 20ë²ˆ ìƒ˜í”Œë§í•˜ì—¬ 20ê°œì˜ bag ìƒì„±
parser.add_argument("--test_num_sample", type=int, default=100,
                    help="number of sampled data points in test dataset") # í…ŒìŠ¤íŠ¸ë„ ê°™ì€ ë°©ì‹ìœ¼ë¡œ 100ê°œì˜ bag ìƒì„±
parser.add_argument('--model', type=str, default='Transformer')
parser.add_argument('--dataset', type=str, default=None)
parser.add_argument('--inter_only', type=_str2bool, default=False) # mixupëœ ìƒ˜í”Œë§Œ í•™ìŠµì— ì‚¬ìš©í• ì§€ ì—¬ë¶€
parser.add_argument('--same_pheno', type=int, default=0) # ê°™ì€ í´ë˜ìŠ¤ë¼ë¦¬ mixupí• ì§€, ë‹¤ë¥¸ í´ë˜ìŠ¤ë¼ë¦¬ í• ì§€
# augment_num == 0 â†’ mixup ì•ˆ í•¨ â†’ same_pheno ë¬´ì˜ë¯¸
# augment_num > 0 â†’ mixup ì‹¤í–‰ë¨ â†’ same_phenoê°€ í™˜ì ìŒ ì„ íƒ ê·œì¹™ì— ì§ì ‘ ì˜í–¥
    # same_pheno=1 â†’ ê°™ì€ í´ë˜ìŠ¤ ë‚´ë¶€ì˜ ë‹¤ì–‘ì„±ì„ í‚¤ìš°ê³  ì‹¶ì„ ë•Œ (í´ë˜ìŠ¤ ê°„ ê²½ê³„ë¥¼ íë¦¬ì§€ ì•ŠìŒ)
    # same_pheno=-1 â†’ í´ë˜ìŠ¤ ê°„ ê²½ê³„ë¥¼ ë¶€ë“œëŸ½ê²Œ í•˜ì—¬ ëª¨ë¸ ì¼ë°˜í™” ìœ ë„
    # same_pheno=0 â†’ ë°ì´í„° ìˆ˜ê°€ ì ê±°ë‚˜ í´ë˜ìŠ¤ ë¶ˆê· í˜•ì´ ì‹¬í•  ë•Œ ë¬´ì‘ìœ„ë¡œ ì„ì–´ ë‹¤ì–‘ì„± ê·¹ëŒ€í™”
parser.add_argument('--augment_num', type=int, default=100) # Mixupëœ ìƒˆë¡œìš´ ê°€ì§œ ìƒ˜í”Œì„ ëª‡ ê°œ ìƒì„±í• ì§€
parser.add_argument('--alpha', type=float, default=1.0) # mixupì˜ ë¹„ìœ¨ (Beta ë¶„í¬ íŒŒë¼ë¯¸í„°)
parser.add_argument('--repeat', type=int, default=5)
parser.add_argument('--all', type=int, default=1)
# all == 0:
    # sample_cells ë§Œí¼ ëœë¤ ìƒ˜í”Œë§ (np.random.choice)
# all == 1
    # ìƒ˜í”Œë§ì„ ê±´ë„ˆë›°ê³  'í•´ë‹¹ í™˜ì(í˜¹ì€ ë¼ë²¨)ì˜ ëª¨ë“  ì…€'ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
parser.add_argument('--min_size', type=int, default=6000)
parser.add_argument('--n_splits', type=int, default=5)
parser.add_argument('--pca', type=_str2bool, default=False)
parser.add_argument('--mix_type', type=int, default=1)
parser.add_argument('--norm_first', type=_str2bool, default=False)
parser.add_argument('--warmup', type=_str2bool, default=False)
parser.add_argument('--top_k', type=int, default=1)

parser.add_argument('--cell_type_annotation', type=str, default="manual_annotation",
    help="ì‚¬ìš©í•  cell type annotation ì»¬ëŸ¼ëª… (manual_annotation ë˜ëŠ” singler_annotation)")

args = parser.parse_args()

# print("# of GPUs is", torch.cuda.device_count())
print(args)

torch.manual_seed(args.seed)
np.random.seed(args.seed)

patient_summary = {}
stats = {}
stats_id = {}

if args.task == 'haniffa' or args.task == 'combat':
    label_dict = {0: 'Non Covid', 1: 'Covid'}
elif args.task == 'severity':
    label_dict = {0: 'mild', 1: 'severe'}
elif args.task == 'stage':
    label_dict = {0: 'convalescence', 1: 'progression'}
elif args.task == 'custom_cardio':
    label_dict = {
    0: 'normal',
    1: 'hypertrophic cardiomyopathy',
    2: 'dilated cardiomyopathy'
}
elif args.task == 'custom_covid':
    label_dict = {0: 'normal', 1: 'COVID-19'}
elif args.task == 'custom_kidney':
    label_dict = {
        0:'Healthy',  # ë‘ healthy subtypeì„ í•˜ë‚˜ì˜ ëŒ€í‘œëª…ìœ¼ë¡œ
        1:'CKD',
        2:'AKI'
} 

# def safe_gather(data_mat, index_tensor):
#     # index_tensor: LongTensor (B, N) ë˜ëŠ” (N,)
#     idx = index_tensor.clone()
#     idx[idx < 0] = 0  # -1 íŒ¨ë”©ì€ 0ìœ¼ë¡œ ì¹˜í™˜í•´ ì•ˆì „ ì¸ë±ì‹±
#     return torch.from_numpy(data_mat[idx.cpu().numpy()])


# main.py - import ê·¼ì²˜
import optuna

# í›„ë³´ ì…‹
HP_SEARCH_SPACE = {
    "learning_rate": [1e-4, 1e-3, 1e-2],
    "epochs": [100],  # ê³ ì •
    "heads": [1, 2, 4],
    "dropout": [0.0, 0.3, 0.5, 0.7],
    "weight_decay": [1e-4, 1e-3, 1e-2],
    # "do_aug": [True, False],
    "emb_dim": [8, 32, 64],
    "augment_num": [100],   # (ì§ˆë¬¸ ëª…ì„¸ìƒ ê³ ì •)
    "pca": [False],         # (ì§ˆë¬¸ ëª…ì„¸ìƒ ê³ ì •)
}

def suggest_hparams(trial):
    return {
        "learning_rate": trial.suggest_categorical("learning_rate", HP_SEARCH_SPACE["learning_rate"]),
        "epochs": trial.suggest_categorical("epochs", HP_SEARCH_SPACE["epochs"]),
        "heads": trial.suggest_categorical("heads", HP_SEARCH_SPACE["heads"]),
        "dropout": trial.suggest_categorical("dropout", HP_SEARCH_SPACE["dropout"]),
        "weight_decay": trial.suggest_categorical("weight_decay", HP_SEARCH_SPACE["weight_decay"]),
        # "do_aug": trial.suggest_categorical("do_aug", HP_SEARCH_SPACE["do_aug"]),
        "emb_dim": trial.suggest_categorical("emb_dim", HP_SEARCH_SPACE["emb_dim"]),
        "augment_num": trial.suggest_categorical("augment_num", HP_SEARCH_SPACE["augment_num"]),
        "pca": trial.suggest_categorical("pca", HP_SEARCH_SPACE["pca"]),
    }


# === OOM ìµœí›„ ì•ˆì „ì¥ì¹˜ ===
# === OOM ìµœí›„ ì•ˆì „ì¥ì¹˜: 8 -> 4 -> 2 -> 1 ë‹¨ê³„ì  ì¬ì‹œë„ ===
def train_with_oom_retry(call, backoff=(8, 4, 2, 1)):
    """
    call: ì¸ì ì—†ëŠ” í•¨ìˆ˜(ëŒë‹¤)ë¡œ train(...) í˜¸ì¶œì„ ê°ìŒ‰ë‹ˆë‹¤.
    backoff: OOM ë°œìƒ ì‹œ ìˆœì°¨ì ìœ¼ë¡œ ì‹œë„í•  batch_size í›„ë³´ë“¤.
             ê° í›„ë³´ëŠ” 'ì›ë˜ batch_size'ì™€ì˜ min()ìœ¼ë¡œ ì ìš©ë©ë‹ˆë‹¤.
    """
    orig_bs = getattr(args, "batch_size", None)
    last_err = None
    try:
        # 1) ì›ë˜ ë°°ì¹˜ë¡œ 1ì°¨ ì‹œë„
        try:
            return call()
        except RuntimeError as e:
            if "out of memory" not in str(e).lower():
                raise
            last_err = e
            print("CUDA OOM ë°œìƒ â€” ë‹¨ê³„ì  ë°±ì˜¤í”„ ì‹œì‘.")

        # 2) ë‹¨ê³„ì  ë°±ì˜¤í”„: 8 â†’ 4 â†’ 2 â†’ 1
        for bs in backoff:
            # ìºì‹œ ë¹„ìš°ê¸°(ê°€ëŠ¥í•˜ë©´)
            if torch.cuda.is_available():
                try:
                    torch.cuda.empty_cache()
                except Exception:
                    pass

            if orig_bs is not None:
                new_bs = min(orig_bs, bs)
                print(f"[OOM RETRY] batch_size {orig_bs} â†’ {new_bs} ë‚®ì¶°ì„œ ë‹¤ì‹œ ì‹œë„í•©ë‹ˆë‹¤.")
                args.batch_size = new_bs

            try:
                return call()   # ì„±ê³µ ì‹œ ë°”ë¡œ ë°˜í™˜
            except RuntimeError as e:
                if "out of memory" in str(e).lower():
                    last_err = e
                    print(f"[OOM RETRY] batch_size={args.batch_size}ì—ì„œë„ OOM. batch_sizeë¥¼ ë” ë‚®ì¶°ì„œ ë‹¤ì‹œ ì‹œë„í•©ë‹ˆë‹¤... (batch_size 1ì´ì—ˆì„ ê²½ìš° ì¤‘ë‹¨í•©ë‹ˆë‹¤.)")
                    continue
                # OOM ì™¸ ì—ëŸ¬ëŠ” ê·¸ëŒ€ë¡œ ì „íŒŒ
                raise

        # 3) ëª¨ë‘ ì‹¤íŒ¨ â†’ ë§ˆì§€ë§‰ OOM ì „íŒŒ
        raise last_err

    finally:
        # âœ… ì—¬ê¸°ì„œ í•­ìƒ ì›ë³µ (ì„±ê³µ/ì‹¤íŒ¨/return/raise ëª¨ë‘ í¬í•¨)
        if orig_bs is not None:
            args.batch_size = orig_bs




def train(x_train, x_valid, x_test, y_train, y_valid, y_test, id_train, id_test,
          data_augmented, data, eval_test: bool = True):    
    dataset_1 = MyDataset(x_train, x_valid, x_test, y_train, y_valid, y_test, id_train, id_test, fold='train')
    dataset_2 = MyDataset(x_train, x_valid, x_test, y_train, y_valid, y_test, id_train, id_test, fold='test')
    dataset_3 = MyDataset(x_train, x_valid, x_test, y_train, y_valid, y_test, id_train, id_test, fold='val')
    train_loader = torch.utils.data.DataLoader(dataset_1, batch_size=args.batch_size, shuffle=True,
                                               collate_fn=dataset_1.collate)
    test_loader = torch.utils.data.DataLoader(dataset_2, batch_size=1, shuffle=False, collate_fn=dataset_2.collate)
    valid_loader = torch.utils.data.DataLoader(dataset_3, batch_size=1, shuffle=False, collate_fn=dataset_3.collate)


    num_train_samples = len(dataset_1)   # â† ì‹¤ì œ train ìƒ˜í”Œ(í™˜ì/ë°±) ê°œìˆ˜
    print(f"ğŸ‘‰ train samples(í•™ìŠµ ìƒ˜í”Œ(bag))={num_train_samples}, batch_size={args.batch_size} -> steps(ìƒ˜í”Œìˆ˜/batchí¬ê¸°)={len(train_loader)}")
    print(f"ğŸ‘‰ valid samples={len(dataset_3)}, batch_size=1 -> steps={len(valid_loader)}")
    print(f"ğŸ‘‰ test  samples={len(dataset_2)}, batch_size=1 -> steps={len(test_loader)}")
    # steps(len(~_loader)) ì„¤ëª… :
    # trainì€ mixup+samplingìœ¼ë¡œ bagì´ ëŠ˜ì–´ë‚œ ê²ƒ(covid : 126bags).
    # testëŠ” mixup ì•ˆ ì“°ê³  --all 1ì´ë¼ í™˜ì=bag(14)ë¡œ ê³ ì •.
    # test bagì„ ëŠ˜ë¦¬ë ¤ë©´ --all 0ë¡œ ì „í™˜í•˜ê³  --test_num_sampleì„ í‚¤ìš°ë©´ ë©ë‹ˆë‹¤.


    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    input_dim = data_augmented[0].shape[-1]

    output_class = len(set(labels_))
    if (output_class == 2):
        output_class = 1

    if args.task in ('custom_cardio', 'custom_kidney'):
        output_class = 3
    elif args.task == 'custom_covid' or output_class == 2:
        output_class = 1
        print("output_class : ", output_class)

    if args.model == 'Transformer':
        model = TransformerPredictor(input_dim=input_dim, model_dim=args.emb_dim, num_classes=output_class,
                                     num_heads=args.heads, num_layers=args.layers, dropout=args.dropout,
                                     input_dropout=0, pca=args.pca, norm_first=args.norm_first)
    elif args.model == 'feedforward':
        model = FeedForward(input_dim=input_dim, h_dim=args.emb_dim, cl=output_class, dropout=args.dropout)
    elif args.model == 'linear':
        model = Linear_Classfier(input_dim=input_dim, cl=output_class)
    elif args.model == 'scfeed':
        model = scFeedForward(input_dim=input_dim, cl=output_class, model_dim=args.emb_dim, dropout=args.dropout, pca=args.pca)

    model = nn.DataParallel(model)
    torch.cuda.empty_cache() # ëª¨ë¸ì„ GPUë¡œ ì˜¬ë¦¬ê¸° ì „ì— ìºì‹œ ë¹„ìš°ê¸°
    model.to(device)
    best_model = model

    allocated = torch.cuda.memory_allocated()
    # í˜„ì¬ ì˜ˆì•½ëœ ë©”ëª¨ë¦¬ (ìºì‹œ í¬í•¨)
    reserved = torch.cuda.memory_reserved()

    # MB ë‹¨ìœ„ë¡œ ë³´ê¸°
    print(f"Memory Allocated: {allocated / 1024 ** 2:.2f} MB")
    print(f"Memory Reserved: {reserved / 1024 ** 2:.2f} MB")

    print(device)

    ################################################################
    # training and evaluation
    ################################################################
    optimizer = Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)
    if args.warmup:
        import transformers
        scheduler = transformers.get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=args.epochs // 10,
                                                                 num_training_steps=args.epochs)
    else:
        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5, last_epoch=-1)
    sigmoid = torch.nn.Sigmoid().to(device)

    max_acc, max_epoch, max_auc, max_loss, max_valid_acc, max_valid_auc = 0, 0, 0, 0, 0, 0
    test_accs, valid_aucs, train_losses, valid_losses, train_accs, test_aucs = [], [0.], [], [], [], []
    best_valid_loss = float("inf")
    wrongs = []
    trigger_times = 0
    patience = 2
    for ep in (range(1, args.epochs + 1)):
        model.train()
        train_loss = []


        for batch in (train_loader):
            x_ = torch.from_numpy(data_augmented[batch[0]]).float().to(device)
            y_ = batch[1].to(device)
            mask_ = batch[3].to(device)
            optimizer.zero_grad()

            out = model(x_, mask_)

            if output_class==1:
                loss = nn.BCELoss()(sigmoid(out), y_)
            elif output_class==3:
                # === Multi-class (ì˜ˆ: 3-class) í•™ìŠµ ì†ì‹¤ ===
                # out: (B, N, C) ë˜ëŠ” (N, C) ë¡œ ê°€ì •. ë§ˆì§€ë§‰ ì°¨ì›ì´ C(í´ë˜ìŠ¤ ìˆ˜).
                # y_: (B, 1) í˜•íƒœ(float)ë¡œ ë“¤ì–´ì˜¤ë¯€ë¡œ longìœ¼ë¡œ ë³€í™˜ í›„, ì…€ ìˆ˜(N)ë§Œí¼ í™•ì¥.
                if out.dim() == 3:
                    B, N, C = out.shape
                    targets = y_.long().view(B, 1).expand(B, N).long()           # (B, N)
                    loss = nn.CrossEntropyLoss()(out.reshape(-1, C), targets.reshape(-1))
                elif out.dim() == 2:
                    N, C = out.shape
                    targets = y_.view(-1).long()           # âœ… [B]
                    # í•„ìš”í•˜ë©´ outì˜ ë°°ì¹˜ Bì™€ ì¼ì¹˜í•˜ëŠ”ì§€ assert
                    assert targets.shape[0] == out.shape[0], f"{targets.shape} vs {out.shape}"

                    loss = nn.CrossEntropyLoss()(out, targets)
                else:
                    raise RuntimeError(f"Unexpected logits shape for multiclass: {out.shape}")


            loss.backward()

            optimizer.step()
            train_loss.append(loss.item())

        scheduler.step()

        train_loss = sum(train_loss) / len(train_loss)
        train_losses.append(train_loss)

        if ep % 1 == 0:
            valid_loss = []
            model.eval()
            pred = []
            true = []
            with torch.no_grad():
                for batch in (valid_loader):
                    x_ = torch.from_numpy(data[batch[0]]).float().to(device).squeeze(0)
                    y_ = batch[1].int().to(device)

                    out = model(x_)
                    if output_class == 1:
                        out = sigmoid(out)

                        loss = nn.BCELoss()(out, y_ * torch.ones(out.shape).to(device))
                        valid_loss.append(loss.item())

                        out = out.detach().cpu().numpy()

                        # majority voting
                        f = lambda x: 1 if x > 0.5 else 0
                        func = np.vectorize(f)
                        out = np.argmax(np.bincount(func(out).reshape(-1))).reshape(-1)
                        pred.append(out)
                        y_ = y_.detach().cpu().numpy()
                        true.append(y_)
                    else:
                        # === Multiclass ===
                        # ì…€ë³„ ë¡œì§“ì´ ë‚˜ì˜¤ë©´ bag í‰ê· ìœ¼ë¡œ ì¶•ì•½ í›„ CE
                        if out.dim() == 3:          # [B=1, N, C]
                            logits = out.mean(dim=1)        # [1, C]
                        else:                        # [1, C] ë˜ëŠ” [N, C]ì¸ ê²½ìš°
                            if out.dim() == 2 and out.shape[0] > 1:  # [N, C]ë¼ë©´ Nì¶• í‰ê· ìœ¼ë¡œ bag ë¡œì§“ ë§Œë“¤ê¸°
                                logits = out.mean(dim=0, keepdim=True)  # [1, C]
                            else:
                                logits = out                         # [1, C]

                        loss = nn.CrossEntropyLoss()(logits, y_.view(-1).long())
                        valid_loss.append(loss.item())

                        # ì˜ˆì¸¡/ì •ë‹µ ê¸°ë¡
                        probs = torch.softmax(logits, dim=-1)     # [1, C]
                        pred_cls = int(torch.argmax(probs, dim=-1).item())
                        pred.append(pred_cls)
                        true.append(int(y_.view(-1).item()))



            # pred = np.concatenate(pred)
            # true = np.concatenate(true)

            valid_loss = sum(valid_loss) / len(valid_loss)
            valid_losses.append(valid_loss)

            if (valid_loss < best_valid_loss):
                best_model = copy.deepcopy(model)
                max_epoch = ep
                best_valid_loss = valid_loss
                max_loss = train_loss

            print("Epoch %d, Train Loss %f, Valid_loss %f" % (ep, train_loss, valid_loss))

            # Early stop
            if (ep > args.epochs - 50) and ep > 1 and (valid_loss > valid_losses[-2]):
                trigger_times += 1
                if trigger_times >= patience:
                    break
            else:
                trigger_times = 0

    if not eval_test:
        # íŠœë‹ ëª©ì : testëŠ” ê±´ë“œë¦¬ì§€ ì•Šê³  valid ê¸°ì¤€ë§Œ ë°˜í™˜
        return None, None, None, None, None, best_valid_loss
    
    # ---- ì´í•˜ ê¸°ì¡´ test í‰ê°€ ì½”ë“œ ìœ ì§€ ----
    best_model.eval()
    test_id = []
    wrong = []
    if output_class == 1:
        pred = []
        true = []
        prob = []

    else:
        preds_mc,  trues_mc,  probvecs_mc = [], [], []   # probvecs_mc: ê° ìƒ˜í”Œì˜ softmax í™•ë¥  ë²¡í„°

    with torch.no_grad():
        for batch in (test_loader):
            x_ = torch.from_numpy(data[batch[0]]).float().to(device).squeeze(0)
            y_ = batch[1].int().numpy()
            id_ = batch[2][0] # í™˜ì/ìƒ˜í”Œ ID

            out = best_model(x_)

            if output_class == 1:
            # === Binary Classification ===
                out = sigmoid(out)
                out = out.detach().cpu().numpy().reshape(-1)

                y_ = y_[0][0]
                true.append(y_)

                if args.model != 'Transformer':
                    prob.append(out[0])
                else:
                    prob.append(out.mean())

                # majority voting
                f = lambda x: 1 if x > 0.5 else 0
                func = np.vectorize(f)
                out = np.argmax(np.bincount(func(out).reshape(-1))).reshape(-1)[0]
                pred.append(out)
                test_id.append([batch[2][0]])
                if out != y_:
                    wrong.append([batch[2][0]])

                # ì¶œë ¥
                # print(f"-- true: {y_}, pred: {out}")
                
                # id_ëŠ” list, ë³´í†µ batch_size=1ì´ë¼ id_[0]ë§Œ ìˆìŒ
                cell_indices = np.array(id_[0])  # [163460, 163461, ...]
                first_idx = int(cell_indices[0]) # ì „ì—­ ì…€ ì¸ë±ìŠ¤ í•˜ë‚˜
                pid = patient_id_all[first_idx]      # âœ… ì‹¤ì œ í™˜ì ID ë¬¸ìì—´

                print(f"í™˜ìID={pid} -- true: {y_} -- pred: {out}")


            else:
                # ===== Multiclass =====                
                # y_ê°€ numpyì¼ ìˆ˜ë„, torchì¼ ìˆ˜ë„ ìˆì„ ë•Œì˜ ë²”ìš© ì²˜ë¦¬
                if torch.is_tensor(y_):
                    y_true = int(y_.detach().cpu().reshape(-1)[0].item())
                else:
                    y_true = int(np.array(y_).reshape(-1)[0])


                # logits -> bag ë¡œì§“
                if out.dim() == 3:       # [B=1, N, C]
                    logits = out.mean(dim=1)                 # [1, C]
                elif out.dim() == 2:     # [N, C] ë˜ëŠ” [1, C]
                    logits = out.mean(dim=0, keepdim=True) if out.shape[0] > 1 else out   # [1, C]
                else:
                    raise RuntimeError(f"Unexpected logits shape: {out.shape}")

                p = torch.softmax(logits, dim=-1).squeeze(0)  # [C]
                y_pred = int(torch.argmax(p).item())

                probvecs_mc.append(p.cpu().numpy())  # AUCìš© í™•ë¥  ë²¡í„°
                preds_mc.append(y_pred)
                trues_mc.append(y_true)
                test_id.append(batch[2][0])         # or test_patient_id[...] ë¡œê¹…
                if y_pred != y_true:
                    wrong.append(test_id[-1])

                # print(f"-- true: {y_true}, pred: {y_pred}")
                
                # id_ëŠ” list, ë³´í†µ batch_size=1ì´ë¼ id_[0]ë§Œ ìˆìŒ
                cell_indices = np.array(id_[0])  # [163460, 163461, ...]
                first_idx = int(cell_indices[0]) # ì „ì—­ ì…€ ì¸ë±ìŠ¤ í•˜ë‚˜
                pid = patient_id_all[first_idx]      # âœ… ì‹¤ì œ í™˜ì ID ë¬¸ìì—´

                print(f"í™˜ìID={pid} -- true: {y_} -- pred: {out}")


    # if len(wrongs) == 0:
    #     wrongs = set(wrong)
    # else:
    #     wrongs = wrongs.intersection(set(wrong))
    
    # ====== ì§‘ê³„ ë° ì§€í‘œ ======
    if output_class == 1:
        test_acc = accuracy_score(true, pred)
        try:
            test_auc = metrics.roc_auc_score(true, prob)
        except ValueError:
            print("âš ï¸ AUC ê³„ì‚° ë¶ˆê°€: test set í´ë˜ìŠ¤ê°€ ë‹¨ì¼ì…ë‹ˆë‹¤.")
            test_auc = np.nan

        cm = confusion_matrix(true, pred).ravel()
        if len(cm) == 4:
            recall    = cm[3] / (cm[3] + cm[2]) if (cm[3] + cm[2]) > 0 else np.nan
            precision = cm[3] / (cm[3] + cm[1]) if (cm[3] + cm[1]) > 0 else np.nan
        else:
            print("âš ï¸ Skipping evaluation due to insufficient class diversity")
            recall = precision = np.nan

        # ë¡œê·¸
        # for i in range(len(pred)):
        #     print(f"{test_ids[i]} -- true: {label_dict[true[i]]} -- pred: {label_dict[pred[i]]}")

    else: # multi-class
        test_acc = accuracy_score(trues_mc, preds_mc)
        try:
            # probvecs_mc: (num_samples, C) ë¡œ ë³€í™˜
            prob_matrix = np.vstack(probvecs_mc)   # shape [N, C]
            test_auc = metrics.roc_auc_score(trues_mc, prob_matrix, multi_class='ovo') # , average='weighted'
        except ValueError:
            print("âš ï¸ AUC ê³„ì‚° ë¶ˆê°€: test setì— ëª¨ë“  í´ë˜ìŠ¤ê°€ í¬í•¨ë˜ì§€ ì•ŠìŒ")
            test_auc = np.nan

        cm = confusion_matrix(trues_mc, preds_mc)
        from sklearn.metrics import precision_recall_fscore_support
        precision, recall, _, _ = precision_recall_fscore_support(
            trues_mc, preds_mc, average='macro', zero_division=0
        )

    # ê³µí†µ ì¶œë ¥
    print("Best performance: Epoch %d, Loss %f, Test ACC %f, Test AUC %f, Test Recall %f, Test Precision %f" %
        (max_epoch, max_loss, test_acc, test_auc, recall, precision))
    print("Confusion Matrix:\n", cm)
    
    # train() ë§ˆì§€ë§‰ ë°˜í™˜ ì§ì „
    # ë§¤ fold/trialë§ˆë‹¤ í•™ìŠµì´ ëë‚  ë•Œ GPU ìºì‹œë¥¼ ë¹„ì›Œ ì¤ë‹ˆë‹¤.
    import gc
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    # train() ë§¨ ë
    return test_auc, test_acc, cm, recall, precision, best_valid_loss


    # # AUC ê³„ì‚° ë°©ì‹ ë¶„ê¸°
    # try:
    #     if output_class == 1:
    #         test_acc = accuracy_score(true, pred)
    #         test_auc = metrics.roc_auc_score(true, prob)
    #     else:
    #         test_acc = accuracy_score(trues_mc, preds_mc)
    #         # probvecs_mc: (num_samples, C) ë¡œ ë³€í™˜
    #         prob_matrix = np.vstack(probvecs_mc)   # shape [N, C]
    #         test_auc = metrics.roc_auc_score(trues_mc, prob_matrix, multi_class='ovr', average='weighted')
    # except ValueError:
    #     print("âš ï¸ AUC ê³„ì‚° ë¶ˆê°€: test setì— ëª¨ë“  í´ë˜ìŠ¤ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ (ì´ì§„ë¶„ë¥˜ì˜ ê²½ìš° í•˜ë‚˜ì˜ classë§Œ ì¡´ì¬. ì‚¼ì¤‘ë¶„ë¥˜ì˜ ê²¨ìš° ë‘ê°œì˜ classë§Œ ì¡´ì¬)")
    #     test_auc = np.nan


    # # for idx in range(len(pred)):
    # #     print(f"{test_id[idx]} -- true: {label_dict[true[idx]]} -- pred: {label_dict[pred[idx]]}")
    # test_accs.append(test_acc)

    # print("true : ", true)
    # print("pred : ", pred)

    # # Confusion Matrix ë° ì§€í‘œ ë¶„ê¸° # í•˜ë‚˜ì˜ ì„ê³„ê°’(ë³´í†µ 0.5)ì—ì„œì˜ ìŠ¤ëƒ…ìƒ·ì¼ ë¿
    # if output_class == 1:
    #     cm = confusion_matrix(true, pred).ravel()

    #     if len(cm) == 4:
    #         recall = cm[3] / (cm[3] + cm[2]) if (cm[3] + cm[2]) > 0 else np.nan
    #         precision = cm[3] / (cm[3] + cm[1]) if (cm[3] + cm[1]) > 0 else np.nan
    #     else:
    #         print("âš ï¸ Skipping evaluation due to insufficient class diversity")
    #         recall = precision = np.nan
    #     print("Confusion Matrix: " + str(cm))

    # else:  #  multiclass ì¼ ë•Œì˜ cm, recall, precision ì •ì˜í•´ì£¼ì„¸ìš”
    #     # ë©€í‹°í´ë˜ìŠ¤ í˜¼ë™í–‰ë ¬ (ì •ë°© í–‰ë ¬)
    #     cm = confusion_matrix(trues_mc, preds_mc)
    #     # ë§¤í¬ë¡œ í‰ê· (í´ë˜ìŠ¤ ê· ë“± ê°€ì¤‘) â€” ìƒí™©ì— ë”°ë¼ 'weighted'ë¥¼ ì¨ë„ ë©ë‹ˆë‹¤.
    #     from sklearn.metrics import precision_recall_fscore_support
    #     precision, recall, _, _ = precision_recall_fscore_support(
    #         trues_mc, preds_mc, average='macro', zero_division=0
    #     )
    #     print("Confusion Matrix:\n", cm)

    # # ë¡œê·¸
    # for i in range(len(preds)):
    #     print(f"{test_ids[i]} -- true: {label_dict[trues[i]]} -- pred: {label_dict[preds[i]]}")
    # print("true : ", trues)
    # print("pred : ", preds)
    # if output_class == 1:
    #     print("Confusion Matrix:", cm)
    # else:
    #     print("Confusion Matrix:\n", cm)


    # print("Best performance: Epoch %d, Loss %f, Test ACC %f, Test AUC %f, Test Recall %f, Test Precision %f" % (
    # max_epoch, max_loss, test_acc, test_auc, recall, precision))
    # print("Confusion Matrix: " + str(cm))
    # for w in wrongs:
    #     v = patient_summary.get(w, 0)
    #     patient_summary[w] = v + 1

    # return test_auc, test_acc, cm, recall, precision


if args.model != 'Transformer':
    args.repeat = 60

# ì´ë¯¸ ë§Œë“¤ì–´ ë‘” train/valid ë¶„í• ê³¼, mixup/sampling ê²°ê³¼ë¥¼ ë°›ì•„ **valid ì„±ëŠ¥(=best_valid_loss)**ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ íŠœë‹í•©ë‹ˆë‹¤.
# ì—¬ê¸°ì„œ testëŠ” ê±´ë“œë¦¬ì§€ ì•ŠìŠµë‹ˆë‹¤.
def optuna_tune_one_fold(prep_A, n_trials=5, top_k=args.top_k):
    def objective(trial):
        hp = suggest_hparams(trial)
        # í•˜ì´í¼ ì£¼ì…
        args.learning_rate = hp["learning_rate"]
        args.weight_decay  = hp["weight_decay"]
        args.epochs        = hp["epochs"]
        args.heads         = hp["heads"]
        args.dropout       = hp["dropout"]
        args.emb_dim       = hp["emb_dim"]
        args.augment_num   = hp["augment_num"]
        args.pca           = hp["pca"]

        # ìœ„í—˜ ì¡°í•© ì‚¬ì „ ê°€ë“œ (ì˜ˆ: ë©”ëª¨ë¦¬ ë¹¡ì„¼ ì¡°í•© ìµœì†Œ ì™„í™”)
        orig_bs = args.batch_size
        if args.heads == 4 and args.emb_dim >= 64:
            args.batch_size = min(args.batch_size, 8) ## ê¸°ë³¸ê°’ 16ì—ì„œ, min(í˜„ì¬ê°’ or 8)ë¡œ ë‚®ì¶¤

        try:
            # íŠœë‹ì€ validë§Œ (eval_test=False)
            def _call():
                data_for_training = prep_A["data_augmented"]  # í˜„ì¬ ì„¤ê³„ìƒ íŠœë‹ì€ mixup ì‚¬ìš©
                return train(
                    prep_A["x_train"], prep_A["x_valid"], [],
                    prep_A["y_train"], prep_A["y_valid"], np.empty((0,1)),
                    prep_A["id_train"], [],
                    data_for_training, prep_A["data"], eval_test=False
                )
            try:
                # âœ… ë‹¤ë‹¨ê³„ OOM ë°±ì˜¤í”„ìœ¼ë¡œ ê°ì‹¸ê¸°
                _, _, _, _, _, best_vloss = train_with_oom_retry(_call, backoff=(8, 4, 2, 1))
            except RuntimeError as e:
                if "out of memory" in str(e).lower():
                    trial.set_user_attr("oom", True)
                    import optuna
                    # âœ… ì´ íŠ¸ë¼ì´ì–¼ì€ 'ì‹¤íŒ¨'ê°€ ì•„ë‹ˆë¼ 'Pruned'ë¡œ ì¢…ë£Œ â†’ ì„±ê³µ trial ì•„ë‹˜
                    raise optuna.exceptions.TrialPruned("Pruned due to OOM after backoff")
                raise
        finally:
            # ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì›ë³µ
            args.batch_size = orig_bs

        return best_vloss # ê²€ì¦(valid)ì—ì„œ ì–»ì€ ì†ì‹¤ ì¤‘ ê°€ì¥ ë‚®ì€ ê°’(best_valid_loss) ì„ ëŒë ¤ì¤Œ

    study = optuna.create_study(direction="minimize") # valid lossë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ íƒìƒ‰
                                                    # search ì•Œê³ ë¦¬ì¦˜: ê¸°ë³¸ì€ TPE (Tree-structured Parzen Estimator). ì¦‰, **ì™„ì „ ë¬´ì‘ìœ„(random search)**ê°€ ì•„ë‹ˆë¼, ì• trialë“¤ì˜ ì„±ëŠ¥ ë¶„í¬ë¥¼ ì°¸ê³ í•´ì„œ ë‹¤ìŒ trialì„ ì ì  ë” promisingí•œ ì˜ì—­ì—ì„œ ìƒ˜í”Œë§í•©ë‹ˆë‹¤.

    # âœ… â€˜ì„±ê³µ(Complete) trialâ€™ì„ ì •í™•íˆ n_trials ê°œ ëª¨ì„ ë•Œê¹Œì§€ ë°˜ë³µ
    target_success = n_trials
    max_attempts   = n_trials * 50  # ë¬´í•œ ë£¨í”„ ë°©ì§€ ìƒí•œ (ìƒí™© ë”°ë¼ ì¡°ì •)
    attempts = 0
    while True:
        # 1ê°œì”© ì‹œë„ (OOM/Prunedë©´ ì„±ê³µ ì¹´ìš´íŠ¸ì— ë¯¸í¬í•¨)
        study.optimize(objective, n_trials=1, catch=())
        attempts += 1

        # í˜„ì¬ ì„±ê³µ(=COMPLETE) trial ìˆ˜ ì§‘ê³„
        n_success = sum(1 for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE)
        if n_success >= target_success:
            break
        if attempts >= max_attempts: # ëª¨ë“  íšŸìˆ˜ ì†Œì§„..
            print(f"[WARN] ì„±ê³µ trialì´ {n_success}/{target_success}ê°œë§Œ í™•ë³´ë˜ì—ˆìŠµë‹ˆë‹¤. (ì‹œë„ {attempts}íšŒ)")
            break

    # ìƒìœ„ top_k trial ë°˜í™˜
    best_trials = sorted(
        [t for t in study.best_trials], key=lambda t: t.value
    )[:top_k]
    return best_trials



# 1. ê¸°ì¡´ ì½”ë“œ
# if args.task != 'custom':
#     p_idx, labels_, cell_type, patient_id, data, cell_type_large = Covid_data(args)
# else:
#     p_idx, labels_, cell_type, patient_id, data, cell_type_large = Custom_data(args)

"""
# 2. covid, cardioë¥¼ ìœ„í•œ custom ì¶”ê°€ ì½”ë“œ
# if args.task == 'custom_cardio':
#     p_idx, labels_, cell_type, patient_id, data, cell_type_large = Custom_data(args)
# elif args.task == 'custom_covid':
#     p_idx, labels_, cell_type, patient_id, data, cell_type_large = Custom_data(args)

# else:
#     p_idx, labels_, cell_type, patient_id, data, cell_type_large = Covid_data(args)


# ë‚´ë¶€ì—ì„œ ëœë¤í•˜ê²Œ splitì„ ìƒì„± 
# rkf = RepeatedKFold(n_splits=abs(args.n_splits), n_repeats=args.repeat * 100, random_state=args.seed)

# num = np.arange(len(p_idx))
# accuracy, aucs, cms, recalls, precisions = [], [], [], [], []
# iter_count = 0

# #  class ë¶„í¬ì™€ split êµ¬ì„± í™•ì¸
# from collections import Counter
# print(Counter(labels_))  # class ë‹¹, ì „ì²´ cell ë‹¨ìœ„ ë¼ë²¨ ë¶„í¬ # ex) Counter({1: 235252, 0: 185441, 2: 171996})
# patient_classes = [labels_[p[0]] for p in p_idx]
# print("í™˜ì ìˆ˜ ê¸°ì¤€ í´ë˜ìŠ¤ ë¶„í¬:")
# print(Counter(patient_classes))
# # for i, p in enumerate(p_idx):
# #     print(f"Sample {i} - Class: {labels_[p[0]]}")


for train_index, test_index in rkf.split(num):
    print(f"ğŸ” Split #{iter_count + 1}")
    print(f"  â†’ train_index í™˜ì ìˆ˜: {len(train_index)}")
    print(f"  â†’ test_index í™˜ì ìˆ˜: {len(test_index)}")

    # ì‹¤ì œ í™˜ì IDë¡œ ë³´ê¸°
    train_ids = [patient_id[p_idx[i][0]] for i in train_index]
    test_ids = [patient_id[p_idx[i][0]] for i in test_index]
    print(f"  â†’ train í™˜ì ID: {train_ids}")
    print(f"  â†’ test  í™˜ì ID: {test_ids}")

    if args.n_splits < 0:
        temp_idx = train_index
        train_index = test_index
        test_index = temp_idx

    label_stat = [] #  train setì— í¬í•¨ëœ í™˜ìë“¤ì˜ ë¼ë²¨ ëª©ë¡
    for idx in train_index:
        label_stat.append(labels_[p_idx[idx][0]])
    unique, cts = np.unique(label_stat, return_counts=True)
    # í›ˆë ¨ ë°ì´í„°(train_index)ì— í´ë˜ìŠ¤ê°€ 2ê°œ ì´ìƒ ì¡´ì¬í•´ì•¼ í•™ìŠµì„ ì§„í–‰í•œë‹¤.
    if len(unique) < 2 or (1 in cts): 
        # í´ë˜ìŠ¤ê°€ í•˜ë‚˜ë°–ì— ì—†ìŒ â†’ ë¶ˆê· í˜• â†’ ìŠ¤í‚µ 
        # or 
        # ë“±ì¥í•œ í´ë˜ìŠ¤ ì¤‘ í•œ í´ë˜ìŠ¤ì˜ í™˜ì ìˆ˜ê°€ 1ëª…ë°–ì— ì•ˆ ë¨ â†’ í•™ìŠµì´ ë¶ˆì•ˆì •í•´ì§ˆ ê°€ëŠ¥ì„±ì´ ë§¤ìš° ë†’ê¸° ë•Œë¬¸ì— skip
        continue
#     print(dict(zip(unique, cts)))
    
    # ì›ë˜ ì½”ë“œì—ëŠ” test setì˜ í´ë˜ìŠ¤ ë¶ˆê· í˜•ì€ ì²´í¬í•˜ì§€ ì•ŠìŒ
    ### âœ… test_index í´ë˜ìŠ¤ í™•ì¸ ì¶”ê°€
    test_labels = [labels_[p_idx[i][0]] for i in test_index]
    if len(set(test_labels)) < 2:
        print(f"âš ï¸  Skipping split: test set has only one class -> {set(test_labels)}")
        continue

    kk = 0
    while True:
        train_index_, valid_index, ty, vy = train_test_split(train_index, label_stat, test_size=0.33,
                                                             random_state=args.seed + kk)
        if len(set(ty)) == 2 and len(set(vy)) == 2:
            break
        kk += 1

    train_index = train_index_
    len_valid = len(valid_index)
    _index = np.concatenate([valid_index, test_index])

    train_ids = []
    for i in train_index:
        # train_ids.append(patient_id[p_idx[i][0]])
        # pca False ; ìˆ˜ì • 1
        train_ids.append(patient_id.iloc[p_idx[i][0]])

#     print(train_ids)

    x_train = []
    x_test = []
    x_valid = []
    y_train = []
    y_valid = []
    y_test = []
    id_train = []
    id_test = []
    id_valid = []
    data_augmented, train_p_idx, labels_augmented, cell_type_augmented = mixups(args, data,
                                                                                [p_idx[idx] for idx in train_index],
                                                                                labels_,
                                                                                cell_type)
    # mixup ì‹¤íŒ¨ ì‹œ split ê±´ë„ˆë›°ê¸°
    if data_augmented is None:
        print("âš ï¸ Skipping split due to insufficient mixup class diversity.")
        continue
    individual_train, individual_test = sampling(args, train_p_idx, [p_idx[idx] for idx in _index], labels_,
                                                 labels_augmented, cell_type_augmented)
    for t in individual_train:
        id, label = [id_l[0] for id_l in t], [id_l[1] for id_l in t]
        x_train += [ii for ii in id]
        y_train += (label)
        id_train += (id)

    temp_idx = np.arange(len(_index))
    for t_idx in temp_idx[len_valid:]:
        id, label = [id_l[0] for id_l in individual_test[t_idx]], [id_l[1] for id_l in individual_test[t_idx]]
        x_test.append([ii for ii in id])
        y_test.append(label[0])
        id_test.append(id)
    for t_idx in temp_idx[:len_valid]:
        id, label = [id_l[0] for id_l in individual_test[t_idx]], [id_l[1] for id_l in individual_test[t_idx]]
        x_valid.append([ii for ii in id])
        y_valid.append(label[0])
        id_valid.append(id)
    x_train, x_valid, x_test, y_train, y_valid, y_test = x_train, x_valid, x_test, np.array(y_train).reshape([-1, 1]), \
                                                         np.array(y_valid).reshape([-1, 1]), np.array(y_test).reshape(
        [-1, 1])
    auc, acc, cm, recall, precision = train(x_train, x_valid, x_test, y_train, y_valid, y_test, id_train, id_test,
                                            data_augmented, data)
    aucs.append(auc)
    accuracy.append(acc)
    cms.append(cm)
    recalls.append(recall)
    precisions.append(precision)
    iter_count += 1
    if iter_count == abs(args.n_splits) * args.repeat:
        break
    print(f"âœ… Total valid splits used: {iter_count}")

    del data_augmented



# print("="*33)
# print("=== Final Evaluation (average across all splits) ===")
# print("="*33)

# print("Best performance: Test ACC %f,   Test AUC %f,   Test Recall %f,   Test Precision %f" % (np.average(accuracy), np.average(aucs), np.average(recalls), np.average(precisions)))

# print("=================================")
# print("=== ì €í¬ ë…¼ë¬¸ìš© Final Evaluation (average across all splits) ===")
# print("=================================")
# print(f"Best performance: "
#       f"Test ACC {np.mean(accuracy):.6f}+-{np.std(accuracy):.6f}, "
#       f"Test AUC {np.mean(aucs):.6f}+-{np.std(aucs):.6f}, "
#       f"Test Recall {np.mean(recalls):.6f}+-{np.std(recalls):.6f}, "
#       f"Test Precision {np.mean(precisions):.6f}+-{np.std(precisions):.6f}")

####################################
######## Only for repeat > 1 #######
####################################
# accuracy = np.array(accuracy).reshape([-1, args.repeat]).mean(0)
# aucs = np.array(aucs).reshape([-1, args.repeat]).mean(0)
# recalls = np.array(recalls).reshape([-1, args.repeat]).mean(0)
# precisions = np.array(precisions).reshape([-1, args.repeat]).mean(0)
# ci_1 = st.t.interval(alpha=0.95, df=len(accuracy) - 1, loc=np.mean(accuracy), scale=st.sem(accuracy))[1] - np.mean(accuracy)
# ci_2 = st.t.interval(alpha=0.95, df=len(aucs) - 1, loc=np.mean(aucs), scale=st.sem(aucs))[1] - np.mean(aucs)
# ci_3 = st.t.interval(alpha=0.95, df=len(recalls) - 1, loc=np.mean(recalls), scale=st.sem(recalls))[1] - np.mean(recalls)
# ci_4 = st.t.interval(alpha=0.95, df=len(precisions) - 1, loc=np.mean(precisions), scale=st.sem(precisions))[1] - np.mean(precisions)
# print("ci: ACC ci %f,   AUC ci %f,   Recall ci %f,   Precision ci %f" % (ci_1, ci_2, ci_3, ci_4))

# print(np.average(cms, 0))
# print(patient_summary)
# print(stats)
# print(stats_id)

"""
# ì‚¬ì „ ìƒì„±ëœ split íŒŒì¼ì„ ê¸°ë°˜ìœ¼ë¡œ ê³ ì •ëœ train/test ë°ì´í„°ì…‹ìœ¼ë¡œ ì‹¤í—˜ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ (2ë²ˆì½”ë“œ) # => ì£¼ì„ì²˜ë¦¬

# 3. for loop ì§ì ‘ êµ¬ì„± (repeat Ã— fold)

per_repeat = []   # ê° repeatì˜ ìš”ì•½ ê¸°ë¡ìš© ë¦¬ìŠ¤íŠ¸

for repeat in range(args.repeat):
    fold_aucs, accuracy, cms, recalls, precisions = [], [], [], [], []
    iter_count = 0
    for fold in range(args.n_splits):
        print(f"ğŸ” Repeat {repeat}, Fold {fold}")
        # train_path = f"/data/project/kim89/0805_data/repeat_{repeat}/fold_{fold}_train.h5ad"
        # test_path = f"/data/project/kim89/0805_data/repeat_{repeat}/fold_{fold}_test.h5ad"

        train_path = f"{args.dataset}/repeat_{repeat}/fold_{fold}_train.h5ad"
        test_path = f"{args.dataset}/repeat_{repeat}/fold_{fold}_test.h5ad"

        train_data = scanpy.read_h5ad(train_path)
        test_data = scanpy.read_h5ad(test_path)

        train_p_index, train_labels, train_cell_type, train_patient_id, train_origin = Custom_data_from_loaded(train_data, args)
        test_p_index, test_labels, test_cell_type, test_patient_id, test_origin = Custom_data_from_loaded(test_data, args)

        labels_ = np.concatenate([train_labels, test_labels])


        print(f"ğŸ” Split #{iter_count + 1}")
        print(f"  â†’ train_p_index í™˜ì ìˆ˜ (í™˜ì ë‹¨ìœ„ë¡œ ë¬¶ì¸ index list): {len(train_p_index)}")
        print(f"  â†’ test_p_index í™˜ì ìˆ˜  (í™˜ì ë‹¨ìœ„ë¡œ ë¬¶ì¸ index list): {len(test_p_index)}")
        # train_p_index_ ì•ˆì—ëŠ” **"í™˜ì ë‹¨ìœ„ ë¬¶ìŒ"**ì´ ë“¤ì–´ìˆìŠµë‹ˆë‹¤.
        # ì¦‰, ê° ì›ì†Œê°€ np.array([...]) í˜•íƒœë¡œ, í•œ í™˜ìì— ì†í•˜ëŠ” ëª¨ë“  ì…€ì˜ ì¸ë±ìŠ¤ë¥¼ ë‹´ê³  ìˆëŠ” ê±°ì˜ˆìš”.
        # ê·¸ë˜ì„œ 86629, 86630, ... ê°™ì€ ìˆ«ìëŠ” ì‹¤ì œ "ì…€ ë²ˆí˜¸(index)"ì¼ ë¿, í™˜ì IDê°€ ì•„ë‹™ë‹ˆë‹¤.

        # ì‹¤ì œ í™˜ì IDë¡œ ë³´ê¸°
        train_ids = [train_patient_id[idx[0]] for idx in train_p_index]
        test_ids = [test_patient_id[idx[0]] for idx in test_p_index]
        print(f"  â†’ train í™˜ì ID: {train_ids}")
        print(f"  â†’ test  í™˜ì ID: {test_ids}")

        # ê° í™˜ìì˜ IDì™€ label í•¨ê»˜ ì¶œë ¥
        print("  â†’ train í™˜ì ID ë° ë¼ë²¨:")
        for idxs in train_p_index:
            idx = idxs[0]
            print(f"    ID: {train_patient_id[idx]}, Label: {train_labels[idx]}")

        print("  â†’ test í™˜ì ID ë° ë¼ë²¨:")
        for idxs in test_p_index:
            idx = idxs[0]
            print(f"    ID: {test_patient_id[idx]}, Label: {test_labels[idx]}")

        p_idx = train_p_index + test_p_index # ì´ì–´ë¶™ì´ê¸°(concatenation) ë¡œ ë™ì‘
        print("ì „ì²´ ë°ì´í„° index ê¸¸ì´", len(p_idx))    

        # if args.n_splits < 0:
        #     temp_idx = train_p_index
        #     train_p_index = test_p_index
        #     test_p_index = temp_idx

        # train_labels: ì „ì²´ ì…€ ë‹¨ìœ„ ë¼ë²¨ (186,636ê°œ)
        # í™˜ì ë‹¨ìœ„ ë¼ë²¨ë§Œ ë½‘ê¸°
        label_stat = [labels_[idx[0]] for idx in train_p_index] #  train setì— í¬í•¨ëœ í™˜ìë“¤ì˜ ë¼ë²¨ ëª©ë¡
        print("label_stat (train í™˜ì ë¼ë²¨ ëª©ë¡) ê°¯ìˆ˜", len(label_stat))
        # label_stat = []
        # for idx in train_p_index:
        #     label_stat.append(labels_[p_idx[idx][0]])

        unique, cts = np.unique(label_stat, return_counts=True)
    

        # í›ˆë ¨ ë°ì´í„°(train_p_index)ì— í´ë˜ìŠ¤ê°€ 2ê°œ ì´ìƒ ì¡´ì¬í•´ì•¼ í•™ìŠµì„ ì§„í–‰í•œë‹¤.
        if len(unique) < 2 or (1 in cts): 
            # í´ë˜ìŠ¤ê°€ í•˜ë‚˜ë°–ì— ì—†ìŒ â†’ ë¶ˆê· í˜• â†’ ìŠ¤í‚µ 
            # or 
            # ë“±ì¥í•œ í´ë˜ìŠ¤ ì¤‘ í•œ í´ë˜ìŠ¤ì˜ í™˜ì ìˆ˜ê°€ 1ëª…ë°–ì— ì•ˆ ë¨ â†’ í•™ìŠµì´ ë¶ˆì•ˆì •í•´ì§ˆ ê°€ëŠ¥ì„±ì´ ë§¤ìš° ë†’ê¸° ë•Œë¬¸ì— skip
            print("í›ˆë ¨ ë°ì´í„°(train_p_index)ì— í´ë˜ìŠ¤ê°€ 2ê°œ ì´ìƒ ì¡´ì¬í•´ì•¼ í•™ìŠµì„ ì§„í–‰", flush=True)
            continue
    #     print(dict(zip(unique, cts)))
        
        # ì›ë˜ ì½”ë“œì—ëŠ” test setì˜ í´ë˜ìŠ¤ ë¶ˆê· í˜•ì€ ì²´í¬í•˜ì§€ ì•ŠìŒ
        # ### âœ… test_p_index í´ë˜ìŠ¤ í™•ì¸ ì¶”ê°€
        # test_label_stat = [labels_[idx[0]] for idx in test_p_index]
        # if len(set(test_label_stat)) < 2:
        #     print(f"âš ï¸  Skipping split: test set has only one class -> {set(test_label_stat)}")
        #     continue

        # train_dataì—ì„œ í™˜ì ë‹¨ìœ„ë¡œ, trainê³¼ validation ë‚˜ëˆ„ê¸°
        print("train_dataì—ì„œ í™˜ì ë‹¨ìœ„ë¡œ, trainê³¼ validation ë‚˜ëˆ„ê¸°")
        print("ê¸°ì¡´ train_p_index",len(train_p_index))
        print("ê¸°ì¡´ (train) label_stat",len(label_stat))

        # 0) ìµœì†Œ í´ë˜ìŠ¤ ìƒ˜í”Œ ìˆ˜ ì ê²€ (stratifyê°€ ìš”êµ¬)
        cts = Counter(label_stat)
        min_cls = min(cts.values())
        if min_cls < 2:
            print(f"âš ï¸ ìµœì†Œ í´ë˜ìŠ¤ ìˆ˜ê°€ 2 ë¯¸ë§Œ: {cts}. ì´ splitì€ ê±´ë„ˆëœë‹ˆë‹¤.")
            continue

        # 1) stratifyë¡œ í•œ ë²ˆì— í•´ê²° (while ë¶ˆí•„ìš”)
        try:
            train_p_index_, valid_p_index, ty, vy = train_test_split(
                train_p_index, 
                # label_stat,
                # [train_labels[idx[0]] for idx in train_p_index],  # í•´ë‹¹ í™˜ì ë¼ë²¨ ëª©ë¡
                label_stat,        # í™˜ì ë‹¨ìœ„ ë¼ë²¨
                test_size=0.33,
                random_state=args.seed
                # stratify=label_stat
                # stratify= [train_labels[idx[0]] for idx in train_p_index]  # í•´ë‹¹ í™˜ì ë¼ë²¨ ëª©ë¡ 
                )
        except ValueError as e:
            # 2) ì‹¤íŒ¨ ì‹œ test_size ì¶•ì†Œí•´ì„œ í•œ ë²ˆ ë” ì‹œë„
            print(f"âš ï¸ stratify ì‹¤íŒ¨({e}). test_size=0.2ë¡œ ì¬ì‹œë„í•©ë‹ˆë‹¤.")
            train_p_index_, valid_p_index, ty, vy = train_test_split(
                train_p_index, train_labels,
                test_size=0.2,
                random_state=args.seed,
                # stratify=label_stat
            )

        # 3) (ì•ˆì „ë§) ë¶„í•  í›„ í´ë˜ìŠ¤ ì „ë¶€ í¬í•¨ëëŠ”ì§€ í™•ì¸
        classes = set(label_stat)
        print("train-vali ë¶„í•  í›„! ")
        print("train y ì˜ classes :",set(ty), "valid y ì˜ classes :",set(vy), "label_stat ì˜ classes :",classes)
        if not (set(ty) == classes and set(vy) == classes):
            print(f"âš ï¸ ë¶„í•  í›„ ì¼ë¶€ í´ë˜ìŠ¤ê°€ ë¹ ì§. (train={set(ty)}, val={set(vy)}) ì´ splitì€ ê±´ë„ˆëœë‹ˆë‹¤.")
            continue

        print("train_p_index_",len(train_p_index_), flush=True)
        print("valid_p_index",len(valid_p_index), flush=True)
        print("test_p_index",len(test_p_index), flush=True)

        train_p_index = train_p_index_
        print("â†’ train í™˜ì ID ë° ë¼ë²¨:")
        print("   ì´ ê°œìˆ˜:", len(train_p_index))
        for idxs in train_p_index:
            idx0 = idxs[0]
            print(f"   í™˜ìID={train_patient_id[idx0]}, Label={train_labels[idx0]}, ì…€ê°œìˆ˜={len(idxs)}")

        print("â†’ valid í™˜ì ID ë° ë¼ë²¨:")
        print("   ì´ ê°œìˆ˜:", len(valid_p_index))
        for idxs in valid_p_index:
            idx0 = idxs[0]
            print(f"   í™˜ìID={train_patient_id[idx0]}, Label={train_labels[idx0]}, ì…€ê°œìˆ˜={len(idxs)}")


        # âœ… testëŠ” test_patient_id/test_labelsë¡œ!
        print("â†’ test í™˜ì ID ë° ë¼ë²¨:")
        print("   ì´ ê°œìˆ˜:", len(test_p_index))
        for idxs in test_p_index:
            idx0 = idxs[0]
            print(f"   í™˜ìID={test_patient_id[idx0]}, Label={test_labels[idx0]}, ì…€ê°œìˆ˜={len(idxs)}")


        train_p_index = train_p_index_
        len_valid = len(valid_p_index)
        # _index = np.concatenate([valid_p_index, test_p_index])


        # train_ids = []
        # for i in train_p_index:
        #     train_ids.append(patient_id.iloc[p_idx[i][0]])

    #     print(train_ids)

        x_train = []
        x_test = []
        x_valid = []
        y_train = []
        y_valid = []
        y_test = []
        id_train = []
        id_test = []
        id_valid = []

        train_cell_type = pd.Series([
            ct if isinstance(ct, str) else "Unknown"
            for ct in train_cell_type
        ])

        print("âœ… Checking cell_type before mixups...")
        print("Unique types:", set([type(x) for x in train_cell_type]))
        print("Example values (first 10):", list(train_cell_type[:10]))
        print("NaN exists?", any([isinstance(x, float) and np.isnan(x) for x in train_cell_type]))
        # 1. Seriesì— NaNì´ ìˆëŠ”ì§€ í™•ì‹¤íˆ í™•ì¸
        print("ğŸ” isna count:", pd.Series(train_cell_type).isna().sum())

        # 2. set ì•ˆì— float (NaN)ê°€ ì„ì—¬ ìˆëŠ”ì§€ í™•ì¸
        print("ğŸ§ª Types in set(cell_type):", set([type(x) for x in set(train_cell_type)]))

        data = np.concatenate([train_origin, test_origin])
        patient_id_all = np.concatenate([np.array(train_patient_id), np.array(test_patient_id)])

        print("ì „ì²´ ë°ì´í„° ê¸¸ì´", len(data))
        
        def to_series_1d(x):
            # numpy / list / Series ëª¨ë‘ 1ì°¨ì› Seriesë¡œ í†µì¼
            if isinstance(x, pd.Series):
                return x.reset_index(drop=True)
            return pd.Series(np.ravel(x))

        train_ct = to_series_1d(train_cell_type)
        test_ct  = to_series_1d(test_cell_type)

        # í–‰ ë°©í–¥ ì´ì–´ë¶™ì´ê¸° + ì¸ë±ìŠ¤ ì´ˆê¸°í™”
        cell_type = pd.concat([train_ct, test_ct], ignore_index=True)
        cell_type = cell_type.astype("string").fillna("Unknown")
        print("ì „ì²´ cell type ê¸¸ì´", cell_type)
        print("cell typeì˜ NaN ê°œìˆ˜ ì²´í¬",cell_type.isna().sum())
        data_augmented, train_p_idx, labels_augmented, cell_type_augmented = mixups(args, data,
                                                                            train_p_index,
                                                                            labels_,
                                                                            cell_type)
        # main.py - repeatÃ—fold ë£¨í”„ ë‚´ë¶€ì—ì„œ, train/valid/test ë¶„í• ì„ ë§Œë“  ë’¤

        # 4-1) íŠœë‹ìš©(=validë§Œ í‰ê°€ í›„ë³´) ë°ì´í„° íŒ¨í‚¹
        # mixups/sampling ì „ì— ì´ë¯¸ ë§Œë“  train_p_index, valid_p_index, test_p_index, data/labels_ê°€ ìˆë‹¤ê³  ê°€ì •
        # (í˜„ì¬ ì½”ë“œì—ì„œ ë°”ë¡œ ìœ„ê¹Œì§€ ì¤€ë¹„ë˜ëŠ” êµ¬ì¡°ì…ë‹ˆë‹¤. ì˜ˆ: eval_p_index êµ¬ì„±ë¶€ ë“±) 

        # ---> (A) ë¨¼ì € "validë§Œ" í‰ê°€ í›„ë³´ë¡œ í•´ì„œ sampling
        eval_only_valid = valid_p_index  # test ë¯¸í¬í•¨
        individual_train_A, individual_eval_A = sampling(
            args,
            train_p_idx,         # mixups() ê²°ê³¼
            eval_only_valid,     # validë§Œ
            labels_, labels_augmented, cell_type_augmented
        )

        # numpyë¡œ ë³€í™˜ (íŠœë‹ objectiveì—ì„œ train í˜¸ì¶œí•  ìˆ˜ ìˆê²Œ)
        x_train_A, y_train_A, id_train_A = [], [], []
        x_valid_A, y_valid_A, id_valid_A = [], [], []
        for t in individual_train_A:
            id_, label_ = [id_l[0] for id_l in t], [id_l[1] for id_l in t]
            x_train_A += [ii for ii in id_]
            y_train_A += label_
            id_train_A += id_
        for t in individual_eval_A:
            id_, label_ = [id_l[0] for id_l in t], [id_l[1] for id_l in t]
            x_valid_A.append([ii for ii in id_])
            y_valid_A.append(label_[0])
            id_valid_A.append(id_)

        prep_A = dict(
            x_train=x_train_A, x_valid=x_valid_A,
            y_train=np.array(y_train_A).reshape([-1,1]),
            y_valid=np.array(y_valid_A).reshape([-1,1]),
            id_train=id_train_A, id_valid=id_valid_A,
            data_augmented=data_augmented, data=data
        )

        # (B) ìµœì¢… í‰ê°€ìš©: valid + test í›„ë³´ë¥¼ í•œ ë²ˆì— ë„˜ê²¨ì„œ sampling
        offset = train_origin.shape[0]
        test_p_index_global = [idx + offset for idx in test_p_index]
        eval_valid_plus_test = valid_p_index + test_p_index_global

        individual_train_B, individual_eval_B = sampling(
            args,
            train_p_idx,                   # mixups() ê²°ê³¼
            eval_valid_plus_test,          # valid + test í›„ë³´
            labels_, labels_augmented, cell_type_augmented
        )

        # numpy/listë¡œ ë³€í™˜
        x_train_B, y_train_B, id_train_B = [], [], []
        for t in individual_train_B:
            ids  = [id_l[0] for id_l in t]
            lbls = [id_l[1] for id_l in t]
            x_train_B += [ii for ii in ids]
            y_train_B += lbls
            id_train_B += ids

        # â˜… í•µì‹¬: ì•ë¶€ë¶„ì€ valid, ë’·ë¶€ë¶„ì€ testë¡œ "ë‹¤ì‹œ ë¶„ë¦¬"
        len_valid = len(valid_p_index)
        x_valid_B, y_valid_B, id_valid_B = [], [], []
        x_test_B,  y_test_B,  id_test_B  = [], [], []

        for t_idx, t in enumerate(individual_eval_B):
            ids  = [id_l[0] for id_l in t]
            lbls = [id_l[1] for id_l in t]
            if t_idx < len_valid:
                x_valid_B.append([ii for ii in ids])
                y_valid_B.append(lbls[0])
                id_valid_B.append(ids)
            else:
                x_test_B.append([ii for ii in ids])
                y_test_B.append(lbls[0])
                id_test_B.append(ids)

        # ë°°ì—´ ëª¨ì–‘ ë§ì¶”ê¸°
        y_train_B = np.array(y_train_B).reshape([-1, 1])
        y_valid_B = np.array(y_valid_B).reshape([-1, 1])
        y_test_B  = np.array(y_test_B ).reshape([-1, 1])


        if args.heads == 4 and args.emb_dim >= 64:
            args.batch_size = min(args.batch_size, 8)


        # (1) Optunaë¡œ íŠœë‹ ; trainê³¼ validationë§Œ ë“¤ì–´ê°
        best_trials = optuna_tune_one_fold(prep_A, n_trials=3, top_k=args.top_k) ### n_trials=3 ; 3ê°€ì§€ì˜ hyperparameter tuningë§Œ ìˆ˜í–‰í•¨

        # (2) ì„ íƒëœ trial(1~3ê°œ) ê°ê°ìœ¼ë¡œ ìµœì¢… í‰ê°€(test í¬í•¨)
        # data_for_training = data_augmented if use_aug else data
        # (ì•ˆì „) top-k í›„ë³´ê°€ ë¹„ì–´ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë¨¼ì € ì²´í¬
        if len(best_trials) == 0:
            print("[WARN] íŠœë‹ ê²°ê³¼ ìœ íš¨í•œ trialì´ ì—†ì–´ ì´ foldë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            fold_aucs.append(np.nan)
            accuracy.append(np.nan)
            recalls.append(np.nan)
            precisions.append(np.nan)
            cms.append(None)
        else:
            # â¬‡ï¸ ì—¬ê¸°ì„œ best_foldë¥¼ 'ì—†ìŒ'ìœ¼ë¡œ ì´ˆê¸°í™” (ë˜ëŠ” auc=-infë¡œ ì´ˆê¸°í™”)
            best_fold = None  # or: {"auc": -float("inf")}

            for t in best_trials:
                hp = t.params
                print("ì„ íƒëœ trial params:", hp)
                # (1) hp â†’ args ì£¼ì…
                args.learning_rate = hp["learning_rate"]
                args.weight_decay  = hp["weight_decay"]
                args.epochs        = hp["epochs"]
                args.heads         = hp["heads"]
                args.dropout       = hp["dropout"]
                args.emb_dim       = hp["emb_dim"]
                args.augment_num   = hp["augment_num"]
                args.pca           = hp["pca"]

                # (2) OOM ì‚¬ì „ ê°€ë“œ
                orig_bs = args.batch_size
                if args.heads == 4 and args.emb_dim >= 64:
                    args.batch_size = min(args.batch_size, 8)

                try:
                    data_for_training = data_augmented  # ì¦ê°• ONì´ë©´ mixup ê²°ê³¼, OFF ì„¤ê³„ë©´ dataë¡œ êµì²´
                    def _call():
                        return train(
                            x_train_B, x_valid_B, x_test_B,
                            y_train_B, y_valid_B, y_test_B,
                            id_train_B, id_test_B,
                            data_for_training, data, eval_test=True
                        )

                    # (3) OOM ë˜í¼ë¡œ ê°ì‹¸ ì‹¤í–‰
                    auc, acc, cm, recall, precision, _ = train_with_oom_retry(_call, backoff=(8, 4, 2, 1))

                    # (4) test AUCê°€ ë” í¬ë©´ best í›„ë³´ë¡œ êµì²´
                    if (best_fold is None) or (auc > best_fold["auc"]):
                        best_fold = {
                            "auc": auc,
                            "acc": acc,
                            "cm": cm,
                            "recall": recall,
                            "precision": precision,
                        }

                except RuntimeError as e:
                    if "out of memory" in str(e).lower():
                        print(f"[WARN] OOM on final test for params {hp} â†’ skip this candidate.")
                        continue
                    else:
                        raise
                finally:
                    args.batch_size = orig_bs  # ì›ë³µ

            # (5) ëª¨ë“  í›„ë³´ê°€ OOM ë“±ìœ¼ë¡œ ì‹¤íŒ¨í–ˆì„ ê°€ëŠ¥ì„± ì²˜ë¦¬
            if best_fold is None:
                fold_aucs.append(np.nan)
                accuracy.append(np.nan)
                recalls.append(np.nan)
                precisions.append(np.nan)
                cms.append(None)
            else:
                # â˜… ì—¬ê¸°ì„œë§Œ ì§‘ê³„ â€” â€˜ê°€ì¥ ì¢‹ì€ í›„ë³´â€™ì˜ test ì§€í‘œ
                fold_aucs.append(best_fold["auc"])
                accuracy.append(best_fold["acc"])
                cms.append(best_fold["cm"])
                recalls.append(best_fold["recall"])
                precisions.append(best_fold["precision"])


        iter_count += 1
        if iter_count == abs(args.n_splits) * args.repeat:
            break
        print(f"âœ… Total valid splits used: {iter_count}")

        # === FOLD CLEANUP: ë‹¤ìŒ foldë¡œ ë„˜ì–´ê°€ê¸° ì „ì— í° ê°ì²´ ì •ë¦¬ ===
        # 1) í° íŒŒì´ì¬ ê°ì²´/ë°°ì—´ì€ ì§ì ‘ None í• ë‹¹ë¡œ ëŠì–´ì£¼ì„¸ìš”.
        #    (ì•„ë˜ ë³€ìˆ˜ëª…ì€ ì—¬ëŸ¬ë¶„ ì½”ë“œì— ë§ê²Œ ì¡°ì •)
        x_train_A = x_valid_A = y_train_A = y_valid_A = id_train_A = id_valid_A = None
        x_train_B = x_valid_B = x_test_B = None
        y_train_B = y_valid_B = y_test_B = None
        id_train_B = id_test_B = None
        individual_train_A = individual_eval_A = None
        individual_train_B = individual_eval_B = None

        # AnnData / ì›ë³¸ ë°°ì—´
        train_data = test_data = None
        data = data_augmented = None
        train_p_idx = labels_augmented = cell_type_augmented = None

        # (Optuna ê°ì²´ë„ foldë³„ë¡œ ìƒì„±í–ˆë‹¤ë©´)
        try:
            study = None
        except NameError:
            pass

        # 2) ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ + CUDA ìºì‹œ ë¹„ìš°ê¸°
        import gc, torch
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            if hasattr(torch.cuda, "ipc_collect"):
                torch.cuda.ipc_collect()

        
    # ğŸ”½ Repeat ë‹¨ìœ„ AUC/ì§€í‘œ ì¶œë ¥
    rep_auc_mean = float(np.nanmean(fold_aucs)) if len(fold_aucs) else float("nan")
    rep_auc_std  = float(np.nanstd(fold_aucs))  if len(fold_aucs) else float("nan")
    rep_acc_mean = float(np.nanmean(accuracy))  if len(accuracy) else float("nan")
    rep_rec_mean = float(np.nanmean(recalls))   if len(recalls) else float("nan")
    rep_pre_mean = float(np.nanmean(precisions)) if len(precisions) else float("nan")
    nan_count    = int(np.count_nonzero(np.isnan(fold_aucs)))
    n_folds      = int(len(fold_aucs))

    print(f"\nğŸ“Œ Repeat {repeat}: í‰ê·  AUC = {rep_auc_mean:.4f}, í‘œì¤€í¸ì°¨ = {rep_auc_std:.4f}")
    print(f"Test ACC í‰ê·  {rep_acc_mean:.6f}, Test Recall í‰ê·  {rep_rec_mean:.6f}, Test Precision í‰ê·  {rep_pre_mean:.6f}")
    print("fold_aucs =", fold_aucs)
    print(f"NaN ê°œìˆ˜: {nan_count} / ì „ì²´ {n_folds}ê°œ\n")

    # â¬‡ï¸ ì´ repeatì˜ ìš”ì•½ì„ ì €ì¥ (ë‚˜ì¤‘ì— ì „ì²´ ìš”ì•½ì—ì„œ ì‚¬ìš©)
    per_repeat.append({
        "repeat": repeat,
        "auc_mean": rep_auc_mean,
        "auc_std": rep_auc_std,
        "acc_mean": rep_acc_mean,
        "recall_mean": rep_rec_mean,
        "precision_mean": rep_pre_mean,
        "nan_count": nan_count,
        "n_folds": n_folds,
    })

    # === REPEAT CLEANUP === (ì˜µì…˜)
    import gc, torch
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        if hasattr(torch.cuda, "ipc_collect"):
            torch.cuda.ipc_collect()

# â¬‡ï¸ ëª¨ë“  Repeatê°€ ëë‚œ ë’¤, ê° Repeatì˜ ê°’ê³¼ ìµœì¢… í‰ê·  ì¶œë ¥
print("\n================ ì „ì²´ Repeat ìš”ì•½ ================")
for s in per_repeat:
    print(
        f"Repeat {s['repeat']}: "
        f"AUC {s['auc_mean']:.4f} Â± {s['auc_std']:.4f} | "
        f"ACC {s['acc_mean']:.4f} | "
        f"Recall {s['recall_mean']:.4f} | "
        f"Precision {s['precision_mean']:.4f} | "
        f"NaN {s['nan_count']}/{s['n_folds']}"
    )

# ìµœì¢… ê²°ê³¼: â€œ5ê°œì˜ foldë¥¼ í‰ê· í•œ ê° Repeatì˜ AUC í‰ê· ë“¤â€ì˜ í‰ê· (=macro í‰ê· )
if len(per_repeat):
    final_auc_mean = float(np.nanmean([s["auc_mean"] for s in per_repeat]))
    final_auc_std  = float(np.nanstd([s["auc_mean"]  for s in per_repeat]))
    print(f"\nğŸ ìµœì¢… ê²°ê³¼ (ê° Repeatì˜ AUC í‰ê· ì˜ í‰ê· ): {final_auc_mean:.4f} Â± {final_auc_std:.4f}")
else:
    print("\nğŸ ìµœì¢… ê²°ê³¼ë¥¼ ê³„ì‚°í•  Repeat ìš”ì•½ì´ ì—†ìŠµë‹ˆë‹¤.")

