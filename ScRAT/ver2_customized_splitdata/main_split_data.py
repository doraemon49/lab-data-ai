from sklearn import metrics
from sklearn.metrics import accuracy_score
import scipy.stats as st
from torch.optim import Adam
from utils import *
import argparse
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import RepeatedKFold
from sklearn.model_selection import train_test_split

from model_baseline import *
from Transformer import TransformerPredictor

from dataloader import *
from collections import defaultdict
import pandas as pd
import numpy as np

from collections import Counter

import functools, sys
# ëª¨ë“  print() ê¸°ë³¸ flush=Trueë¡œ ê°•ì œ
print = functools.partial(print, flush=True)
# (íŒŒì´ì¬ 3.7+ ì´ìƒ) stdoutì„ ì¤„ë‹¨ìœ„ ë²„í¼ë§ìœ¼ë¡œ ë³€ê²½
try:
    sys.stdout.reconfigure(line_buffering=True)
except Exception:
    pass


def _str2bool(v):
    return v.lower() in ("yes", "y", "true", "t", "1")


def int_or_float(x):
    try:
        return int(x)
    except ValueError:
        return float(x)


parser = argparse.ArgumentParser(description='scRNA diagnosis')

parser.add_argument('--seed', type=int, default=240)
parser.add_argument('--batch_size', type=int, default=16)
parser.add_argument('--learning_rate', type=float, default=3e-3)
parser.add_argument('--weight_decay', type=float, default=1e-4)
parser.add_argument('--epochs', type=int, default=100)
parser.add_argument("--task", type=str, default="severity")
parser.add_argument('--emb_dim', type=int, default=128)  # embedding dim
parser.add_argument('--h_dim', type=int, default=128)  # hidden dim of the model
parser.add_argument('--dropout', type=float, default=0.3)  # dropout
parser.add_argument('--layers', type=int, default=1)
parser.add_argument('--heads', type=int, default=8)
parser.add_argument("--train_sample_cells", type=int, default=500,
                    help="number of cells in one sample in train dataset") # í•™ìŠµ ì‹œ ê° í™˜ì ìƒ˜í”Œì—ì„œ 500ê°œ ì„¸í¬ë¥¼ ëœë¤ ì„ íƒ # ì‚¬ìš© 1: ë°ì´í„° ë¡œë“œ ì‹œ í™˜ì ëª…ìˆ˜ ì¡°ì •. # ì‚¬ìš©2: --all 1 í•´ì•¼ì§€ë§Œ ëœë¤ ìƒ˜í”Œë§ ì§„í–‰
parser.add_argument("--test_sample_cells", type=int, default=500,
                    help="number of cells in one sample in test dataset") # í…ŒìŠ¤íŠ¸ ì‹œì—ë„ ë™ì¼í•˜ê²Œ 500ê°œ ì„¸í¬ ì„ íƒ
parser.add_argument("--train_num_sample", type=int, default=20,
                    help="number of sampled data points in train dataset") # í•œ ëª…ì˜ í™˜ìì—ì„œ 500ê°œì˜ ì„¸í¬ë¥¼ 20ë²ˆ ìƒ˜í”Œë§í•˜ì—¬ 20ê°œì˜ bag ìƒì„±
parser.add_argument("--test_num_sample", type=int, default=100,
                    help="number of sampled data points in test dataset") # í…ŒìŠ¤íŠ¸ë„ ê°™ì€ ë°©ì‹ìœ¼ë¡œ 100ê°œì˜ bag ìƒì„±
parser.add_argument('--model', type=str, default='Transformer')
parser.add_argument('--dataset', type=str, default=None)
parser.add_argument('--inter_only', type=_str2bool, default=False) # mixupëœ ìƒ˜í”Œë§Œ í•™ìŠµì— ì‚¬ìš©í• ì§€ ì—¬ë¶€
parser.add_argument('--same_pheno', type=int, default=0) # ê°™ì€ í´ë˜ìŠ¤ë¼ë¦¬ mixupí• ì§€, ë‹¤ë¥¸ í´ë˜ìŠ¤ë¼ë¦¬ í• ì§€
# augment_num == 0 â†’ mixup ì•ˆ í•¨ â†’ same_pheno ë¬´ì˜ë¯¸
# augment_num > 0 â†’ mixup ì‹¤í–‰ë¨ â†’ same_phenoê°€ í™˜ì ìŒ ì„ íƒ ê·œì¹™ì— ì§ì ‘ ì˜í–¥
    # same_pheno=1 â†’ ê°™ì€ í´ë˜ìŠ¤ ë‚´ë¶€ì˜ ë‹¤ì–‘ì„±ì„ í‚¤ìš°ê³  ì‹¶ì„ ë•Œ (í´ë˜ìŠ¤ ê°„ ê²½ê³„ë¥¼ íë¦¬ì§€ ì•ŠìŒ)
    # same_pheno=-1 â†’ í´ë˜ìŠ¤ ê°„ ê²½ê³„ë¥¼ ë¶€ë“œëŸ½ê²Œ í•˜ì—¬ ëª¨ë¸ ì¼ë°˜í™” ìœ ë„
    # same_pheno=0 â†’ ë°ì´í„° ìˆ˜ê°€ ì ê±°ë‚˜ í´ë˜ìŠ¤ ë¶ˆê· í˜•ì´ ì‹¬í•  ë•Œ ë¬´ì‘ìœ„ë¡œ ì„ì–´ ë‹¤ì–‘ì„± ê·¹ëŒ€í™”
parser.add_argument('--augment_num', type=int, default=0) # Mixupëœ ìƒˆë¡œìš´ ê°€ì§œ ìƒ˜í”Œì„ ëª‡ ê°œ ìƒì„±í• ì§€
parser.add_argument('--alpha', type=float, default=1.0) # mixupì˜ ë¹„ìœ¨ (Beta ë¶„í¬ íŒŒë¼ë¯¸í„°)
parser.add_argument('--repeat', type=int, default=3)
parser.add_argument('--all', type=int, default=1)
# all == 0:
    # sample_cells ë§Œí¼ ëœë¤ ìƒ˜í”Œë§ (np.random.choice)
# all == 1
    # ìƒ˜í”Œë§ì„ ê±´ë„ˆë›°ê³  'í•´ë‹¹ í™˜ì(í˜¹ì€ ë¼ë²¨)ì˜ ëª¨ë“  ì…€'ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
parser.add_argument('--min_size', type=int, default=6000)
parser.add_argument('--n_splits', type=int, default=5)
parser.add_argument('--pca', type=_str2bool, default=True)
parser.add_argument('--mix_type', type=int, default=1)
parser.add_argument('--norm_first', type=_str2bool, default=False)
parser.add_argument('--warmup', type=_str2bool, default=False)
parser.add_argument('--top_k', type=int, default=1)

parser.add_argument('--cell_type_annotation', type=str, default="manual_annotation",
    help="ì‚¬ìš©í•  cell type annotation ì»¬ëŸ¼ëª… (manual_annotation ë˜ëŠ” singler_annotation)")

args = parser.parse_args()

# print("# of GPUs is", torch.cuda.device_count())
print(args)

torch.manual_seed(args.seed)
np.random.seed(args.seed)

patient_summary = {}
stats = {}
stats_id = {}

if args.task == 'haniffa' or args.task == 'combat':
    label_dict = {0: 'Non Covid', 1: 'Covid'}
elif args.task == 'severity':
    label_dict = {0: 'mild', 1: 'severe'}
elif args.task == 'stage':
    label_dict = {0: 'convalescence', 1: 'progression'}
elif args.task == 'custom_cardio':
    label_dict = {
    0: 'normal',
    1: 'hypertrophic cardiomyopathy',
    2: 'dilated cardiomyopathy'
}
elif args.task == 'custom_covid':
    label_dict = {0: 'normal', 1: 'COVID-19'}
elif args.task == 'custom_kidney':
    label_dict = {
        0:'Healthy',  # ë‘ healthy subtypeì„ í•˜ë‚˜ì˜ ëŒ€í‘œëª…ìœ¼ë¡œ
        1:'CKD',
        2:'AKI'
} 

def safe_gather(data_mat, index_tensor):
    # index_tensor: LongTensor (B, N) ë˜ëŠ” (N,)
    idx = index_tensor.clone()
    idx[idx < 0] = 0  # -1 íŒ¨ë”©ì€ 0ìœ¼ë¡œ ì¹˜í™˜í•´ ì•ˆì „ ì¸ë±ì‹±
    return torch.from_numpy(data_mat[idx.cpu().numpy()])


def train(x_train, x_valid, x_test, y_train, y_valid, y_test, id_train, id_test, data_augmented, data):
    dataset_1 = MyDataset(x_train, x_valid, x_test, y_train, y_valid, y_test, id_train, id_test, fold='train')
    dataset_2 = MyDataset(x_train, x_valid, x_test, y_train, y_valid, y_test, id_train, id_test, fold='test')
    dataset_3 = MyDataset(x_train, x_valid, x_test, y_train, y_valid, y_test, id_train, id_test, fold='val')
    train_loader = torch.utils.data.DataLoader(dataset_1, batch_size=args.batch_size, shuffle=True,
                                               collate_fn=dataset_1.collate)
    test_loader = torch.utils.data.DataLoader(dataset_2, batch_size=1, shuffle=False, collate_fn=dataset_2.collate)
    valid_loader = torch.utils.data.DataLoader(dataset_3, batch_size=1, shuffle=False, collate_fn=dataset_3.collate)

    print("ğŸ‘‰ train_loader ê¸¸ì´ (ìƒ˜í”Œìˆ˜/batchí¬ê¸°):", len(train_loader))
    print("ğŸ‘‰ test_loader ê¸¸ì´:", len(test_loader))
    print("ğŸ‘‰ valid_loader ê¸¸ì´:", len(valid_loader))


    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    input_dim = data_augmented[0].shape[-1]

    output_class = len(set(labels_))
    if (output_class == 2):
        output_class = 1

    if args.task == 'custom_covid':
        output_class = 1
    if args.task == 'custom_cardio':
        output_class = 3
    if args.task == 'custom_kidney':
        output_class = 3

    print("output_class : ", output_class)

    if args.model == 'Transformer':
        model = TransformerPredictor(input_dim=input_dim, model_dim=args.emb_dim, num_classes=output_class,
                                     num_heads=args.heads, num_layers=args.layers, dropout=args.dropout,
                                     input_dropout=0, pca=args.pca, norm_first=args.norm_first)
    elif args.model == 'feedforward':
        model = FeedForward(input_dim=input_dim, h_dim=args.emb_dim, cl=output_class, dropout=args.dropout)
    elif args.model == 'linear':
        model = Linear_Classfier(input_dim=input_dim, cl=output_class)
    elif args.model == 'scfeed':
        model = scFeedForward(input_dim=input_dim, cl=output_class, model_dim=args.emb_dim, dropout=args.dropout, pca=args.pca)

    model = nn.DataParallel(model)
    torch.cuda.empty_cache() # ëª¨ë¸ì„ GPUë¡œ ì˜¬ë¦¬ê¸° ì „ì— ìºì‹œ ë¹„ìš°ê¸°
    model.to(device)
    best_model = model

    allocated = torch.cuda.memory_allocated()
    # í˜„ì¬ ì˜ˆì•½ëœ ë©”ëª¨ë¦¬ (ìºì‹œ í¬í•¨)
    reserved = torch.cuda.memory_reserved()

    # MB ë‹¨ìœ„ë¡œ ë³´ê¸°
    print(f"Memory Allocated: {allocated / 1024 ** 2:.2f} MB")
    print(f"Memory Reserved: {reserved / 1024 ** 2:.2f} MB")

    print(device)

    ################################################################
    # training and evaluation
    ################################################################
    optimizer = Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)
    if args.warmup:
        scheduler = transformers.get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=args.epochs // 10,
                                                                 num_training_steps=args.epochs)
    else:
        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5, last_epoch=-1)
    sigmoid = torch.nn.Sigmoid().to(device)

    max_acc, max_epoch, max_auc, max_loss, max_valid_acc, max_valid_auc = 0, 0, 0, 0, 0, 0
    test_accs, valid_aucs, train_losses, valid_losses, train_accs, test_aucs = [], [0.], [], [], [], []
    best_valid_loss = float("inf")
    wrongs = []
    trigger_times = 0
    patience = 2
    for ep in (range(1, args.epochs + 1)):
        model.train()
        train_loss = []


        for batch in (train_loader):
            x_ = torch.from_numpy(data_augmented[batch[0]]).float().to(device)
            y_ = batch[1].to(device)
            mask_ = batch[3].to(device)
            optimizer.zero_grad()

            out = model(x_, mask_)

            if output_class==1:
                loss = nn.BCELoss()(sigmoid(out), y_)
            elif output_class==3:
                # === Multi-class (ì˜ˆ: 3-class) í•™ìŠµ ì†ì‹¤ ===
                # out: (B, N, C) ë˜ëŠ” (N, C) ë¡œ ê°€ì •. ë§ˆì§€ë§‰ ì°¨ì›ì´ C(í´ë˜ìŠ¤ ìˆ˜).
                # y_: (B, 1) í˜•íƒœ(float)ë¡œ ë“¤ì–´ì˜¤ë¯€ë¡œ longìœ¼ë¡œ ë³€í™˜ í›„, ì…€ ìˆ˜(N)ë§Œí¼ í™•ì¥.
                if out.dim() == 3:
                    B, N, C = out.shape
                    targets = y_.long().view(B, 1).expand(B, N).long()           # (B, N)
                    loss = nn.CrossEntropyLoss()(out.reshape(-1, C), targets.reshape(-1))
                elif out.dim() == 2:
                    N, C = out.shape
                    targets = y_.view(-1).long()           # âœ… [B]
                    # í•„ìš”í•˜ë©´ outì˜ ë°°ì¹˜ Bì™€ ì¼ì¹˜í•˜ëŠ”ì§€ assert
                    assert targets.shape[0] == out.shape[0], f"{targets.shape} vs {out.shape}"

                    loss = nn.CrossEntropyLoss()(out, targets)
                else:
                    raise RuntimeError(f"Unexpected logits shape for multiclass: {out.shape}")


            loss.backward()

            optimizer.step()
            train_loss.append(loss.item())

        scheduler.step()

        train_loss = sum(train_loss) / len(train_loss)
        train_losses.append(train_loss)

        if ep % 1 == 0:
            valid_loss = []
            model.eval()
            pred = []
            true = []
            with torch.no_grad():
                for batch in (valid_loader):
                    x_ = torch.from_numpy(data[batch[0]]).float().to(device).squeeze(0)
                    y_ = batch[1].int().to(device)

                    out = model(x_)
                    if output_class == 1:
                        out = sigmoid(out)

                        loss = nn.BCELoss()(out, y_ * torch.ones(out.shape).to(device))
                        valid_loss.append(loss.item())

                        out = out.detach().cpu().numpy()

                        # majority voting
                        f = lambda x: 1 if x > 0.5 else 0
                        func = np.vectorize(f)
                        out = np.argmax(np.bincount(func(out).reshape(-1))).reshape(-1)
                        pred.append(out)
                        y_ = y_.detach().cpu().numpy()
                        true.append(y_)
                    else:
                        # === Multiclass ===
                        # ì…€ë³„ ë¡œì§“ì´ ë‚˜ì˜¤ë©´ bag í‰ê· ìœ¼ë¡œ ì¶•ì•½ í›„ CE
                        if out.dim() == 3:          # [B=1, N, C]
                            logits = out.mean(dim=1)        # [1, C]
                        else:                        # [1, C] ë˜ëŠ” [N, C]ì¸ ê²½ìš°
                            if out.dim() == 2 and out.shape[0] > 1:  # [N, C]ë¼ë©´ Nì¶• í‰ê· ìœ¼ë¡œ bag ë¡œì§“ ë§Œë“¤ê¸°
                                logits = out.mean(dim=0, keepdim=True)  # [1, C]
                            else:
                                logits = out                         # [1, C]

                        loss = nn.CrossEntropyLoss()(logits, y_.view(-1).long())
                        valid_loss.append(loss.item())

                        # ì˜ˆì¸¡/ì •ë‹µ ê¸°ë¡
                        probs = torch.softmax(logits, dim=-1)     # [1, C]
                        pred_cls = int(torch.argmax(probs, dim=-1).item())
                        pred.append(pred_cls)
                        true.append(int(y_.view(-1).item()))

                


            # pred = np.concatenate(pred)
            # true = np.concatenate(true)

            valid_loss = sum(valid_loss) / len(valid_loss)
            valid_losses.append(valid_loss)

            if (valid_loss < best_valid_loss):
                best_model = copy.deepcopy(model)
                max_epoch = ep
                best_valid_loss = valid_loss
                max_loss = train_loss

            print("Epoch %d, Train Loss %f, Valid_loss %f" % (ep, train_loss, valid_loss))

            # Early stop
            if (ep > args.epochs - 50) and ep > 1 and (valid_loss > valid_losses[-2]):
                trigger_times += 1
                if trigger_times >= patience:
                    break
            else:
                trigger_times = 0

    best_model.eval()
    test_id = []
    wrong = []
    if output_class == 1:
        pred = []
        true = []
        prob = []

    else:
        preds_mc,  trues_mc,  probvecs_mc = [], [], []   # probvecs_mc: ê° ìƒ˜í”Œì˜ softmax í™•ë¥  ë²¡í„°

    with torch.no_grad():
        for batch in (test_loader):
            x_ = torch.from_numpy(data[batch[0]]).float().to(device).squeeze(0)
            y_ = batch[1].int().numpy()
            id_ = batch[2][0]

            out = best_model(x_)

            if output_class == 1:
            # === Binary Classification ===
                out = sigmoid(out)
                out = out.detach().cpu().numpy().reshape(-1)

                y_ = y_[0][0]
                true.append(y_)

                if args.model != 'Transformer':
                    prob.append(out[0])
                else:
                    prob.append(out.mean())

                # majority voting
                f = lambda x: 1 if x > 0.5 else 0
                func = np.vectorize(f)
                out = np.argmax(np.bincount(func(out).reshape(-1))).reshape(-1)[0]
                pred.append(out)
                test_id.append([batch[2][0]])
                if out != y_:
                    wrong.append([batch[2][0]])

            else:
                # ===== Multiclass =====
                # logitsì„ [1, C]ë¡œ ì¶•ì•½
                if logits.dim() == 3:            # [B=1, N, C]
                    logits_bag = logits.mean(dim=1)      # [1, C]
                elif logits.dim() == 2:          # [N, C] ë˜ëŠ” [1, C]
                    if logits.shape[0] > 1:      # [N, C]ì´ë©´ N í‰ê· 
                        logits_bag = logits.mean(dim=0, keepdim=True)   # [1, C]
                    else:
                        logits_bag = logits                         # [1, C]
                else:
                    raise RuntimeError(f"Unexpected logits shape: {logits.shape}")

                # ì •ë‹µ (ìŠ¤ì¹¼ë¼)
                # y_ = batch[1]  # torch.FloatTensor shape [1,1] (DataLoader collateì—ì„œ ë³´ì¥)
                # y_true = int(y_.detach().cpu().reshape(-1)[0].item())  # âœ… ì•ˆì „
                
                # y_ê°€ numpyì¼ ìˆ˜ë„, torchì¼ ìˆ˜ë„ ìˆì„ ë•Œì˜ ë²”ìš© ì²˜ë¦¬
                if torch.is_tensor(y_):
                    y_true = int(y_.detach().cpu().reshape(-1)[0].item())
                else:
                    y_true = int(np.array(y_).reshape(-1)[0])



                # logits -> bag ë¡œì§“
                if out.dim() == 3:       # [B=1, N, C]
                    logits = out.mean(dim=1)                 # [1, C]
                elif out.dim() == 2:     # [N, C] ë˜ëŠ” [1, C]
                    logits = out.mean(dim=0, keepdim=True) if out.shape[0] > 1 else out   # [1, C]
                else:
                    raise RuntimeError(f"Unexpected logits shape: {out.shape}")

                p = torch.softmax(logits, dim=-1).squeeze(0)  # [C]
                y_pred = int(torch.argmax(p).item())

                probvecs_mc.append(p.cpu().numpy())  # AUCìš© í™•ë¥  ë²¡í„°
                preds_mc.append(y_pred)
                trues_mc.append(y_true)
                test_id.append(batch[2][0])         # or test_patient_id[...] ë¡œê¹…
                if y_pred != y_true:
                    wrong.append(test_id[-1])
                

    # if len(wrongs) == 0:
    #     wrongs = set(wrong)
    # else:
    #     wrongs = wrongs.intersection(set(wrong))
    
    # ====== ì§‘ê³„ ë° ì§€í‘œ ======
    if output_class == 1:
        test_acc = accuracy_score(true, pred)
        try:
            test_auc = metrics.roc_auc_score(true, prob)
        except ValueError:
            print("âš ï¸ AUC ê³„ì‚° ë¶ˆê°€: test set í´ë˜ìŠ¤ê°€ ë‹¨ì¼ì…ë‹ˆë‹¤.")
            test_auc = np.nan

        cm = confusion_matrix(true, pred).ravel()
        if len(cm) == 4:
            recall    = cm[3] / (cm[3] + cm[2]) if (cm[3] + cm[2]) > 0 else np.nan
            precision = cm[3] / (cm[3] + cm[1]) if (cm[3] + cm[1]) > 0 else np.nan
        else:
            recall = precision = np.nan

        # ë¡œê·¸
        for i in range(len(pred)):
            print(f"{test_ids[i]} -- true: {label_dict[true[i]]} -- pred: {label_dict[pred[i]]}")

    else: # multi-class
        test_acc = accuracy_score(trues_mc, preds_mc)
        try:
            # probvecs_mc: (num_samples, C) ë¡œ ë³€í™˜
            prob_matrix = np.vstack(probvecs_mc)   # shape [N, C]
            test_auc = metrics.roc_auc_score(trues_mc, prob_matrix, multi_class='ovo') # , average='weighted'
        except ValueError:
            print("âš ï¸ AUC ê³„ì‚° ë¶ˆê°€: test setì— ëª¨ë“  í´ë˜ìŠ¤ê°€ í¬í•¨ë˜ì§€ ì•ŠìŒ")
            test_auc = np.nan

        cm = confusion_matrix(trues_mc, preds_mc)
        from sklearn.metrics import precision_recall_fscore_support
        precision, recall, _, _ = precision_recall_fscore_support(
            trues_mc, preds_mc, average='macro', zero_division=0
        )

    # ê³µí†µ ì¶œë ¥
    print("Best performance: Epoch %d, Loss %f, Test ACC %f, Test AUC %f, Test Recall %f, Test Precision %f" %
        (max_epoch, max_loss, test_acc, test_auc, recall, precision))
    print("Confusion Matrix:\n", cm)
    # # AUC ê³„ì‚° ë°©ì‹ ë¶„ê¸°
    # try:
    #     if output_class == 1:
    #         test_acc = accuracy_score(true, pred)
    #         test_auc = metrics.roc_auc_score(true, prob)
    #     else:
    #         test_acc = accuracy_score(trues_mc, preds_mc)
    #         # probvecs_mc: (num_samples, C) ë¡œ ë³€í™˜
    #         prob_matrix = np.vstack(probvecs_mc)   # shape [N, C]
    #         test_auc = metrics.roc_auc_score(trues_mc, prob_matrix, multi_class='ovr', average='weighted')
    # except ValueError:
    #     print("âš ï¸ AUC ê³„ì‚° ë¶ˆê°€: test setì— ëª¨ë“  í´ë˜ìŠ¤ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ (ì´ì§„ë¶„ë¥˜ì˜ ê²½ìš° í•˜ë‚˜ì˜ classë§Œ ì¡´ì¬. ì‚¼ì¤‘ë¶„ë¥˜ì˜ ê²¨ìš° ë‘ê°œì˜ classë§Œ ì¡´ì¬)")
    #     test_auc = np.nan


    # # for idx in range(len(pred)):
    # #     print(f"{test_id[idx]} -- true: {label_dict[true[idx]]} -- pred: {label_dict[pred[idx]]}")
    # test_accs.append(test_acc)

    # print("true : ", true)
    # print("pred : ", pred)

    # # Confusion Matrix ë° ì§€í‘œ ë¶„ê¸° # í•˜ë‚˜ì˜ ì„ê³„ê°’(ë³´í†µ 0.5)ì—ì„œì˜ ìŠ¤ëƒ…ìƒ·ì¼ ë¿
    # if output_class == 1:
    #     cm = confusion_matrix(true, pred).ravel()

    #     if len(cm) == 4:
    #         recall = cm[3] / (cm[3] + cm[2]) if (cm[3] + cm[2]) > 0 else np.nan
    #         precision = cm[3] / (cm[3] + cm[1]) if (cm[3] + cm[1]) > 0 else np.nan
    #     else:
    #         print("âš ï¸ Skipping evaluation due to insufficient class diversity")
    #         recall = precision = np.nan
    #     print("Confusion Matrix: " + str(cm))

    # else:  #  multiclass ì¼ ë•Œì˜ cm, recall, precision ì •ì˜í•´ì£¼ì„¸ìš”
    #     # ë©€í‹°í´ë˜ìŠ¤ í˜¼ë™í–‰ë ¬ (ì •ë°© í–‰ë ¬)
    #     cm = confusion_matrix(trues_mc, preds_mc)
    #     # ë§¤í¬ë¡œ í‰ê· (í´ë˜ìŠ¤ ê· ë“± ê°€ì¤‘) â€” ìƒí™©ì— ë”°ë¼ 'weighted'ë¥¼ ì¨ë„ ë©ë‹ˆë‹¤.
    #     from sklearn.metrics import precision_recall_fscore_support
    #     precision, recall, _, _ = precision_recall_fscore_support(
    #         trues_mc, preds_mc, average='macro', zero_division=0
    #     )
    #     print("Confusion Matrix:\n", cm)

    # # ë¡œê·¸
    # for i in range(len(preds)):
    #     print(f"{test_ids[i]} -- true: {label_dict[trues[i]]} -- pred: {label_dict[preds[i]]}")
    # print("true : ", trues)
    # print("pred : ", preds)
    # if output_class == 1:
    #     print("Confusion Matrix:", cm)
    # else:
    #     print("Confusion Matrix:\n", cm)


    # print("Best performance: Epoch %d, Loss %f, Test ACC %f, Test AUC %f, Test Recall %f, Test Precision %f" % (
    # max_epoch, max_loss, test_acc, test_auc, recall, precision))
    # print("Confusion Matrix: " + str(cm))
    # for w in wrongs:
    #     v = patient_summary.get(w, 0)
    #     patient_summary[w] = v + 1

    return test_auc, test_acc, cm, recall, precision


if args.model != 'Transformer':
    args.repeat = 60

# 1. ê¸°ì¡´ ì½”ë“œ
# if args.task != 'custom':
#     p_idx, labels_, cell_type, patient_id, data, cell_type_large = Covid_data(args)
# else:
#     p_idx, labels_, cell_type, patient_id, data, cell_type_large = Custom_data(args)

"""
# 2. covid, cardioë¥¼ ìœ„í•œ custom ì¶”ê°€ ì½”ë“œ
# if args.task == 'custom_cardio':
#     p_idx, labels_, cell_type, patient_id, data, cell_type_large = Custom_data(args)
# elif args.task == 'custom_covid':
#     p_idx, labels_, cell_type, patient_id, data, cell_type_large = Custom_data(args)

# else:
#     p_idx, labels_, cell_type, patient_id, data, cell_type_large = Covid_data(args)


# ë‚´ë¶€ì—ì„œ ëœë¤í•˜ê²Œ splitì„ ìƒì„± 
# rkf = RepeatedKFold(n_splits=abs(args.n_splits), n_repeats=args.repeat * 100, random_state=args.seed)

# num = np.arange(len(p_idx))
# accuracy, aucs, cms, recalls, precisions = [], [], [], [], []
# iter_count = 0

# #  class ë¶„í¬ì™€ split êµ¬ì„± í™•ì¸
# from collections import Counter
# print(Counter(labels_))  # class ë‹¹, ì „ì²´ cell ë‹¨ìœ„ ë¼ë²¨ ë¶„í¬ # ex) Counter({1: 235252, 0: 185441, 2: 171996})
# patient_classes = [labels_[p[0]] for p in p_idx]
# print("í™˜ì ìˆ˜ ê¸°ì¤€ í´ë˜ìŠ¤ ë¶„í¬:")
# print(Counter(patient_classes))
# # for i, p in enumerate(p_idx):
# #     print(f"Sample {i} - Class: {labels_[p[0]]}")


for train_index, test_index in rkf.split(num):
    print(f"ğŸ” Split #{iter_count + 1}")
    print(f"  â†’ train_index í™˜ì ìˆ˜: {len(train_index)}")
    print(f"  â†’ test_index í™˜ì ìˆ˜: {len(test_index)}")

    # ì‹¤ì œ í™˜ì IDë¡œ ë³´ê¸°
    train_ids = [patient_id[p_idx[i][0]] for i in train_index]
    test_ids = [patient_id[p_idx[i][0]] for i in test_index]
    print(f"  â†’ train í™˜ì ID: {train_ids}")
    print(f"  â†’ test  í™˜ì ID: {test_ids}")

    if args.n_splits < 0:
        temp_idx = train_index
        train_index = test_index
        test_index = temp_idx

    label_stat = [] #  train setì— í¬í•¨ëœ í™˜ìë“¤ì˜ ë¼ë²¨ ëª©ë¡
    for idx in train_index:
        label_stat.append(labels_[p_idx[idx][0]])
    unique, cts = np.unique(label_stat, return_counts=True)
    # í›ˆë ¨ ë°ì´í„°(train_index)ì— í´ë˜ìŠ¤ê°€ 2ê°œ ì´ìƒ ì¡´ì¬í•´ì•¼ í•™ìŠµì„ ì§„í–‰í•œë‹¤.
    if len(unique) < 2 or (1 in cts): 
        # í´ë˜ìŠ¤ê°€ í•˜ë‚˜ë°–ì— ì—†ìŒ â†’ ë¶ˆê· í˜• â†’ ìŠ¤í‚µ 
        # or 
        # ë“±ì¥í•œ í´ë˜ìŠ¤ ì¤‘ í•œ í´ë˜ìŠ¤ì˜ í™˜ì ìˆ˜ê°€ 1ëª…ë°–ì— ì•ˆ ë¨ â†’ í•™ìŠµì´ ë¶ˆì•ˆì •í•´ì§ˆ ê°€ëŠ¥ì„±ì´ ë§¤ìš° ë†’ê¸° ë•Œë¬¸ì— skip
        continue
#     print(dict(zip(unique, cts)))
    
    # ì›ë˜ ì½”ë“œì—ëŠ” test setì˜ í´ë˜ìŠ¤ ë¶ˆê· í˜•ì€ ì²´í¬í•˜ì§€ ì•ŠìŒ
    ### âœ… test_index í´ë˜ìŠ¤ í™•ì¸ ì¶”ê°€
    test_labels = [labels_[p_idx[i][0]] for i in test_index]
    if len(set(test_labels)) < 2:
        print(f"âš ï¸  Skipping split: test set has only one class -> {set(test_labels)}")
        continue

    kk = 0
    while True:
        train_index_, valid_index, ty, vy = train_test_split(train_index, label_stat, test_size=0.33,
                                                             random_state=args.seed + kk)
        if len(set(ty)) == 2 and len(set(vy)) == 2:
            break
        kk += 1

    train_index = train_index_
    len_valid = len(valid_index)
    _index = np.concatenate([valid_index, test_index])

    train_ids = []
    for i in train_index:
        # train_ids.append(patient_id[p_idx[i][0]])
        # pca False ; ìˆ˜ì • 1
        train_ids.append(patient_id.iloc[p_idx[i][0]])

#     print(train_ids)

    x_train = []
    x_test = []
    x_valid = []
    y_train = []
    y_valid = []
    y_test = []
    id_train = []
    id_test = []
    id_valid = []
    data_augmented, train_p_idx, labels_augmented, cell_type_augmented = mixups(args, data,
                                                                                [p_idx[idx] for idx in train_index],
                                                                                labels_,
                                                                                cell_type)
    # mixup ì‹¤íŒ¨ ì‹œ split ê±´ë„ˆë›°ê¸°
    if data_augmented is None:
        print("âš ï¸ Skipping split due to insufficient mixup class diversity.")
        continue
    individual_train, individual_test = sampling(args, train_p_idx, [p_idx[idx] for idx in _index], labels_,
                                                 labels_augmented, cell_type_augmented)
    for t in individual_train:
        id, label = [id_l[0] for id_l in t], [id_l[1] for id_l in t]
        x_train += [ii for ii in id]
        y_train += (label)
        id_train += (id)

    temp_idx = np.arange(len(_index))
    for t_idx in temp_idx[len_valid:]:
        id, label = [id_l[0] for id_l in individual_test[t_idx]], [id_l[1] for id_l in individual_test[t_idx]]
        x_test.append([ii for ii in id])
        y_test.append(label[0])
        id_test.append(id)
    for t_idx in temp_idx[:len_valid]:
        id, label = [id_l[0] for id_l in individual_test[t_idx]], [id_l[1] for id_l in individual_test[t_idx]]
        x_valid.append([ii for ii in id])
        y_valid.append(label[0])
        id_valid.append(id)
    x_train, x_valid, x_test, y_train, y_valid, y_test = x_train, x_valid, x_test, np.array(y_train).reshape([-1, 1]), \
                                                         np.array(y_valid).reshape([-1, 1]), np.array(y_test).reshape(
        [-1, 1])
    auc, acc, cm, recall, precision = train(x_train, x_valid, x_test, y_train, y_valid, y_test, id_train, id_test,
                                            data_augmented, data)
    aucs.append(auc)
    accuracy.append(acc)
    cms.append(cm)
    recalls.append(recall)
    precisions.append(precision)
    iter_count += 1
    if iter_count == abs(args.n_splits) * args.repeat:
        break
    print(f"âœ… Total valid splits used: {iter_count}")

    del data_augmented



# print("="*33)
# print("=== Final Evaluation (average across all splits) ===")
# print("="*33)

# print("Best performance: Test ACC %f,   Test AUC %f,   Test Recall %f,   Test Precision %f" % (np.average(accuracy), np.average(aucs), np.average(recalls), np.average(precisions)))

# print("=================================")
# print("=== ì €í¬ ë…¼ë¬¸ìš© Final Evaluation (average across all splits) ===")
# print("=================================")
# print(f"Best performance: "
#       f"Test ACC {np.mean(accuracy):.6f}+-{np.std(accuracy):.6f}, "
#       f"Test AUC {np.mean(aucs):.6f}+-{np.std(aucs):.6f}, "
#       f"Test Recall {np.mean(recalls):.6f}+-{np.std(recalls):.6f}, "
#       f"Test Precision {np.mean(precisions):.6f}+-{np.std(precisions):.6f}")

####################################
######## Only for repeat > 1 #######
####################################
# accuracy = np.array(accuracy).reshape([-1, args.repeat]).mean(0)
# aucs = np.array(aucs).reshape([-1, args.repeat]).mean(0)
# recalls = np.array(recalls).reshape([-1, args.repeat]).mean(0)
# precisions = np.array(precisions).reshape([-1, args.repeat]).mean(0)
# ci_1 = st.t.interval(alpha=0.95, df=len(accuracy) - 1, loc=np.mean(accuracy), scale=st.sem(accuracy))[1] - np.mean(accuracy)
# ci_2 = st.t.interval(alpha=0.95, df=len(aucs) - 1, loc=np.mean(aucs), scale=st.sem(aucs))[1] - np.mean(aucs)
# ci_3 = st.t.interval(alpha=0.95, df=len(recalls) - 1, loc=np.mean(recalls), scale=st.sem(recalls))[1] - np.mean(recalls)
# ci_4 = st.t.interval(alpha=0.95, df=len(precisions) - 1, loc=np.mean(precisions), scale=st.sem(precisions))[1] - np.mean(precisions)
# print("ci: ACC ci %f,   AUC ci %f,   Recall ci %f,   Precision ci %f" % (ci_1, ci_2, ci_3, ci_4))

# print(np.average(cms, 0))
# print(patient_summary)
# print(stats)
# print(stats_id)

"""
# ì‚¬ì „ ìƒì„±ëœ split íŒŒì¼ì„ ê¸°ë°˜ìœ¼ë¡œ ê³ ì •ëœ train/test ë°ì´í„°ì…‹ìœ¼ë¡œ ì‹¤í—˜ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ (2ë²ˆì½”ë“œ) # => ì£¼ì„ì²˜ë¦¬

# 3. for loop ì§ì ‘ êµ¬ì„± (repeat Ã— fold)

for repeat in range(args.repeat):
    fold_aucs, accuracy, cms, recalls, precisions = [], [], [], [], []
    iter_count = 0
    for fold in range(args.n_splits):
        print(f"ğŸ” Repeat {repeat}, Fold {fold}")
        # train_path = f"/data/project/kim89/0805_data/repeat_{repeat}/fold_{fold}_train.h5ad"
        # test_path = f"/data/project/kim89/0805_data/repeat_{repeat}/fold_{fold}_test.h5ad"

        train_path = f"{args.dataset}/repeat_{repeat}/fold_{fold}_train.h5ad"
        test_path = f"{args.dataset}/repeat_{repeat}/fold_{fold}_test.h5ad"

        train_data = scanpy.read_h5ad(train_path)
        test_data = scanpy.read_h5ad(test_path)

        train_p_index, train_labels, train_cell_type, train_patient_id, train_origin = Custom_data_from_loaded(train_data, args)
        test_p_index, test_labels, test_cell_type, test_patient_id, test_origin = Custom_data_from_loaded(test_data, args)

        labels_ = np.concatenate([train_labels, test_labels])


        print(f"ğŸ” Split #{iter_count + 1}")
        print(f"  â†’ train_p_index í™˜ì ìˆ˜ (í™˜ì ë‹¨ìœ„ë¡œ ë¬¶ì¸ index list): {len(train_p_index)}")
        print(f"  â†’ test_p_index í™˜ì ìˆ˜  (í™˜ì ë‹¨ìœ„ë¡œ ë¬¶ì¸ index list): {len(test_p_index)}")
        # train_p_index_ ì•ˆì—ëŠ” **"í™˜ì ë‹¨ìœ„ ë¬¶ìŒ"**ì´ ë“¤ì–´ìˆìŠµë‹ˆë‹¤.
        # ì¦‰, ê° ì›ì†Œê°€ np.array([...]) í˜•íƒœë¡œ, í•œ í™˜ìì— ì†í•˜ëŠ” ëª¨ë“  ì…€ì˜ ì¸ë±ìŠ¤ë¥¼ ë‹´ê³  ìˆëŠ” ê±°ì˜ˆìš”.
        # ê·¸ë˜ì„œ 86629, 86630, ... ê°™ì€ ìˆ«ìëŠ” ì‹¤ì œ "ì…€ ë²ˆí˜¸(index)"ì¼ ë¿, í™˜ì IDê°€ ì•„ë‹™ë‹ˆë‹¤.

        # ì‹¤ì œ í™˜ì IDë¡œ ë³´ê¸°
        train_ids = [train_patient_id[idx[0]] for idx in train_p_index]
        test_ids = [test_patient_id[idx[0]] for idx in test_p_index]
        print(f"  â†’ train í™˜ì ID: {train_ids}")
        print(f"  â†’ test  í™˜ì ID: {test_ids}")

        # ê° í™˜ìì˜ IDì™€ label í•¨ê»˜ ì¶œë ¥
        print("  â†’ train í™˜ì ID ë° ë¼ë²¨:")
        for idxs in train_p_index:
            idx = idxs[0]
            print(f"    ID: {train_patient_id[idx]}, Label: {train_labels[idx]}")

        print("  â†’ test í™˜ì ID ë° ë¼ë²¨:")
        for idxs in test_p_index:
            idx = idxs[0]
            print(f"    ID: {test_patient_id[idx]}, Label: {test_labels[idx]}")

        p_idx = train_p_index + test_p_index # ì´ì–´ë¶™ì´ê¸°(concatenation) ë¡œ ë™ì‘
        print("ì „ì²´ ë°ì´í„° index ê¸¸ì´", len(p_idx))    

        # if args.n_splits < 0:
        #     temp_idx = train_p_index
        #     train_p_index = test_p_index
        #     test_p_index = temp_idx

        # train_labels: ì „ì²´ ì…€ ë‹¨ìœ„ ë¼ë²¨ (186,636ê°œ)
        # í™˜ì ë‹¨ìœ„ ë¼ë²¨ë§Œ ë½‘ê¸°
        label_stat = [labels_[idx[0]] for idx in train_p_index] #  train setì— í¬í•¨ëœ í™˜ìë“¤ì˜ ë¼ë²¨ ëª©ë¡
        print("label_stat (train í™˜ì ë¼ë²¨ ëª©ë¡) ê°¯ìˆ˜", len(label_stat))
        # label_stat = []
        # for idx in train_p_index:
        #     label_stat.append(labels_[p_idx[idx][0]])

        unique, cts = np.unique(label_stat, return_counts=True)
    

        # í›ˆë ¨ ë°ì´í„°(train_p_index)ì— í´ë˜ìŠ¤ê°€ 2ê°œ ì´ìƒ ì¡´ì¬í•´ì•¼ í•™ìŠµì„ ì§„í–‰í•œë‹¤.
        if len(unique) < 2 or (1 in cts): 
            # í´ë˜ìŠ¤ê°€ í•˜ë‚˜ë°–ì— ì—†ìŒ â†’ ë¶ˆê· í˜• â†’ ìŠ¤í‚µ 
            # or 
            # ë“±ì¥í•œ í´ë˜ìŠ¤ ì¤‘ í•œ í´ë˜ìŠ¤ì˜ í™˜ì ìˆ˜ê°€ 1ëª…ë°–ì— ì•ˆ ë¨ â†’ í•™ìŠµì´ ë¶ˆì•ˆì •í•´ì§ˆ ê°€ëŠ¥ì„±ì´ ë§¤ìš° ë†’ê¸° ë•Œë¬¸ì— skip
            print("í›ˆë ¨ ë°ì´í„°(train_p_index)ì— í´ë˜ìŠ¤ê°€ 2ê°œ ì´ìƒ ì¡´ì¬í•´ì•¼ í•™ìŠµì„ ì§„í–‰", flush=True)
            continue
    #     print(dict(zip(unique, cts)))
        
        # ì›ë˜ ì½”ë“œì—ëŠ” test setì˜ í´ë˜ìŠ¤ ë¶ˆê· í˜•ì€ ì²´í¬í•˜ì§€ ì•ŠìŒ
        # ### âœ… test_p_index í´ë˜ìŠ¤ í™•ì¸ ì¶”ê°€
        # test_label_stat = [labels_[idx[0]] for idx in test_p_index]
        # if len(set(test_label_stat)) < 2:
        #     print(f"âš ï¸  Skipping split: test set has only one class -> {set(test_label_stat)}")
        #     continue

        # train_dataì—ì„œ í™˜ì ë‹¨ìœ„ë¡œ, trainê³¼ validation ë‚˜ëˆ„ê¸°
        print("train_dataì—ì„œ í™˜ì ë‹¨ìœ„ë¡œ, trainê³¼ validation ë‚˜ëˆ„ê¸°")
        print("ê¸°ì¡´ train_p_index",len(train_p_index))
        print("ê¸°ì¡´ (train) label_stat",len(label_stat))

        # 0) ìµœì†Œ í´ë˜ìŠ¤ ìƒ˜í”Œ ìˆ˜ ì ê²€ (stratifyê°€ ìš”êµ¬)
        cts = Counter(label_stat)
        min_cls = min(cts.values())
        if min_cls < 2:
            print(f"âš ï¸ ìµœì†Œ í´ë˜ìŠ¤ ìˆ˜ê°€ 2 ë¯¸ë§Œ: {cts}. ì´ splitì€ ê±´ë„ˆëœë‹ˆë‹¤.")
            continue

        # 1) stratifyë¡œ í•œ ë²ˆì— í•´ê²° (while ë¶ˆí•„ìš”)
        try:
            train_p_index_, valid_p_index, ty, vy = train_test_split(
                train_p_index, 
                # label_stat,
                # [train_labels[idx[0]] for idx in train_p_index],  # í•´ë‹¹ í™˜ì ë¼ë²¨ ëª©ë¡
                label_stat,        # í™˜ì ë‹¨ìœ„ ë¼ë²¨
                test_size=0.33,
                random_state=args.seed
                # stratify=label_stat
                # stratify= [train_labels[idx[0]] for idx in train_p_index]  # í•´ë‹¹ í™˜ì ë¼ë²¨ ëª©ë¡ 
                )
        except ValueError as e:
            # 2) ì‹¤íŒ¨ ì‹œ test_size ì¶•ì†Œí•´ì„œ í•œ ë²ˆ ë” ì‹œë„
            print(f"âš ï¸ stratify ì‹¤íŒ¨({e}). test_size=0.2ë¡œ ì¬ì‹œë„í•©ë‹ˆë‹¤.")
            train_p_index_, valid_p_index, ty, vy = train_test_split(
                train_p_index, train_labels,
                test_size=0.2,
                random_state=args.seed,
                # stratify=label_stat
            )

        # 3) (ì•ˆì „ë§) ë¶„í•  í›„ í´ë˜ìŠ¤ ì „ë¶€ í¬í•¨ëëŠ”ì§€ í™•ì¸
        classes = set(label_stat)
        print("train-vali ë¶„í•  í›„! ")
        print("train y ì˜ classes :",set(ty), "valid y ì˜ classes :",set(vy), "label_stat ì˜ classes :",classes)
        if not (set(ty) == classes and set(vy) == classes):
            print(f"âš ï¸ ë¶„í•  í›„ ì¼ë¶€ í´ë˜ìŠ¤ê°€ ë¹ ì§. (train={set(ty)}, val={set(vy)}) ì´ splitì€ ê±´ë„ˆëœë‹ˆë‹¤.")
            continue

        print("train_p_index_",len(train_p_index_), flush=True)
        print("valid_p_index",len(valid_p_index), flush=True)
        print("test_p_index",len(test_p_index), flush=True)

        train_p_index = train_p_index_
        print("â†’ train í™˜ì ID ë° ë¼ë²¨:")
        print("   ì´ ê°œìˆ˜:", len(train_p_index))
        for idxs in train_p_index:
            idx0 = idxs[0]
            print(f"   í™˜ìID={train_patient_id[idx0]}, Label={train_labels[idx0]}, ì…€ê°œìˆ˜={len(idxs)}")

        print("â†’ valid í™˜ì ID ë° ë¼ë²¨:")
        print("   ì´ ê°œìˆ˜:", len(valid_p_index))
        for idxs in valid_p_index:
            idx0 = idxs[0]
            print(f"   í™˜ìID={train_patient_id[idx0]}, Label={train_labels[idx0]}, ì…€ê°œìˆ˜={len(idxs)}")


        # âœ… testëŠ” test_patient_id/test_labelsë¡œ!
        print("â†’ test í™˜ì ID ë° ë¼ë²¨:")
        print("   ì´ ê°œìˆ˜:", len(test_p_index))
        for idxs in test_p_index:
            idx0 = idxs[0]
            print(f"   í™˜ìID={test_patient_id[idx0]}, Label={test_labels[idx0]}, ì…€ê°œìˆ˜={len(idxs)}")


        train_p_index = train_p_index_
        len_valid = len(valid_p_index)
        # _index = np.concatenate([valid_p_index, test_p_index])


        # train_ids = []
        # for i in train_p_index:
        #     train_ids.append(patient_id.iloc[p_idx[i][0]])

    #     print(train_ids)

        x_train = []
        x_test = []
        x_valid = []
        y_train = []
        y_valid = []
        y_test = []
        id_train = []
        id_test = []
        id_valid = []

        train_cell_type = pd.Series([
            ct if isinstance(ct, str) else "Unknown"
            for ct in train_cell_type
        ])

        print("âœ… Checking cell_type before mixups...")
        print("Unique types:", set([type(x) for x in train_cell_type]))
        print("Example values (first 10):", list(train_cell_type[:10]))
        print("NaN exists?", any([isinstance(x, float) and np.isnan(x) for x in train_cell_type]))
        # 1. Seriesì— NaNì´ ìˆëŠ”ì§€ í™•ì‹¤íˆ í™•ì¸
        print("ğŸ” isna count:", pd.Series(train_cell_type).isna().sum())

        # 2. set ì•ˆì— float (NaN)ê°€ ì„ì—¬ ìˆëŠ”ì§€ í™•ì¸
        print("ğŸ§ª Types in set(cell_type):", set([type(x) for x in set(train_cell_type)]))

        # 1) Train ìª½ mixup/ì¦ê°•
        # if args.augment_num > 0:
        #     print("data augment ì‹¤í–‰ í•¨")
        #     data_augmented, train_p_index_aug, labels_aug, cell_type_aug = mixups(
        #         args, train_origin, train_p_index_, train_labels, train_cell_type
        #     )
        #     if data_augmented is None:
        #         print("âš ï¸ Skipping due to insufficient classes for mixup")
        #         continue
        # else:
        #     print("data augment ì‹¤í–‰ ì•ˆ í•¨")
        #     data_augmented = train_origin
        #     train_p_index_aug = train_p_index_
        #     labels_aug = train_labels
        #     cell_type_aug = train_cell_type

        data = np.concatenate([train_origin, test_origin])
        print("ì „ì²´ ë°ì´í„° ê¸¸ì´", len(data))
        
        def to_series_1d(x):
            # numpy / list / Series ëª¨ë‘ 1ì°¨ì› Seriesë¡œ í†µì¼
            if isinstance(x, pd.Series):
                return x.reset_index(drop=True)
            return pd.Series(np.ravel(x))

        train_ct = to_series_1d(train_cell_type)
        test_ct  = to_series_1d(test_cell_type)

        # í–‰ ë°©í–¥ ì´ì–´ë¶™ì´ê¸° + ì¸ë±ìŠ¤ ì´ˆê¸°í™”
        cell_type = pd.concat([train_ct, test_ct], ignore_index=True)
        cell_type = cell_type.astype("string").fillna("Unknown")
        print("ì „ì²´ cell type ê¸¸ì´", cell_type)
        print("cell typeì˜ NaN ê°œìˆ˜ ì²´í¬",cell_type.isna().sum())
        data_augmented, train_p_idx, labels_augmented, cell_type_augmented = mixups(args, data,
                                                                            train_p_index,
                                                                            labels_,
                                                                            cell_type)

        _index = valid_p_index + test_p_index  # âœ… ë¦¬ìŠ¤íŠ¸ë¼ë¦¬ ê²°í•©
                                                        
        # 1) train/test ê²°í•© ê¸¸ì´ ê¸°ì¤€ìœ¼ë¡œ ì˜¤í”„ì…‹ ê³„ì‚°
        offset = train_origin.shape[0]

        # 2) test ìª½ ì¸ë±ìŠ¤ë“¤ì— ì˜¤í”„ì…‹ ì ìš©
        test_p_index_global = [idx + offset for idx in test_p_index]

        # 3) valid + testë¥¼ ê·¸ëŒ€ë¡œ í•©ì³ì„œ í‰ê°€ í›„ë³´ ë§Œë“¤ê¸°
        eval_p_index = valid_p_index + test_p_index_global

        # 4) mixup ê²°ê³¼ë¥¼ ì‚¬ìš©í•´ train ìª½ì€ train_p_index_augë¥¼ ì“°ê³ ,
        #    test ìª½ì€ eval_p_indexë¥¼ ê·¸ëŒ€ë¡œ sampling()ì— ì „ë‹¬
        individual_train, individual_test = sampling(
            args,
            train_p_idx,      # mixups()ê°€ ëŒë ¤ì¤€ train ìª½(ì¦ê°• í¬í•¨) í™˜ìë³„ ì…€ ì¸ë±ìŠ¤ ë°°ì—´ ë¦¬ìŠ¤íŠ¸
            eval_p_index,           # ì´ë¯¸ "ë°°ì—´ ë¦¬ìŠ¤íŠ¸" í˜•íƒœ â†’ p_idx[...]ë¡œ ë‹¤ì‹œ ì¸ë±ì‹± ê¸ˆì§€
            labels_,                # train+test ë¼ë²¨ì„ concat í•œ ë²¡í„° (ë°°ì—´ ì¸ë±ìŠ¤ê°€ ì „ì—­ ê¸°ì¤€ì´ì–´ì•¼ í•¨)
            labels_augmented,             # mixup í›„ ë¼ë²¨
            cell_type_augmented           # mixup í›„ ì…€íƒ€ì…
        )
  
        # # í‰ê°€ìš© ì¸ë±ìŠ¤ëŠ” valid + testë¡œ í•©ì³ì„œ sampling (scRAT êµ¬ì¡°ìƒ í•˜ë‚˜ë¡œ ë¬¶ì–´ì„œ sampling)
        # eval_p_index = valid_p_index + test_p_index
        # print("eval_p_index len: ",len(eval_p_index))

        # individual_train, individual_eval = sampling(
        #     args,
        #     train_p_index_aug,
        #     eval_p_index,
        #     train_labels,
        #     labels_aug,
        #     cell_type_aug
        # )
        # print("individual_train", len(individual_train))
        # print("individual_eval", len(individual_eval))

        for t in individual_train:
            id, label = [id_l[0] for id_l in t], [id_l[1] for id_l in t]
            x_train += [ii for ii in id]
            y_train += (label)
            id_train += (id)

        temp_idx = np.arange(len(_index))
        for t_idx in temp_idx[len_valid:]:
            id, label = [id_l[0] for id_l in individual_test[t_idx]], [id_l[1] for id_l in individual_test[t_idx]]
            x_test.append([ii for ii in id])
            y_test.append(label[0])
            id_test.append(id)
        for t_idx in temp_idx[:len_valid]:
            id, label = [id_l[0] for id_l in individual_test[t_idx]], [id_l[1] for id_l in individual_test[t_idx]]
            x_valid.append([ii for ii in id])
            y_valid.append(label[0])
            id_valid.append(id)
            

        # n_valid = len(valid_p_index)
        # for i in range(len(eval_p_index)):
        #     ids, labels = [x[0] for x in individual_eval[i]], [x[1] for x in individual_eval[i]]
        #     if i < n_valid:
        #         x_valid.append(ids)
        #         y_valid.append(labels[0])
        #         id_valid.append(ids)
        #     else:
        #         x_test.append(ids)
        #         y_test.append(labels[0])
        #         id_test.append(ids)


        x_train, x_valid, x_test, y_train, y_valid, y_test = x_train, x_valid, x_test, np.array(y_train).reshape([-1, 1]), \
                                                            np.array(y_valid).reshape([-1, 1]), np.array(y_test).reshape([-1, 1])
        print("train dataì˜ x, y, id ê¸¸ì´:", len(x_train), len(y_train), len(id_train))
        print("valid dataì˜ x, y, id ê¸¸ì´:", len(x_valid), len(y_valid), len(id_valid))
        print("test dataì˜ x, y, id ê¸¸ì´:", len(x_test), len(y_test), len(id_test))

        # 4) í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        # auc, acc, cm, recall, precision = train(
        #     x_train, x_valid, x_test,
        #     y_train, y_valid, y_test,
        #     id_train, id_test,
        #     data_augmented=data_augmented,  # numpy array
        #     data=train_origin             # or full data if needed
        # )
        auc, acc, cm, recall, precision = train(x_train, x_valid, x_test, y_train, y_valid, y_test, id_train, id_test,
                                                data_augmented, data)
        

        fold_aucs.append(auc)
        accuracy.append(acc)
        cms.append(cm)
        recalls.append(recall)
        precisions.append(precision)
        iter_count += 1
        if iter_count == abs(args.n_splits) * args.repeat:
            break
        print(f"âœ… Total valid splits used: {iter_count}")

        del data_augmented
        
    # ğŸ”½ Repeat ë‹¨ìœ„ AUC ì¶œë ¥ ì¶”ê°€
    print(f"\nğŸ“Œ Repeat {repeat}: í‰ê·  AUC = {np.nanmean(fold_aucs):.4f}, í‘œì¤€í¸ì°¨ = {np.nanstd(fold_aucs):.4f}")
    print(
      f"Test ACC í‰ê·  {np.nanmean(accuracy):.6f}, "
      f"Test Recall í‰ê·  {np.nanmean(recalls):.6f}, "
      f"Test Precision í‰ê·  {np.nanmean(precisions):.6f}")
    
    print("fold_aucs =", fold_aucs)
    nan_count = np.count_nonzero(np.isnan(fold_aucs))
    print(f"NaN ê°œìˆ˜: {nan_count} / ì „ì²´ {len(fold_aucs)}ê°œ\n\n")


